{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Overview\n",
    "Copy from https://www.kaggle.com/kmader/quickdraw-with-wavenet-classifier/data\n",
    "The notebook is modified from one that was made for the [Quick, Draw Dataset](https://www.kaggle.com/google/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n",
    "\n",
    "## This Notebook\n",
    "The notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an a WaveNet-style classifier (wavenet in its original implemention is https://deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/ is intended for synthesis, but the dilated convolution approach can be applied). We use the implementation [here](https://github.com/mjpyeon/wavenet-classifier/blob/master/WaveNetClassifier.py) as a reference.\n",
    "\n",
    "## Fun Models\n",
    "\n",
    "After the classification models, we try to build a few models to understand what the WaveNet actually does. Here we experiment step by step to see how the prediction changes with each stop\n",
    "\n",
    "### Next Steps\n",
    "The next steps could be\n",
    "- use more data to train\n",
    "- include the country code (different countries draw different things, different ways)\n",
    "- more complex models\n",
    "\n",
    "## setting\n",
    "set KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39077055d70da04344d314d12dba7f846a5e3721"
   },
   "source": [
    "### Model Parameters\n",
    "Here we keep all of the parameters to make keeping track of versions and hyperparameter optimization  easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join( 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4e8c68db0704a070ead7a5cb324ab59dd4b5aebd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xfile = 'F:\\workspace\\j6stock\\XOpenHighLowCloseVol_tp10_cl10.txt'\n",
    "seq_len = 300\n",
    "\n",
    "n_filters = 32 #64\n",
    "dilation_depth = 8\n",
    "activation = 'softmax'\n",
    "scale_ratio = 1\n",
    "kernel_size = 2\n",
    "pool_size_1 = 4\n",
    "pool_size_2 = 8\n",
    "batch_size = 4096 #batch_size = 4096\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 750\n",
    "VALID_SAMPLES = 75\n",
    "TEST_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "# def _stack_it(raw_strokes):\n",
    "#     \"\"\"preprocess the string and make \n",
    "#     a standard Nx3 stroke vector\"\"\"\n",
    "#     stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "#     # unwrap the list\n",
    "#     in_strokes = [(xi,yi,i)  \n",
    "#      for i,(x,y) in enumerate(stroke_vec) \n",
    "#      for xi,yi in zip(x,y)]\n",
    "#     c_strokes = np.stack(in_strokes)\n",
    "#     # replace stroke id with 1 for continue, 2 for new\n",
    "#     c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "#     c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "#     # pad the strokes with zeros\n",
    "#     return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "#                          maxlen=STROKE_COUNT, \n",
    "#                          padding='post').swapaxes(0, 1)\n",
    "# def read_batch(samples=5, \n",
    "#                start_row=0,\n",
    "#                max_rows=1000):\n",
    "#     \"\"\"\n",
    "#     load and process the csv files\n",
    "#     this function is horribly inefficient but simple\n",
    "#     \"\"\"\n",
    "#     out_df_list = []\n",
    "#     for c_path in ALL_TRAIN_PATHS:\n",
    "#         c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "#         c_df.columns=COL_NAMES\n",
    "#         out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "#     full_df = pd.concat(out_df_list)\n",
    "#     full_df['drawing'] = full_df['drawing'].\\\n",
    "#         map(_stack_it)\n",
    "    \n",
    "#     return full_df\n",
    "\n",
    "\n",
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "    #TODO Use previous close instead of open\n",
    "    df['change'] = df['close'] - df['open']\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'high'] = df.at[i, 'high'] - (df.at[i, 'open'] if df.at[i, 'open'] > df.at[i, 'close'] else df.at[i, 'close'])\n",
    "        df.at[i, 'low'] = (df.at[i, 'close'] if df.at[i, 'close'] < df.at[i, 'open'] else df.at[i, 'open']) - df.at[i, 'low']\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop('open', axis=1)\n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        #df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        #df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        #df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "        df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
    "        #df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "    df.dropna(inplace=True)\n",
    "               \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "    \n",
    "    #drop for wavenet and not enough memory\n",
    "    df.drop(labels=['change'], axis=1, inplace=True)\n",
    "    df.drop(labels=['high'], axis=1, inplace=True)\n",
    "    df.drop(labels=['low'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df = get_stock_data( ma=[50, 100, 200])\n",
    "df = get_stock_data()\n",
    "\n",
    "amount_of_features = len(df.columns)-1\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "        x_result.append(data[index-seq_len: index,:-1]) # index : index + 22days\n",
    "        y_result.append(data[index ,amount_of_features]);\n",
    "\n",
    "    #print('---', data[0])\n",
    "    #print('---', x_result[0])\n",
    "    #print('---', y_result[0])\n",
    "    x_result = np.array(x_result)\n",
    "    y_result = np.array(y_result)\n",
    "    row = round(0.6 * y_result.shape[0]) # 80% split\n",
    "    print (\"Amount of training data = {}\".format(0.9 * x_result.shape[0]))\n",
    "    print (\"Amount of testing data = {}\".format(0.1 * y_result.shape[0]))\n",
    "     \n",
    "    X_train = x_result[:int(row), :] # 90% date\n",
    "    y_train = y_result[:int(row)] # 90% date\n",
    "        \n",
    "\n",
    "    X_test = x_result[int(row):, :]\n",
    "    y_test = y_result[int(row):]\n",
    "    # filter for 1 and -1 for validation only\n",
    "    X_test = X_test[y_test[:]!=0,:]\n",
    "    y_test = y_test[y_test[:]!=0]\n",
    "    #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "    #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43c53589bedc9a53ea398be98cacc3cca4b24a76"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 1\n",
      "Amount of training data = 261936.0\n",
      "Amount of testing data = 29104.0\n"
     ]
    }
   ],
   "source": [
    "classes = [1, 0, -1]\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "\n",
    "X_tr, lab_tr, X_vld, lab_vld = load_data(df, seq_len)\n",
    "y_tr = lb.transform(lab_tr)\n",
    "y_vld = lb.transform(lab_vld)\n",
    "\n",
    "\n",
    "# train_args = dict(samples=TRAIN_SAMPLES, \n",
    "#                   start_row=0, \n",
    "#                   max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "# valid_args = dict(samples=VALID_SAMPLES, \n",
    "#                   start_row=train_args['max_rows']+1, \n",
    "#                   max_rows=VALID_SAMPLES+25)\n",
    "# test_args = dict(samples=TEST_SAMPLES, \n",
    "#                  start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "#                  max_rows=TEST_SAMPLES+25)\n",
    "# train_df = read_batch(**train_args)\n",
    "# valid_df = read_batch(**valid_args)\n",
    "# test_df = read_batch(**test_args)\n",
    "# word_encoder = LabelEncoder()\n",
    "# word_encoder.fit(train_df['word'])\n",
    "# print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174624, 300, 1)\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_X = X_tr\n",
    "train_y = y_tr\n",
    "valid_X = X_vld\n",
    "valid_y = y_vld\n",
    "test_X = X_vld\n",
    "test_y = y_vld\n",
    "print(train_X.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a4b3fed86dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_ax\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrand_idxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_axs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtest_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# only keep valid points\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlab_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlab_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6oAAAOJCAYAAAAURN+GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAACAoSiqAAAADEVRBQAAYCj/C53nOt6lqpSxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x1152 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "    #c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# WaveNet to Parse Strokes\n",
    "The model suggeted from the WaveNet article by DeepMind (taken from site mentioned above) is\n",
    "\n",
    "![Suggested Model](https://storage.googleapis.com/deepmind-live-cms/documents/wavenet_conv_gif.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "original_input (InputLayer)     (None, 300, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_1 (Conv1D)         (None, 300, 32)      96          original_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_tanh (Conv1D)    (None, 300, 32)      2080        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_2_sigm (Conv1D)    (None, 300, 32)      2080        dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_1 (Multiply)   (None, 300, 32)      0           dilated_conv_2_tanh[0][0]        \n",
      "                                                                 dilated_conv_2_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_1 (Conv1D)                 (None, 300, 32)      1056        gated_activation_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_1 (Add)          (None, 300, 32)      0           skip_1[0][0]                     \n",
      "                                                                 dilated_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_tanh (Conv1D)    (None, 300, 32)      2080        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_4_sigm (Conv1D)    (None, 300, 32)      2080        residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_2 (Multiply)   (None, 300, 32)      0           dilated_conv_4_tanh[0][0]        \n",
      "                                                                 dilated_conv_4_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_2 (Conv1D)                 (None, 300, 32)      1056        gated_activation_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_2 (Add)          (None, 300, 32)      0           skip_2[0][0]                     \n",
      "                                                                 residual_block_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_tanh (Conv1D)    (None, 300, 32)      2080        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_8_sigm (Conv1D)    (None, 300, 32)      2080        residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_3 (Multiply)   (None, 300, 32)      0           dilated_conv_8_tanh[0][0]        \n",
      "                                                                 dilated_conv_8_sigm[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "skip_3 (Conv1D)                 (None, 300, 32)      1056        gated_activation_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_3 (Add)          (None, 300, 32)      0           skip_3[0][0]                     \n",
      "                                                                 residual_block_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_tanh (Conv1D)   (None, 300, 32)      2080        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_16_sigm (Conv1D)   (None, 300, 32)      2080        residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_4 (Multiply)   (None, 300, 32)      0           dilated_conv_16_tanh[0][0]       \n",
      "                                                                 dilated_conv_16_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_4 (Conv1D)                 (None, 300, 32)      1056        gated_activation_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_4 (Add)          (None, 300, 32)      0           skip_4[0][0]                     \n",
      "                                                                 residual_block_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_tanh (Conv1D)   (None, 300, 32)      2080        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_32_sigm (Conv1D)   (None, 300, 32)      2080        residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_5 (Multiply)   (None, 300, 32)      0           dilated_conv_32_tanh[0][0]       \n",
      "                                                                 dilated_conv_32_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_5 (Conv1D)                 (None, 300, 32)      1056        gated_activation_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_5 (Add)          (None, 300, 32)      0           skip_5[0][0]                     \n",
      "                                                                 residual_block_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_tanh (Conv1D)   (None, 300, 32)      2080        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_64_sigm (Conv1D)   (None, 300, 32)      2080        residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_6 (Multiply)   (None, 300, 32)      0           dilated_conv_64_tanh[0][0]       \n",
      "                                                                 dilated_conv_64_sigm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "skip_6 (Conv1D)                 (None, 300, 32)      1056        gated_activation_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_6 (Add)          (None, 300, 32)      0           skip_6[0][0]                     \n",
      "                                                                 residual_block_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_tanh (Conv1D)  (None, 300, 32)      2080        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_128_sigm (Conv1D)  (None, 300, 32)      2080        residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_7 (Multiply)   (None, 300, 32)      0           dilated_conv_128_tanh[0][0]      \n",
      "                                                                 dilated_conv_128_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_7 (Conv1D)                 (None, 300, 32)      1056        gated_activation_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "residual_block_7 (Add)          (None, 300, 32)      0           skip_7[0][0]                     \n",
      "                                                                 residual_block_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_tanh (Conv1D)  (None, 300, 32)      2080        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dilated_conv_256_sigm (Conv1D)  (None, 300, 32)      2080        residual_block_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_8 (Multiply)   (None, 300, 32)      0           dilated_conv_256_tanh[0][0]      \n",
      "                                                                 dilated_conv_256_sigm[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "skip_8 (Conv1D)                 (None, 300, 32)      1056        gated_activation_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "skip_connections (Add)          (None, 300, 32)      0           skip_1[0][0]                     \n",
      "                                                                 skip_2[0][0]                     \n",
      "                                                                 skip_3[0][0]                     \n",
      "                                                                 skip_4[0][0]                     \n",
      "                                                                 skip_5[0][0]                     \n",
      "                                                                 skip_6[0][0]                     \n",
      "                                                                 skip_7[0][0]                     \n",
      "                                                                 skip_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 300, 32)      0           skip_connections[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_5ms (Conv1D)               (None, 300, 32)      4128        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_200Hz (AveragePoo (None, 75, 32)       0           conv_5ms[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms (Conv1D)             (None, 75, 32)       8224        downsample_to_200Hz[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_500ms_target_shape (Conv1D (None, 75, 3)        771         conv_500ms[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "downsample_to_2Hz (AveragePooli (None, 10, 3)        0           conv_500ms_target_shape[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv1D)             (None, 10, 3)        84          downsample_to_2Hz[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_pooling (GlobalAveragePoo (None, 3)            0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "final_activation (Activation)   (None, 3)            0           final_pooling[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 55,031\n",
      "Trainable params: 55,031\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Input, Activation, AveragePooling1D, Add, Multiply, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "input_shape = train_X.shape[1:]\n",
    "output_shape = train_y.shape[1:]\n",
    "def residual_block(x, i):\n",
    "    tanh_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_tanh' % (kernel_size ** i), \n",
    "                      activation='tanh'\n",
    "                      )(x)\n",
    "    sigm_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_sigm' % (kernel_size ** i), \n",
    "                      activation='sigmoid'\n",
    "                      )(x)\n",
    "    z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "    skip = Conv1D(n_filters, 1, name='skip_%d'%(i))(z)\n",
    "    res = Add(name='residual_block_%d' % (i))([skip, x])\n",
    "    return res, skip\n",
    "x = Input(shape=input_shape, name='original_input')\n",
    "skip_connections = []\n",
    "out = Conv1D(n_filters, 2, dilation_rate=1, padding='causal', name='dilated_conv_1')(x)\n",
    "for i in range(1, dilation_depth + 1):\n",
    "    out, skip = residual_block(out,i)\n",
    "    skip_connections.append(skip)\n",
    "out = Add(name='skip_connections')(skip_connections)\n",
    "out = Activation('relu')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_1, strides = 1, padding='same', name='conv_5ms', activation = 'relu')(out)\n",
    "out = AveragePooling1D(pool_size_1, padding='same', name='downsample_to_200Hz')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_2, padding='same', activation='relu', name='conv_500ms')(out)\n",
    "out = Conv1D(output_shape[0], pool_size_2, padding='same', activation='relu', name='conv_500ms_target_shape')(out)\n",
    "out = AveragePooling1D(pool_size_2, padding='same',name = 'downsample_to_2Hz')(out)\n",
    "out = Conv1D(output_shape[0], (int) (input_shape[0] / (pool_size_1*pool_size_2)), padding='same', name='final_conv')(out)\n",
    "out = GlobalAveragePooling1D(name='final_pooling')(out)\n",
    "out = Activation(activation, name='final_activation')(out)\n",
    "\n",
    "stroke_read_model = Model(x, out)  \n",
    "stroke_read_model.compile(optimizer='adam', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=5) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174624 samples, validate on 70634 samples\n",
      "Epoch 1/50\n",
      "174624/174624 [==============================] - 2125s 12ms/step - loss: 1.0329 - acc: 0.4637 - top_3_accuracy: 1.0000 - val_loss: 1.0973 - val_acc: 0.3401 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.10044 to 1.09727, saving model to stroke_lstm_model_weights.best.hdf5\n",
      "Epoch 2/50\n",
      "174624/174624 [==============================] - 1995s 11ms/step - loss: 1.0259 - acc: 0.4695 - top_3_accuracy: 1.0000 - val_loss: 1.1476 - val_acc: 0.3455 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/50\n",
      "174624/174624 [==============================] - 2847s 16ms/step - loss: 1.0220 - acc: 0.4724 - top_3_accuracy: 1.0000 - val_loss: 1.1339 - val_acc: 0.3619 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/50\n",
      "174624/174624 [==============================] - 2326s 13ms/step - loss: 1.0152 - acc: 0.4773 - top_3_accuracy: 1.0000 - val_loss: 1.1357 - val_acc: 0.3617 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/50\n",
      "174624/174624 [==============================] - 2702s 15ms/step - loss: 1.0065 - acc: 0.4819 - top_3_accuracy: 1.0000 - val_loss: 1.0975 - val_acc: 0.3642 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/50\n",
      "174624/174624 [==============================] - 2923s 17ms/step - loss: 1.0019 - acc: 0.4828 - top_3_accuracy: 1.0000 - val_loss: 1.1266 - val_acc: 0.3279 - val_top_3_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x177bcbcc780>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 50,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee75c585-b134-4ea2-b8f3-219e24efd1f1",
    "_uuid": "6b9cdf52d233de60108d72f540db978801b578c1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "test_cat = np.argmax(test_y, 1)\n",
    "pred_y = stroke_read_model.predict(test_X, batch_size = 4096)\n",
    "pred_cat = np.argmax(pred_y, 1)\n",
    "plt.matshow(confusion_matrix(test_cat, pred_cat))\n",
    "print(classification_report(test_cat, pred_cat, \n",
    "                            target_names = [x for x in ['1', '0', '-1']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db1d371b-4b2c-478f-b6df-76db58a24fbe",
    "_uuid": "bd9a16adcb46e07d7949644e69bf3483f7dce571"
   },
   "source": [
    "# Reading Point by Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf7dff37-c634-4930-8dae-3dba8090c251",
    "_uuid": "c43e87e7eccfb72dd35e64d872a7d658ffa535a3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes = [1, 0, -1]\n",
    "points_to_use = [5, 15, 20, 30, 40, 50]\n",
    "points_to_user = [108]\n",
    "samples = 12\n",
    "word_dex = lambda x: classes[x]\n",
    "rand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\n",
    "fig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples/8*24))\n",
    "for c_id, c_axs in zip(rand_idxs, m_axs):\n",
    "    res_idx = np.argmax(test_y[c_id])\n",
    "    goal_cat = classes[res_idx]\n",
    "    \n",
    "    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n",
    "        test_arr = test_X[c_id, :].copy()\n",
    "        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n",
    "        stroke_pred = stroke_read_model.predict(np.expand_dims(test_arr,0))[0]\n",
    "        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n",
    "        top_10_sum = np.sum(stroke_pred[top_10_idx])\n",
    "        \n",
    "        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "        lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "        for i in np.unique(lab_idx):\n",
    "            c_ax.plot(test_arr[lab_idx==i,0], \n",
    "                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n",
    "                      '.-')\n",
    "        c_ax.axis('off')\n",
    "        if pt_idx == (len(points_to_use)-1):\n",
    "            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum))\n",
    "        else:\n",
    "            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum, \n",
    "                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]/top_10_sum, \n",
    "                                                                 100*stroke_pred[res_idx]/top_10_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "193d66e021e6a0b29975ce42067bf65b10d283d7"
   },
   "source": [
    "# Submission\n",
    "We can create a submission using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "436a4fce-3843-4c84-8eeb-0161fe3c4e04",
    "_uuid": "4f3a40e23f2e917b68171822944491ab348e15b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(test_path)\n",
    "sub_df['drawing'] = sub_df['drawing'].map(_stack_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f2fb3a5463b7c71330771e4d84cbfe6f5c618c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_vec = np.stack(sub_df['drawing'].values, 0)\n",
    "sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d785199df3d8171c867a7b8016699bda8399c1df",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e022e1391e1fb7dfe4481b940d78be96c59a19e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\n",
    "top_3_pred[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd6ee8bab00ca3686472f8670c9a5bd823084e6b"
   },
   "source": [
    "## Show some predictions on the submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eabd3abcd81094a334dda6540a155b6d51580940",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "rand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\n",
    "for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "    test_arr = sub_vec[c_id]\n",
    "    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "    lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "    for i in np.unique(lab_idx):\n",
    "        c_ax.plot(test_arr[lab_idx==i,0], \n",
    "                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "    c_ax.axis('off')\n",
    "    c_ax.set_title(top_3_pred[c_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c0ca90c42b2c8568675ff5064bfe1acae6fcbac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df['word'] = top_3_pred\n",
    "sub_df[['key_id', 'word']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "338ece2c2655ecc1cbb27ad8685c72348046a72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import imsave\n",
    "# step size for gradient ascent\n",
    "step = 1.\n",
    "layer_dict = dict([(layer.name, layer) for layer in stroke_read_model.layers])\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "layer_name = 'final_conv'\n",
    "filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "# build a loss function that maximizes the activation\n",
    "# of the nth filter of the layer considered\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(layer_output[ :, filter_index])\n",
    "\n",
    "# compute the gradient of the input picture wrt this loss\n",
    "# this is the placeholder for the input images\n",
    "input_img = stroke_read_model.input\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# normalization trick: we normalize the gradient\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "# we start from a gray image with some noise\n",
    "input_img_data = np.random.random((3, input_shape[0], input_shape[1])) * 20 + 128.\n",
    "# run gradient ascent for 20 steps\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    input_img_data += grads_value * step\n",
    "    \n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    print(x.shape)\n",
    "    x = x.transpose((1,0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "img = input_img_data[0]\n",
    "img = deprocess_image(img)\n",
    "imsave('%s_filter_%d.png' % (layer_name, filter_index), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
