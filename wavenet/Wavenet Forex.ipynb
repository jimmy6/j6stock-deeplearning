{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "522ce330-d4f9-4fbe-9684-4b65fd684cca",
    "_uuid": "174484daa5f084ce4970f5048d3e05d2c4429787"
   },
   "source": [
    "# Overview\n",
    "Copy from https://www.kaggle.com/kmader/quickdraw-with-wavenet-classifier/data\n",
    "The notebook is modified from one that was made for the [Quick, Draw Dataset](https://www.kaggle.com/google/tinyquickdraw), it would actually be interesting to see how beneficial a transfer learning approach using that data as a starting point could be.\n",
    "\n",
    "## This Notebook\n",
    "The notebook takes and preprocesses the data from the QuickDraw Competition step (strokes) and trains an a WaveNet-style classifier (wavenet in its original implemention is https://deepmind.com/blog/high-fidelity-speech-synthesis-wavenet/ is intended for synthesis, but the dilated convolution approach can be applied). We use the implementation [here](https://github.com/mjpyeon/wavenet-classifier/blob/master/WaveNetClassifier.py) as a reference.\n",
    "\n",
    "## Fun Models\n",
    "\n",
    "After the classification models, we try to build a few models to understand what the WaveNet actually does. Here we experiment step by step to see how the prediction changes with each stop\n",
    "\n",
    "### Next Steps\n",
    "The next steps could be\n",
    "- use more data to train\n",
    "- include the country code (different countries draw different things, different ways)\n",
    "- more complex models\n",
    "\n",
    "## setting\n",
    "set KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39077055d70da04344d314d12dba7f846a5e3721"
   },
   "source": [
    "### Model Parameters\n",
    "Here we keep all of the parameters to make keeping track of versions and hyperparameter optimization  easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "#from tf.keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, CategoricalHinge, Recall, Precision\n",
    "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from glob import glob\n",
    "import gc\n",
    "gc.enable()\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "base_dir = os.path.join( 'input')\n",
    "test_path = os.path.join(base_dir, 'test_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4e8c68db0704a070ead7a5cb324ab59dd4b5aebd"
   },
   "outputs": [],
   "source": [
    "xfile = 'C:\\workspace\\j6stock\\XOpenHighLowCloseVol_tp10_cl10.txt'\n",
    "seq_len = 60*24*2\n",
    "\n",
    "n_filters = 16 #64\n",
    "dilation_depth = 4\n",
    "activation = 'softmax'\n",
    "scale_ratio = 1\n",
    "kernel_size = 4\n",
    "pool_size_1 = 8\n",
    "pool_size_2 = 16\n",
    "batch_size = 4096\n",
    "STROKE_COUNT = 196\n",
    "TRAIN_SAMPLES = 750\n",
    "VALID_SAMPLES = 75\n",
    "TEST_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7acacf8e960084782425ef1a1a3fd532a240ad48"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2cd5514babfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#df = get_stock_data( ma=[50, 100, 200])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stock_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mamount_of_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2cd5514babfd>\u001b[0m in \u001b[0;36mget_stock_data\u001b[1;34m(normalize, ma)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m#df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'change'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'change'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mma\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmoving\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified', '*.csv'))\n",
    "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
    "\n",
    "# def _stack_it(raw_strokes):\n",
    "#     \"\"\"preprocess the string and make \n",
    "#     a standard Nx3 stroke vector\"\"\"\n",
    "#     stroke_vec = literal_eval(raw_strokes) # string->list\n",
    "#     # unwrap the list\n",
    "#     in_strokes = [(xi,yi,i)  \n",
    "#      for i,(x,y) in enumerate(stroke_vec) \n",
    "#      for xi,yi in zip(x,y)]\n",
    "#     c_strokes = np.stack(in_strokes)\n",
    "#     # replace stroke id with 1 for continue, 2 for new\n",
    "#     c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "#     c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "#     # pad the strokes with zeros\n",
    "#     return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "#                          maxlen=STROKE_COUNT, \n",
    "#                          padding='post').swapaxes(0, 1)\n",
    "# def read_batch(samples=5, \n",
    "#                start_row=0,\n",
    "#                max_rows=1000):\n",
    "#     \"\"\"\n",
    "#     load and process the csv files\n",
    "#     this function is horribly inefficient but simple\n",
    "#     \"\"\"\n",
    "#     out_df_list = []\n",
    "#     for c_path in ALL_TRAIN_PATHS:\n",
    "#         c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
    "#         c_df.columns=COL_NAMES\n",
    "#         out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
    "#     full_df = pd.concat(out_df_list)\n",
    "#     full_df['drawing'] = full_df['drawing'].\\\n",
    "#         map(_stack_it)\n",
    "    \n",
    "#     return full_df\n",
    "\n",
    "\n",
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "    #TODO Use previous close instead of open\n",
    "    df['change'] = df['close'] - df['open']\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'high'] = df.at[i, 'high'] - (df.at[i, 'open'] if df.at[i, 'open'] > df.at[i, 'close'] else df.at[i, 'close'])\n",
    "        df.at[i, 'low'] = (df.at[i, 'close'] if df.at[i, 'close'] < df.at[i, 'open'] else df.at[i, 'open']) - df.at[i, 'low']\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop('open', axis=1)\n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        #df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        #df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        #df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "        df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
    "        df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "    df.dropna(inplace=True)\n",
    "               \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "    \n",
    "    #drop for wavenet and not enough memory\n",
    "    #df.drop(labels=['change'], axis=1, inplace=True)\n",
    "    df.drop(labels=['high'], axis=1, inplace=True)\n",
    "    df.drop(labels=['low'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df = get_stock_data( ma=[50, 100, 200])\n",
    "df = get_stock_data()\n",
    "\n",
    "amount_of_features = len(df.columns)-1\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "        x_result.append(data[index-seq_len: index,:-1]) # index : index + 22days\n",
    "        y_result.append(data[index ,amount_of_features]);\n",
    "\n",
    "    #print('---', data[0])\n",
    "    #print('---', x_result[0])\n",
    "    #print('---', y_result[0])\n",
    "    x_result = np.array(x_result)\n",
    "    y_result = np.array(y_result)\n",
    "    row = round(0.6 * y_result.shape[0]) # 80% split\n",
    "    print (\"Amount of training data = {}\".format(0.9 * x_result.shape[0]))\n",
    "    print (\"Amount of testing data = {}\".format(0.1 * y_result.shape[0]))\n",
    "     \n",
    "    X_train = x_result[:int(row), :] # 90% date\n",
    "    y_train = y_result[:int(row)] # 90% date\n",
    "        \n",
    "\n",
    "    X_test = x_result[int(row):, :]\n",
    "    y_test = y_result[int(row):]\n",
    "    # filter for 1 and -1 for validation only\n",
    "    X_test = X_test[y_test[:]!=0,:]\n",
    "    y_test = y_test[y_test[:]!=0]\n",
    "    #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "    #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43c53589bedc9a53ea398be98cacc3cca4b24a76"
   },
   "source": [
    "# Reading and Parsing\n",
    "Since it is too much data (23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "classes = [1, 0, -1]\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "\n",
    "X_tr, lab_tr, X_vld, lab_vld = load_data(df, seq_len)\n",
    "y_tr = lb.transform(lab_tr)\n",
    "y_vld = lb.transform(lab_vld)\n",
    "\n",
    "\n",
    "# train_args = dict(samples=TRAIN_SAMPLES, \n",
    "#                   start_row=0, \n",
    "#                   max_rows=int(TRAIN_SAMPLES*1.5))\n",
    "# valid_args = dict(samples=VALID_SAMPLES, \n",
    "#                   start_row=train_args['max_rows']+1, \n",
    "#                   max_rows=VALID_SAMPLES+25)\n",
    "# test_args = dict(samples=TEST_SAMPLES, \n",
    "#                  start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
    "#                  max_rows=TEST_SAMPLES+25)\n",
    "# train_df = read_batch(**train_args)\n",
    "# valid_df = read_batch(**valid_args)\n",
    "# test_df = read_batch(**test_args)\n",
    "# word_encoder = LabelEncoder()\n",
    "# word_encoder.fit(train_df['word'])\n",
    "# print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6d29237e-ece3-4dfd-9095-475296f4a608",
    "_uuid": "8bae16a4973a215861fbb536a602c4f5abf3b4bf"
   },
   "source": [
    "# Stroke-based Classification\n",
    "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff5ddced-d77e-473f-899d-82cf11ad2bd9",
    "_uuid": "409468f1d5abd17b819482473a4f354a61f8d7ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_X = X_tr\n",
    "train_y = y_tr\n",
    "valid_X = X_vld\n",
    "valid_y = y_vld\n",
    "test_X = X_vld\n",
    "test_y = y_vld\n",
    "print(train_X.shape)\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56240ed9-42b0-4f62-b3d1-f92017f04e30",
    "_uuid": "5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8"
   },
   "outputs": [],
   "source": [
    "# fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "# rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
    "# for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "#     test_arr = train_X[c_id]\n",
    "#     test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "#     lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "#     for i in np.unique(lab_idx):\n",
    "#         c_ax.plot(test_arr[lab_idx==i,0], \n",
    "#                 np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "#     c_ax.axis('off')\n",
    "    #c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5e1d5bba-0fb4-432c-bd0b-ad69be0ef9ac",
    "_uuid": "b4a087a17798c2ec8eb520bc916bcad38d4ebff2",
    "collapsed": true
   },
   "source": [
    "# WaveNet to Parse Strokes\n",
    "The model suggeted from the WaveNet article by DeepMind (taken from site mentioned above) is\n",
    "\n",
    "![Suggested Model](https://storage.googleapis.com/deepmind-live-cms/documents/wavenet_conv_gif.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d65490e7-302e-4232-afe7-4e9499010e31",
    "_uuid": "ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, CategoricalHinge, Recall, Precision\n",
    "from tensorflow.keras.layers import Conv1D, Input, Activation, AveragePooling1D, Add, Multiply, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "input_shape = train_X.shape[1:]\n",
    "output_shape = train_y.shape[1:]\n",
    "def residual_block(x, i):\n",
    "    tanh_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_tanh' % (kernel_size ** i), \n",
    "                      activation='tanh'\n",
    "                      )(x)\n",
    "    sigm_out = Conv1D(n_filters, \n",
    "                      kernel_size, \n",
    "                      dilation_rate = kernel_size**i, \n",
    "                      padding='causal', \n",
    "                      name='dilated_conv_%d_sigm' % (kernel_size ** i), \n",
    "                      activation='sigmoid'\n",
    "                      )(x)\n",
    "    z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n",
    "    skip = Conv1D(n_filters, 1, name='skip_%d'%(i))(z)\n",
    "    res = Add(name='residual_block_%d' % (i))([skip, x])\n",
    "    return res, skip\n",
    "x = Input(shape=input_shape, name='original_input')\n",
    "skip_connections = []\n",
    "\n",
    "out = Conv1D(32, kernel_size = 10, strides = 5, activation='relu')(x)\n",
    "out = Conv1D(32, kernel_size = 10, strides = 5, activation='relu')(out)\n",
    "out = Conv1D(32, kernel_size = 4, strides = 2, activation='relu')(out)\n",
    "out = Conv1D(32, kernel_size = 4, strides = 2, activation='relu')(out)\n",
    "out = Conv1D(32, kernel_size = 4, strides = 2, activation='relu')(out)\n",
    "out = Conv1D(32, kernel_size = 4, strides = 2, activation='relu')(out)\n",
    "\n",
    "out = Conv1D(n_filters, 2, dilation_rate=1, padding='causal', name='dilated_conv_1')(out)\n",
    "for i in range(1, dilation_depth + 1):\n",
    "    out, skip = residual_block(out,i)\n",
    "    skip_connections.append(skip)\n",
    "out = Add(name='skip_connections')(skip_connections)\n",
    "out = Activation('relu')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_1, strides = 2, padding='same', name='conv_5ms', activation = 'relu')(out)\n",
    "out = AveragePooling1D(pool_size_1, padding='same', name='downsample_to_200Hz')(out)\n",
    "\n",
    "out = Conv1D(n_filters, pool_size_2, padding='same', activation='relu', name='conv_500ms')(out)\n",
    "out = Conv1D(output_shape[0], pool_size_2, padding='same', activation='relu', name='conv_500ms_target_shape')(out)\n",
    "out = AveragePooling1D(pool_size_2, padding='same',name = 'downsample_to_2Hz')(out)\n",
    "out = Conv1D(output_shape[0], (int) (input_shape[0] / (pool_size_1*pool_size_2)), padding='same', name='final_conv')(out)\n",
    "out = GlobalAveragePooling1D(name='final_pooling')(out)\n",
    "out = Activation(activation, name='final_activation')(out)\n",
    "\n",
    "stroke_read_model = Model(x, out)  \n",
    "stroke_read_model.compile(optimizer='adam', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy', top_3_accuracy\n",
    "                                   , Recall(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                   , Recall(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                   , Precision(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                   , Precision(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                  ])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a549512-a9d9-4afd-b748-3e1c3296e193",
    "_uuid": "5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195"
   },
   "outputs": [],
   "source": [
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=10) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint\n",
    "                  #, early, reduceLROnPlat\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "825b3af8-9451-487b-a1e1-538f2f1489e1",
    "_uuid": "ed2fc26af74aed1a93bbc253d61b72db5a81f5cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "stroke_read_model.fit(train_X, train_y,\n",
    "                      validation_data = (valid_X, valid_y), \n",
    "                      batch_size = batch_size,\n",
    "                      epochs = 100,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7eb5b62-cf57-4380-8786-9ddc05be658f",
    "_uuid": "858059b6c16d81f86460bef8fcf595e0d68d12b2"
   },
   "outputs": [],
   "source": [
    "stroke_read_model.load_weights(weight_path)\n",
    "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 4096)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%, Recall S %2.8f%%, Recall L %2.8f%%, Precision S %2.8f%%, Precision L %2.8f%%' % (100*lstm_results[1], 100*lstm_results[2], lstm_results[3], lstm_results[4], lstm_results[5], lstm_results[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee75c585-b134-4ea2-b8f3-219e24efd1f1",
    "_uuid": "6b9cdf52d233de60108d72f540db978801b578c1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "test_cat = np.argmax(test_y, 1)\n",
    "pred_y = stroke_read_model.predict(test_X, batch_size = 4096)\n",
    "pred_cat = np.argmax(pred_y, 1)\n",
    "plt.matshow(confusion_matrix(test_cat, pred_cat))\n",
    "print(classification_report(test_cat, pred_cat, \n",
    "                            target_names = [x for x in ['1', '0', '-1']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db1d371b-4b2c-478f-b6df-76db58a24fbe",
    "_uuid": "bd9a16adcb46e07d7949644e69bf3483f7dce571"
   },
   "source": [
    "# Reading Point by Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf7dff37-c634-4930-8dae-3dba8090c251",
    "_uuid": "c43e87e7eccfb72dd35e64d872a7d658ffa535a3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classes = [1, 0, -1]\n",
    "points_to_use = [5, 15, 20, 30, 40, 50]\n",
    "points_to_user = [108]\n",
    "samples = 12\n",
    "word_dex = lambda x: classes[x]\n",
    "rand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\n",
    "fig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples/8*24))\n",
    "for c_id, c_axs in zip(rand_idxs, m_axs):\n",
    "    res_idx = np.argmax(test_y[c_id])\n",
    "    goal_cat = classes[res_idx]\n",
    "    \n",
    "    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n",
    "        test_arr = test_X[c_id, :].copy()\n",
    "        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n",
    "        stroke_pred = stroke_read_model.predict(np.expand_dims(test_arr,0))[0]\n",
    "        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n",
    "        top_10_sum = np.sum(stroke_pred[top_10_idx])\n",
    "        \n",
    "        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "        lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "        for i in np.unique(lab_idx):\n",
    "            c_ax.plot(test_arr[lab_idx==i,0], \n",
    "                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n",
    "                      '.-')\n",
    "        c_ax.axis('off')\n",
    "        if pt_idx == (len(points_to_use)-1):\n",
    "            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum))\n",
    "        else:\n",
    "            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum, \n",
    "                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]/top_10_sum, \n",
    "                                                                 100*stroke_pred[res_idx]/top_10_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "193d66e021e6a0b29975ce42067bf65b10d283d7"
   },
   "source": [
    "# Submission\n",
    "We can create a submission using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "436a4fce-3843-4c84-8eeb-0161fe3c4e04",
    "_uuid": "4f3a40e23f2e917b68171822944491ab348e15b3"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(test_path)\n",
    "sub_df['drawing'] = sub_df['drawing'].map(_stack_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f2fb3a5463b7c71330771e4d84cbfe6f5c618c4"
   },
   "outputs": [],
   "source": [
    "sub_vec = np.stack(sub_df['drawing'].values, 0)\n",
    "sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d785199df3d8171c867a7b8016699bda8399c1df"
   },
   "outputs": [],
   "source": [
    "top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e022e1391e1fb7dfe4481b940d78be96c59a19e"
   },
   "outputs": [],
   "source": [
    "top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\n",
    "top_3_pred[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd6ee8bab00ca3686472f8670c9a5bd823084e6b"
   },
   "source": [
    "## Show some predictions on the submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eabd3abcd81094a334dda6540a155b6d51580940"
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
    "rand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\n",
    "for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
    "    test_arr = sub_vec[c_id]\n",
    "    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
    "    lab_idx = np.cumsum(test_arr[:,2]-1)\n",
    "    for i in np.unique(lab_idx):\n",
    "        c_ax.plot(test_arr[lab_idx==i,0], \n",
    "                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
    "    c_ax.axis('off')\n",
    "    c_ax.set_title(top_3_pred[c_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c0ca90c42b2c8568675ff5064bfe1acae6fcbac"
   },
   "outputs": [],
   "source": [
    "sub_df['word'] = top_3_pred\n",
    "sub_df[['key_id', 'word']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "338ece2c2655ecc1cbb27ad8685c72348046a72e"
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "# step size for gradient ascent\n",
    "step = 1.\n",
    "layer_dict = dict([(layer.name, layer) for layer in stroke_read_model.layers])\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "layer_name = 'final_conv'\n",
    "filter_index = 0  # can be any integer from 0 to 511, as there are 512 filters in that layer\n",
    "\n",
    "# build a loss function that maximizes the activation\n",
    "# of the nth filter of the layer considered\n",
    "layer_output = layer_dict[layer_name].output\n",
    "loss = K.mean(layer_output[ :, filter_index])\n",
    "\n",
    "# compute the gradient of the input picture wrt this loss\n",
    "# this is the placeholder for the input images\n",
    "input_img = stroke_read_model.input\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# normalization trick: we normalize the gradient\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "# we start from a gray image with some noise\n",
    "input_img_data = np.random.random((3, input_shape[0], input_shape[1])) * 20 + 128.\n",
    "# run gradient ascent for 20 steps\n",
    "for i in range(20):\n",
    "    loss_value, grads_value = iterate([input_img_data])\n",
    "    input_img_data += grads_value * step\n",
    "    \n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    print(x.shape)\n",
    "    x = x.transpose((1,0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "img = input_img_data[0]\n",
    "img = deprocess_image(img)\n",
    "imsave('%s_filter_%d.png' % (layer_name, filter_index), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
