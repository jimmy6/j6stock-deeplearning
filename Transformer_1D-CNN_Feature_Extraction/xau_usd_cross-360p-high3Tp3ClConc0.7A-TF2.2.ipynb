{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "e1baa7518f18a7ff1f2767df1b29c051955502ff"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow_addons as tfa\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, basename, splitext, isfile, exists\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, CategoricalHinge, Recall, Precision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras import metrics\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.compat.v2.keras.layers import Input\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random, os, sys\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "tf.compat.v1.disable_eager_execution\n",
    "pd.set_option('precision', 30)\n",
    "np.set_printoptions(precision = 30)\n",
    "\n",
    "\n",
    "#np.random.seed(368)\n",
    "#tf.random.set_seed(368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size : <TensorSliceDataset shapes: (((1,), (5,)), (3,)), types: ((tf.float64, tf.float64), tf.int32)>\n",
      "Total Filtered Size : <TensorSliceDataset shapes: (((1,), (5,)), (3,)), types: ((tf.float64, tf.float64), tf.int32)>\n",
      "[[0.13306324385041624]\n",
      " [0.1332525589085356 ]\n",
      " [0.13310110686203958]\n",
      " ...\n",
      " [0.3347973697827924 ]\n",
      " [0.33417894059293474]\n",
      " [0.33525172592228003]]\n",
      "[0 0 1]\n",
      "[0 0 1]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "xfile='C:\\\\workspace\\\\j6stock\\\\xau_usd_OHLC3.0Tp3.0Cl100Vp.txt'\n",
    "\n",
    "seq_len = 60 * 6\n",
    "seq_len_long = 60 *6 #60*10 # 3 days + 2 features is enough memory\n",
    "batch_size = int(2048/3)       # Batch size\n",
    "# mini_batch_size = 64       # Batch size\n",
    "\n",
    "learning_rate = 0.005  #0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "y_column = 6\n",
    "compute_val_at = 0\n",
    "acc_filtered_r = 0.8\n",
    "\n",
    "\n",
    "upperTailFilter = 0.4\n",
    "lowerTailFilter = 0.4\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "#from keras.layers.recurrent import LSTM\n",
    "#from keras.models import load_model\n",
    "#import keras\n",
    "import pandas as pd ## can be remove once pandas_datareader 0.7 using\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like ## can be remove once pandas_datareader 0.7 using\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "\n",
    "\n",
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    global upperTailFilter, lowerTailFilter\n",
    "    \n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "    #TODO Use previous close instead of open\n",
    "    df['change'] = df['close'] - df['open']\n",
    "    df['tail_upper'] = df['high'].copy()\n",
    "    df['tail_lower'] = df['low'].copy()\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'tail_upper'] = df.at[i, 'high'] - (df.at[i, 'open'] if df.at[i, 'open'] > df.at[i, 'close'] else df.at[i, 'close'])\n",
    "        df.at[i, 'tail_lower'] = (df.at[i, 'close'] if df.at[i, 'close'] < df.at[i, 'open'] else df.at[i, 'open']) - df.at[i, 'low']\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.drop('open', axis=1, inplace=True)\n",
    "    df.drop('high', axis=1, inplace=True)\n",
    "    df.drop('low', axis=1, inplace=True)\n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean() \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        \n",
    "        #df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        #df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        #df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "        min_max_scaler_close = preprocessing.MinMaxScaler()\n",
    "        df['close'] = min_max_scaler_close.fit_transform(df['close'].values.reshape(-1,1))\n",
    "#         df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1)) # no rescale for keep the negative value\n",
    "        df['tail_upper'] = min_max_scaler.fit_transform(df['tail_upper'].values.reshape(-1,1))\n",
    "        upperTailFilter = min_max_scaler.transform([[upperTailFilter]])[0][0] \n",
    "        df['tail_lower'] = min_max_scaler.fit_transform(df['tail_lower'].values.reshape(-1,1))\n",
    "        lowerTailFilter = min_max_scaler.transform([[lowerTailFilter]])[0][0] \n",
    "        \n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))   \n",
    "                #pd.concat([min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1)), df], axis=1)\n",
    "                ma_data = df['{}ma'.format(moving)]\n",
    "                df.drop(labels=['{}ma'.format(moving)], axis=1, inplace=True)\n",
    "                df = pd.concat([ma_data, df], axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = get_stock_data( ma=[50, 100, 200])\n",
    "df = get_stock_data()\n",
    "\n",
    "# amount_of_features = len(df.columns)-1+(input2Length*-1)\n",
    "\n",
    "# def load_data(stock, seq_len):\n",
    "#     print (\"Amount of features = {}\".format(amount_of_features))\n",
    "#     data = stock.as_matrix()\n",
    "#     sequence_length = seq_len + 1 # index starting from 0\n",
    "#     x_result = []\n",
    "#     x_result2 = []\n",
    "#     y_result = []\n",
    "#     for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "#         x_result.append(data[index-seq_len: index,\n",
    "#                              :-1 + (input2Length*-1) # -2 is ignore Input2 features\n",
    "#                             ]) # index : index + 22days\n",
    "#         x_result2.append(data[index, -1 + (input2Length*-1):-1])\n",
    "#         y_result.append(data[index ,-1]);\n",
    "\n",
    "#     x_result, x_result2, y_result = shuffle(x_result, x_result2, y_result , random_state=2)\n",
    "\n",
    "#     #print('---', data[0])\n",
    "#     #print('---', x_result[0])\n",
    "#     #print('---', y_result[0])\n",
    "#     x_result = np.array(x_result)\n",
    "#     x_result2 = np.array(x_result2)\n",
    "#     y_result = np.array(y_result)\n",
    "#     print (\"Amount of data = {}\".format(y_result.shape[0]))\n",
    "\n",
    "#     percentageSplit = 0.5 # 60% split\n",
    "#     row = round(percentageSplit * y_result.shape[0]) \n",
    "#     print (\"Split = {}\".format(row))\n",
    " \n",
    "#     X_train = x_result[:int(row), :] \n",
    "#     X_train2 = x_result2[:int(row), :] \n",
    "#     y_train = y_result[:int(row)] \n",
    "#     print (\"Amount of training data = {}\".format(y_train.shape[0]))\n",
    "#     X_test = x_result[int(row):, :]\n",
    "#     X_test2 = x_result2[int(row):, :]\n",
    "#     y_test = y_result[int(row):]\n",
    "#     # filter for 1 and -1 for validation only\n",
    "#     X_test = X_test[y_test[:]!=0,:]\n",
    "#     X_test2 = X_test2[y_test[:]!=0,:]\n",
    "#     y_test = y_test[y_test[:]!=0]\n",
    "    \n",
    "#     # split 50% again for test and validation set\n",
    "#     row = round(percentageSplit * y_test.shape[0]) \n",
    "#     X_val = X_test[int(row):, :]\n",
    "#     X_val2 = X_test2[int(row):, :]\n",
    "#     y_val = y_test[int(row):]\n",
    "#     print (\"Amount of validation data = {}\".format(y_val.shape[0]))\n",
    "#     X_test = X_test[:int(row), :]\n",
    "#     X_test2 = X_test2[:int(row), :]\n",
    "#     y_test = y_test[:int(row)]\n",
    "#     print (\"Amount of testing data = {}\".format(y_test.shape[0]))\n",
    "#     #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "#     #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "#     #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))    \n",
    "#     return [X_train, X_train2, y_train, X_test, X_test2, y_test, X_val, X_val2, y_val]\n",
    "\n",
    "\n",
    "\n",
    "classes = [1, 0, -1]\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "lb.transform([-1, 0, 1])\n",
    "\n",
    "df_to_dataset = df[[# 'change',\n",
    "                     'close',\n",
    "                    # '240ma'\n",
    "                   ]].copy()\n",
    "df_to_dataset_short = df[[# 'change',\n",
    "                     'close',\n",
    "                    # '240ma'\n",
    "                   ]].copy()\n",
    "\n",
    "df_to_dataset_input2 = df[[ 'tail_upper', 'tail_lower', 'change' ,'cross_360p_high', 'cross_1440p_high'\n",
    "                          ]].copy()\n",
    "\n",
    "amount_of_features = len(df_to_dataset.columns)\n",
    "amount_of_short_features = len(df_to_dataset_short.columns)\n",
    "\n",
    "input2Length = len(df_to_dataset_input2.columns)\n",
    "\n",
    "df.drop(labels=['close'], axis=1, inplace=True)\n",
    "df.drop(labels=['change'], axis=1, inplace=True)\n",
    "df.drop(labels=['tail_upper'], axis=1, inplace=True)\n",
    "df.drop(labels=['tail_lower'], axis=1, inplace=True)\n",
    "df.drop(labels=['cross_360p_high'], axis=1, inplace=True)\n",
    "df.drop(labels=['cross_1440p_high'], axis=1, inplace=True)\n",
    "\n",
    "df_to_dataset_y = lb.transform(df[['y_result']].copy())\n",
    "df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "\n",
    "train_data_no = int(len(df_to_dataset_y)/2)\n",
    "test_data_no = int(train_data_no/2)\n",
    "v_data_no = test_data_no\n",
    "\n",
    "train_x_1 = df_to_dataset.iloc[:train_data_no].values\n",
    "train_x_2 = df_to_dataset_input2.iloc[:train_data_no].values\n",
    "train_y = df_to_dataset_y[:train_data_no]                                                   \n",
    "\n",
    "test_x_1 = df_to_dataset.iloc[train_data_no:train_data_no+test_data_no].values\n",
    "test_x_2 = df_to_dataset_input2.iloc[train_data_no:train_data_no+test_data_no].values\n",
    "test_y = df_to_dataset_y[train_data_no:train_data_no+test_data_no]                                                   \n",
    "\n",
    "v_x_1 = df_to_dataset.iloc[train_data_no+test_data_no:].values\n",
    "v_x_2 = df_to_dataset_input2.iloc[train_data_no+test_data_no:].values\n",
    "v_y = df_to_dataset_y[train_data_no+test_data_no:]                                                   \n",
    "\n",
    "\n",
    "def make_window_dataset(ds, window_size=seq_len, shift=1, stride=1):\n",
    "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "  def batch(sub):\n",
    "    ret = ()\n",
    "    for index in range(2):\n",
    "      ret = ret + ( sub[index].batch(window_size, drop_remainder=True), )\n",
    "    return ret\n",
    "  def sub_to_batch(sub, sub2): \n",
    "    return tf.data.Dataset.zip((batch(sub), (sub2.batch(window_size, drop_remainder=True))))\n",
    "  \n",
    "  windows = windows.flat_map(sub_to_batch)\n",
    "  windows = windows.map(lambda sub1, sub2: ((sub1[0], sub1[1][-1:][0]), (sub2[-1:][0] #, sub2[-1:][0]\n",
    "                                                                        )))\n",
    "  return windows\n",
    "\n",
    "def filter_fn(a, b):\n",
    "#   return a[1][0]>=upperTailFilter or a[1][1]>=lowerTailFilter or a[1][3]==1 or a[1][4]==1\n",
    "  return a[1][3]==1\n",
    "\n",
    "train_dataset_x = tf.data.Dataset.from_tensor_slices(((train_x_1, train_x_2),(train_y)))\n",
    "train_dataset = make_window_dataset(train_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True) \n",
    "\n",
    "test_dataset_x = tf.data.Dataset.from_tensor_slices(((test_x_1, test_x_2),(test_y)))\n",
    "test_dataset = make_window_dataset(test_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True)\n",
    "\n",
    "v_dataset_x = tf.data.Dataset.from_tensor_slices(((v_x_1, v_x_2),(v_y)))\n",
    "v_dataset = make_window_dataset(v_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "print('Total Size : {}'.format(train_dataset_x))\n",
    "print('Total Filtered Size : {}'.format(v_dataset_x))\n",
    "\n",
    "\n",
    "# X_tr, X_tr2, lab_tr, X_test, X_test2, lab_test, X_vld, X_vld2, lab_vld = load_data(df, seq_len)\n",
    "# y_tr = lb.transform(lab_tr)\n",
    "# y_vld = lb.transform(lab_vld)\n",
    "# y_test = lb.transform(lab_test)\n",
    "\n",
    "\n",
    "# train_X = X_tr\n",
    "# train_X2 = X_tr2\n",
    "# train_y = y_tr\n",
    "# valid_X = X_vld\n",
    "# valid_X2 = X_vld2\n",
    "# valid_y = y_vld\n",
    "# test_X = X_test\n",
    "# test_X2 = X_test2\n",
    "# test_y = y_test\n",
    "\n",
    "print(train_x_1)\n",
    "print(train_y[0])\n",
    "print(train_y[1])\n",
    "print(train_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shujian/transformer-with-lstm\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "\n",
    "\n",
    "embed_size = 60\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='tanh')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "def CnnTransformerModel():\n",
    "#    i = tf.compat.v2.keras.layers.Flatten(input_shape=(batch_size, amount_of_features))\n",
    "    i = tf.compat.v2.keras.layers.Input(shape = (seq_len, amount_of_features)#, batch_size=mini_batch_size\n",
    "                                       )\n",
    "    \n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64*2, kernel_size = 3, dilation_rate=2)(i)\n",
    "#     x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, dilation_rate=2)(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, dilation_rate=2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, dilation_rate=2)(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('tanh')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "#     x2 = tf.compat.v2.keras.layers.Convolution1D(64*2, kernel_size = 3, strides=2)(i)\n",
    "# #     x2 = tf.compat.v2.keras.layers.BatchNormalization()(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Activation('tanh')(x2)\n",
    "#     x2 = tf.keras.layers.Dropout(rate=0.2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, strides=2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.BatchNormalization()(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Activation('tanh')(x2)\n",
    "#     x2 = tf.keras.layers.Dropout(rate=0.2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, strides=2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Activation('tanh')(x2)\n",
    "#     x2 = tf.keras.layers.Dropout(rate=0.2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 3, strides=2)(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.BatchNormalization()(x2)\n",
    "#     x2 = tf.compat.v2.keras.layers.Activation('tanh')(x2)\n",
    "#     x2 = tf.keras.layers.Dropout(rate=0.2)(x2)\n",
    "\n",
    "    \n",
    "#     x = tf.compat.v2.keras.layers.Convolution1D(32, kernel_size = 4, strides = 2)(x)\n",
    "#     x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "#     x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "#     x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "#     x = tf.compat.v2.keras.layers.Convolution1D(32, kernel_size = 4, dilation_rate=5)(x)\n",
    "#     x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "#     x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "#     x = (CuDNNLSTM(16, return_sequences = True, return_state = False))(x)\n",
    "#     x, slf_attn = MultiHeadAttention(n_head=int(80), d_model=300, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "#     avg_pool2 = GlobalAveragePooling1D()(x2)\n",
    "#     max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    \n",
    "    x = Dense(300)(concatenate([max_pool, avg_pool\n",
    "                                #, max_pool2, avg_pool2\n",
    "                               ]\n",
    "                                ))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(600)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(1200)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(600)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(300)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(130)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(70)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(35)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    \n",
    "    x = Dense(15)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)    \n",
    "    \n",
    "    x = Dense(7)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    \n",
    "    tailInput = Input(shape=(input2Length,))\n",
    "    tailLayers = Dense(input2Length, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(input2Length*2, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(input2Length*2*2, activation='tanh')(tailInput)\n",
    "    tailLayers = BatchNormalization()(tailInput)\n",
    "    tailLayers = Dropout(0.2)(tailInput)\n",
    "    tailLayers = Dense(input2Length*2*2*2, activation='tanh')(tailInput)\n",
    "    tailLayers = BatchNormalization()(tailInput)\n",
    "    tailLayers = Dropout(0.2)(tailInput)\n",
    "    tailLayers = Dense(input2Length*2*2, activation='tanh')(tailInput)    \n",
    "    tailLayers = Dense(input2Length*2, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(3, activation='tanh')(tailInput)\n",
    "    \n",
    "#     concat = concatenate([avg_pool, max_pool, tailLayers])\n",
    "    concat = concatenate([x, tailLayers])\n",
    "\n",
    "    y = Dense(3,activation = 'softmax')(concat)\n",
    "    \n",
    "\n",
    "    return Model(inputs = [i, tailInput], outputs = [y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CnnTransformerModel()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.090, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, \n",
    "                          loss='categorical_crossentropy'\n",
    "#                           loss_weights=[1., 1.]\n",
    "#                           loss_weights=[1.]\n",
    "#                             , metrics=['accuracy']\n",
    "                           , metrics=['accuracy' \n",
    "#                                       , tfa.metrics.MultiLabelConfusionMatrix(num_classes=3)\n",
    "#                                       , Recall(class_id=0)\n",
    "#                                 , Recall(class_id=2)\n",
    "#                                   , Precision(class_id=0)\n",
    "#                                  , Precision(class_id=2)\n",
    "                                 ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing The Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes = True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bed51171db8bf8c5e1731119fd31205bfd80a82"
   },
   "source": [
    "Train the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "e1061c7cde0687450300f516735aad0cc5dbf08f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 21:58:13.151742 14480 callbacks.py:2207] `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "      2/Unknown - 0s 0s/step - loss: 0.5783 - accuracy: 0.67 - 1s 736ms/step - loss: 0.5715 - accuracy: 0.6957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 21:59:34.703655 14480 callbacks.py:307] Method (on_train_batch_end) is slow compared to the batch update (0.686321). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18/Unknown - 2s 514ms/step - loss: 0.5731 - accuracy: 0.706 - 2s 400ms/step - loss: 0.5830 - accuracy: 0.700 - 2s 333ms/step - loss: 0.5855 - accuracy: 0.695 - 2s 288ms/step - loss: 0.5908 - accuracy: 0.690 - 2s 255ms/step - loss: 0.5891 - accuracy: 0.689 - 2s 231ms/step - loss: 0.5786 - accuracy: 0.696 - 2s 212ms/step - loss: 0.5841 - accuracy: 0.693 - 2s 197ms/step - loss: 0.5830 - accuracy: 0.695 - 2s 185ms/step - loss: 0.5863 - accuracy: 0.694 - 2s 174ms/step - loss: 0.5826 - accuracy: 0.697 - 2s 166ms/step - loss: 0.5779 - accuracy: 0.700 - 2s 158ms/step - loss: 0.5763 - accuracy: 0.701 - 2s 152ms/step - loss: 0.5793 - accuracy: 0.699 - 2s 146ms/step - loss: 0.5807 - accuracy: 0.697 - 2s 141ms/step - loss: 0.5827 - accuracy: 0.696 - 2s 136ms/step - loss: 0.5852 - accuracy: 0.6946\n",
      "Epoch 00001: val_loss improved from inf to 0.76618, saving model to xau_usd_cross-360p-high2Tp1Cl100VpConc.0.77.hdf5\n",
      "18/18 [==============================] - 3s 165ms/step - loss: 0.5852 - accuracy: 0.6946 - val_loss: 0.7662 - val_accuracy: 0.4923 - lr: 0.0010\n",
      "Epoch 2/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.70 - ETA: 0s - loss: 0.5574 - accuracy: 0.71 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5834 - accuracy: 0.6949\n",
      "Epoch 00002: val_loss did not improve from 0.76618\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5834 - accuracy: 0.6949 - val_loss: 0.7840 - val_accuracy: 0.4808 - lr: 0.0010\n",
      "Epoch 3/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.71 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5857 - accuracy: 0.69 - ETA: 0s - loss: 0.5853 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5809 - accuracy: 0.70 - ETA: 0s - loss: 0.5812 - accuracy: 0.70 - ETA: 0s - loss: 0.5841 - accuracy: 0.69 - ETA: 0s - loss: 0.5814 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.6931\n",
      "Epoch 00003: val_loss improved from 0.76618 to 0.76522, saving model to xau_usd_cross-360p-high2Tp1Cl100VpConc.0.77.hdf5\n",
      "18/18 [==============================] - 1s 70ms/step - loss: 0.5846 - accuracy: 0.6931 - val_loss: 0.7652 - val_accuracy: 0.4864 - lr: 0.0010\n",
      "Epoch 4/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5800 - accuracy: 0.68 - ETA: 0s - loss: 0.5725 - accuracy: 0.69 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5866 - accuracy: 0.69 - ETA: 0s - loss: 0.5879 - accuracy: 0.69 - ETA: 0s - loss: 0.5914 - accuracy: 0.68 - ETA: 0s - loss: 0.5858 - accuracy: 0.68 - ETA: 0s - loss: 0.5814 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5744 - accuracy: 0.69 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5852 - accuracy: 0.6909\n",
      "Epoch 00004: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5852 - accuracy: 0.6909 - val_loss: 0.7741 - val_accuracy: 0.4754 - lr: 0.0010\n",
      "Epoch 5/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5782 - accuracy: 0.68 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5923 - accuracy: 0.68 - ETA: 0s - loss: 0.5938 - accuracy: 0.68 - ETA: 0s - loss: 0.5876 - accuracy: 0.68 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5836 - accuracy: 0.69 - ETA: 0s - loss: 0.5857 - accuracy: 0.69 - ETA: 0s - loss: 0.5835 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5855 - accuracy: 0.69 - ETA: 0s - loss: 0.5870 - accuracy: 0.6918\n",
      "Epoch 00005: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5870 - accuracy: 0.6918 - val_loss: 0.7785 - val_accuracy: 0.4800 - lr: 0.0010\n",
      "Epoch 6/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.69 - ETA: 0s - loss: 0.5605 - accuracy: 0.70 - ETA: 0s - loss: 0.5577 - accuracy: 0.71 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5835 - accuracy: 0.6947\n",
      "Epoch 00006: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5835 - accuracy: 0.6947 - val_loss: 0.7895 - val_accuracy: 0.4751 - lr: 0.0010\n",
      "Epoch 7/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5660 - accuracy: 0.69 - ETA: 0s - loss: 0.5636 - accuracy: 0.70 - ETA: 0s - loss: 0.5647 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5848 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5710 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.69 - ETA: 0s - loss: 0.5801 - accuracy: 0.69 - ETA: 0s - loss: 0.5750 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.6946\n",
      "Epoch 00007: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5812 - accuracy: 0.6938 - val_loss: 0.7886 - val_accuracy: 0.4762 - lr: 0.0010\n",
      "Epoch 8/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.70 - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5653 - accuracy: 0.71 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.70 - ETA: 0s - loss: 0.5855 - accuracy: 0.69 - ETA: 0s - loss: 0.5852 - accuracy: 0.69 - ETA: 0s - loss: 0.5851 - accuracy: 0.69 - ETA: 0s - loss: 0.5842 - accuracy: 0.69 - ETA: 0s - loss: 0.5833 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5849 - accuracy: 0.6922\n",
      "Epoch 00008: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5849 - accuracy: 0.6922 - val_loss: 0.7688 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 9/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.69 - ETA: 0s - loss: 0.5645 - accuracy: 0.70 - ETA: 0s - loss: 0.5663 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.69 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.69 - ETA: 0s - loss: 0.5775 - accuracy: 0.69 - ETA: 0s - loss: 0.5808 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5841 - accuracy: 0.6900\n",
      "Epoch 00009: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5841 - accuracy: 0.6900 - val_loss: 0.7889 - val_accuracy: 0.4819 - lr: 0.0010\n",
      "Epoch 10/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.71 - ETA: 0s - loss: 0.5521 - accuracy: 0.71 - ETA: 0s - loss: 0.5568 - accuracy: 0.71 - ETA: 0s - loss: 0.5674 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.69 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5814 - accuracy: 0.6957\n",
      "Epoch 00010: val_loss did not improve from 0.76522\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5814 - accuracy: 0.6957 - val_loss: 0.7883 - val_accuracy: 0.4822 - lr: 0.0010\n",
      "Epoch 11/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.72 - ETA: 0s - loss: 0.5527 - accuracy: 0.71 - ETA: 0s - loss: 0.5544 - accuracy: 0.71 - ETA: 0s - loss: 0.5758 - accuracy: 0.69 - ETA: 0s - loss: 0.5833 - accuracy: 0.68 - ETA: 0s - loss: 0.5797 - accuracy: 0.68 - ETA: 0s - loss: 0.5690 - accuracy: 0.69 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.69 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.6947\n",
      "Epoch 00011: val_loss improved from 0.76522 to 0.76463, saving model to xau_usd_cross-360p-high2Tp1Cl100VpConc.0.76.hdf5\n",
      "18/18 [==============================] - 1s 70ms/step - loss: 0.5789 - accuracy: 0.6947 - val_loss: 0.7646 - val_accuracy: 0.4822 - lr: 0.0010\n",
      "Epoch 12/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.69 - ETA: 0s - loss: 0.5612 - accuracy: 0.70 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5859 - accuracy: 0.69 - ETA: 0s - loss: 0.5872 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5840 - accuracy: 0.69 - ETA: 0s - loss: 0.5863 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.6949\n",
      "Epoch 00012: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5825 - accuracy: 0.6949 - val_loss: 0.7659 - val_accuracy: 0.4839 - lr: 0.0010\n",
      "Epoch 13/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.70 - ETA: 0s - loss: 0.5572 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.71 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.69 - ETA: 0s - loss: 0.5742 - accuracy: 0.69 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.6937\n",
      "Epoch 00013: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5824 - accuracy: 0.6937 - val_loss: 0.7687 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 14/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.71 - ETA: 0s - loss: 0.5662 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.71 - ETA: 0s - loss: 0.5774 - accuracy: 0.71 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5836 - accuracy: 0.69 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5809 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5839 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5811 - accuracy: 0.69 - ETA: 0s - loss: 0.5827 - accuracy: 0.69 - ETA: 0s - loss: 0.5854 - accuracy: 0.6927\n",
      "Epoch 00014: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5854 - accuracy: 0.6927 - val_loss: 0.7685 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 15/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5637 - accuracy: 0.72 - ETA: 0s - loss: 0.5606 - accuracy: 0.72 - ETA: 0s - loss: 0.5770 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.71 - ETA: 0s - loss: 0.5814 - accuracy: 0.70 - ETA: 0s - loss: 0.5689 - accuracy: 0.71 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5823 - accuracy: 0.6956\n",
      "Epoch 00015: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 0.5823 - accuracy: 0.6956 - val_loss: 0.7738 - val_accuracy: 0.4804 - lr: 0.0010\n",
      "Epoch 16/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.67 - ETA: 0s - loss: 0.5634 - accuracy: 0.68 - ETA: 0s - loss: 0.5658 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.68 - ETA: 0s - loss: 0.5702 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5737 - accuracy: 0.69 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5816 - accuracy: 0.6915\n",
      "Epoch 00016: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 0.5816 - accuracy: 0.6915 - val_loss: 0.7754 - val_accuracy: 0.4809 - lr: 0.0010\n",
      "Epoch 17/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.69 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5875 - accuracy: 0.69 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5735 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5839 - accuracy: 0.6941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5839 - accuracy: 0.6941 - val_loss: 0.7657 - val_accuracy: 0.4923 - lr: 0.0010\n",
      "Epoch 18/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.70 - ETA: 0s - loss: 0.5474 - accuracy: 0.71 - ETA: 0s - loss: 0.5547 - accuracy: 0.71 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5666 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.6983\n",
      "Epoch 00018: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5794 - accuracy: 0.6983 - val_loss: 0.7821 - val_accuracy: 0.4837 - lr: 0.0010\n",
      "Epoch 19/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5571 - accuracy: 0.72 - ETA: 0s - loss: 0.5625 - accuracy: 0.71 - ETA: 0s - loss: 0.5652 - accuracy: 0.71 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5752 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5775 - accuracy: 0.69 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.6948\n",
      "Epoch 00019: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5806 - accuracy: 0.6948 - val_loss: 0.7694 - val_accuracy: 0.4846 - lr: 0.0010\n",
      "Epoch 20/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.73 - ETA: 0s - loss: 0.5573 - accuracy: 0.71 - ETA: 0s - loss: 0.5591 - accuracy: 0.71 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5798 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.6991\n",
      "Epoch 00020: val_loss did not improve from 0.76463\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5799 - accuracy: 0.6991 - val_loss: 0.7750 - val_accuracy: 0.4841 - lr: 0.0010\n",
      "Epoch 21/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.72 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5632 - accuracy: 0.71 - ETA: 0s - loss: 0.5754 - accuracy: 0.71 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5836 - accuracy: 0.70 - ETA: 0s - loss: 0.5829 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5787 - accuracy: 0.70 - ETA: 0s - loss: 0.5801 - accuracy: 0.70 - ETA: 0s - loss: 0.5834 - accuracy: 0.69 - ETA: 0s - loss: 0.5836 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5806 - accuracy: 0.70 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5862 - accuracy: 0.6969\n",
      "Epoch 00021: val_loss improved from 0.76463 to 0.75113, saving model to xau_usd_cross-360p-high2Tp1Cl100VpConc.0.75.hdf5\n",
      "18/18 [==============================] - 1s 70ms/step - loss: 0.5862 - accuracy: 0.6969 - val_loss: 0.7511 - val_accuracy: 0.5060 - lr: 0.0010\n",
      "Epoch 22/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.68 - ETA: 0s - loss: 0.5665 - accuracy: 0.69 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5864 - accuracy: 0.69 - ETA: 0s - loss: 0.5915 - accuracy: 0.69 - ETA: 0s - loss: 0.5865 - accuracy: 0.69 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5827 - accuracy: 0.69 - ETA: 0s - loss: 0.5837 - accuracy: 0.69 - ETA: 0s - loss: 0.5870 - accuracy: 0.69 - ETA: 0s - loss: 0.5852 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5866 - accuracy: 0.69 - ETA: 0s - loss: 0.5881 - accuracy: 0.6904\n",
      "Epoch 00022: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5881 - accuracy: 0.6904 - val_loss: 0.7527 - val_accuracy: 0.4969 - lr: 0.0010\n",
      "Epoch 23/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5624 - accuracy: 0.71 - ETA: 0s - loss: 0.5662 - accuracy: 0.70 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5878 - accuracy: 0.68 - ETA: 0s - loss: 0.5877 - accuracy: 0.68 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5864 - accuracy: 0.68 - ETA: 0s - loss: 0.5883 - accuracy: 0.68 - ETA: 0s - loss: 0.5905 - accuracy: 0.68 - ETA: 0s - loss: 0.5882 - accuracy: 0.68 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5856 - accuracy: 0.69 - ETA: 0s - loss: 0.5892 - accuracy: 0.6887\n",
      "Epoch 00023: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 0.5892 - accuracy: 0.6887 - val_loss: 0.7797 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 24/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.71 - ETA: 0s - loss: 0.5744 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5839 - accuracy: 0.70 - ETA: 0s - loss: 0.5851 - accuracy: 0.70 - ETA: 0s - loss: 0.5887 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5854 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5844 - accuracy: 0.69 - ETA: 0s - loss: 0.5872 - accuracy: 0.6919\n",
      "Epoch 00024: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5872 - accuracy: 0.6919 - val_loss: 0.7798 - val_accuracy: 0.4786 - lr: 0.0010\n",
      "Epoch 25/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.69 - ETA: 0s - loss: 0.5589 - accuracy: 0.70 - ETA: 0s - loss: 0.5580 - accuracy: 0.71 - ETA: 0s - loss: 0.5686 - accuracy: 0.71 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5693 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.6977\n",
      "Epoch 00025: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5795 - accuracy: 0.6977 - val_loss: 0.7751 - val_accuracy: 0.4857 - lr: 0.0010\n",
      "Epoch 26/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.71 - ETA: 0s - loss: 0.5867 - accuracy: 0.70 - ETA: 0s - loss: 0.5826 - accuracy: 0.70 - ETA: 0s - loss: 0.5890 - accuracy: 0.69 - ETA: 0s - loss: 0.5850 - accuracy: 0.69 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5847 - accuracy: 0.6949\n",
      "Epoch 00026: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5847 - accuracy: 0.6949 - val_loss: 0.7682 - val_accuracy: 0.4897 - lr: 0.0010\n",
      "Epoch 27/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5576 - accuracy: 0.70 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5840 - accuracy: 0.69 - ETA: 0s - loss: 0.5882 - accuracy: 0.69 - ETA: 0s - loss: 0.5862 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5856 - accuracy: 0.69 - ETA: 0s - loss: 0.5837 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5850 - accuracy: 0.69 - ETA: 0s - loss: 0.5879 - accuracy: 0.6914\n",
      "Epoch 00027: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5879 - accuracy: 0.6914 - val_loss: 0.7738 - val_accuracy: 0.4866 - lr: 0.0010\n",
      "Epoch 28/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.71 - ETA: 0s - loss: 0.5517 - accuracy: 0.70 - ETA: 0s - loss: 0.5566 - accuracy: 0.71 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5814 - accuracy: 0.70 - ETA: 0s - loss: 0.5789 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.71 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5839 - accuracy: 0.6966\n",
      "Epoch 00028: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5839 - accuracy: 0.6966 - val_loss: 0.7830 - val_accuracy: 0.4885 - lr: 0.0010\n",
      "Epoch 29/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5880 - accuracy: 0.69 - ETA: 0s - loss: 0.5865 - accuracy: 0.69 - ETA: 0s - loss: 0.5902 - accuracy: 0.69 - ETA: 0s - loss: 0.5877 - accuracy: 0.68 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5865 - accuracy: 0.69 - ETA: 0s - loss: 0.5841 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.6943\n",
      "Epoch 00029: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5843 - accuracy: 0.6943 - val_loss: 0.7944 - val_accuracy: 0.4852 - lr: 0.0010\n",
      "Epoch 30/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.69 - ETA: 0s - loss: 0.5695 - accuracy: 0.69 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5809 - accuracy: 0.70 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5851 - accuracy: 0.69 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5816 - accuracy: 0.69 - ETA: 0s - loss: 0.5840 - accuracy: 0.69 - ETA: 0s - loss: 0.5816 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5844 - accuracy: 0.6935\n",
      "Epoch 00030: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5844 - accuracy: 0.6935 - val_loss: 0.7881 - val_accuracy: 0.4870 - lr: 0.0010\n",
      "Epoch 31/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.70 - ETA: 0s - loss: 0.5798 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.71 - ETA: 0s - loss: 0.5888 - accuracy: 0.69 - ETA: 0s - loss: 0.6006 - accuracy: 0.68 - ETA: 0s - loss: 0.5962 - accuracy: 0.68 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5885 - accuracy: 0.69 - ETA: 0s - loss: 0.5901 - accuracy: 0.69 - ETA: 0s - loss: 0.5883 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5845 - accuracy: 0.69 - ETA: 0s - loss: 0.5856 - accuracy: 0.69 - ETA: 0s - loss: 0.5885 - accuracy: 0.68 - ETA: 0s - loss: 0.5904 - accuracy: 0.6865\n",
      "Epoch 00031: val_loss did not improve from 0.75113\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5904 - accuracy: 0.6865 - val_loss: 0.7993 - val_accuracy: 0.4740 - lr: 0.0010\n",
      "Epoch 32/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5905 - accuracy: 0.69 - ETA: 0s - loss: 0.5892 - accuracy: 0.69 - ETA: 0s - loss: 0.5949 - accuracy: 0.68 - ETA: 0s - loss: 0.5933 - accuracy: 0.68 - ETA: 0s - loss: 0.5852 - accuracy: 0.69 - ETA: 0s - loss: 0.5901 - accuracy: 0.68 - ETA: 0s - loss: 0.5899 - accuracy: 0.68 - ETA: 0s - loss: 0.5932 - accuracy: 0.68 - ETA: 0s - loss: 0.5918 - accuracy: 0.69 - ETA: 0s - loss: 0.5885 - accuracy: 0.69 - ETA: 0s - loss: 0.5870 - accuracy: 0.69 - ETA: 0s - loss: 0.5891 - accuracy: 0.69 - ETA: 0s - loss: 0.5908 - accuracy: 0.68 - ETA: 0s - loss: 0.5921 - accuracy: 0.69 - ETA: 0s - loss: 0.5931 - accuracy: 0.6891\n",
      "Epoch 00032: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5931 - accuracy: 0.6891 - val_loss: 0.7550 - val_accuracy: 0.5077 - lr: 0.0010\n",
      "Epoch 33/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.71 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5689 - accuracy: 0.71 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.70 - ETA: 0s - loss: 0.5880 - accuracy: 0.69 - ETA: 0s - loss: 0.5846 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5837 - accuracy: 0.69 - ETA: 0s - loss: 0.5845 - accuracy: 0.69 - ETA: 0s - loss: 0.5871 - accuracy: 0.69 - ETA: 0s - loss: 0.5841 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5823 - accuracy: 0.69 - ETA: 0s - loss: 0.5860 - accuracy: 0.69 - ETA: 0s - loss: 0.5876 - accuracy: 0.6918\n",
      "Epoch 00033: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5876 - accuracy: 0.6918 - val_loss: 0.7617 - val_accuracy: 0.4756 - lr: 0.0010\n",
      "Epoch 34/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.71 - ETA: 0s - loss: 0.5647 - accuracy: 0.70 - ETA: 0s - loss: 0.5616 - accuracy: 0.71 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5801 - accuracy: 0.70 - ETA: 0s - loss: 0.5989 - accuracy: 0.68 - ETA: 0s - loss: 0.5934 - accuracy: 0.68 - ETA: 0s - loss: 0.5852 - accuracy: 0.69 - ETA: 0s - loss: 0.5908 - accuracy: 0.68 - ETA: 0s - loss: 0.5899 - accuracy: 0.68 - ETA: 0s - loss: 0.5922 - accuracy: 0.68 - ETA: 0s - loss: 0.5871 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5836 - accuracy: 0.69 - ETA: 0s - loss: 0.5859 - accuracy: 0.69 - ETA: 0s - loss: 0.5882 - accuracy: 0.6917\n",
      "Epoch 00034: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5882 - accuracy: 0.6917 - val_loss: 0.7777 - val_accuracy: 0.4870 - lr: 0.0010\n",
      "Epoch 35/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5547 - accuracy: 0.68 - ETA: 0s - loss: 0.5632 - accuracy: 0.68 - ETA: 0s - loss: 0.5675 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.68 - ETA: 0s - loss: 0.5835 - accuracy: 0.68 - ETA: 0s - loss: 0.5874 - accuracy: 0.68 - ETA: 0s - loss: 0.5858 - accuracy: 0.68 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5814 - accuracy: 0.69 - ETA: 0s - loss: 0.5855 - accuracy: 0.68 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5754 - accuracy: 0.69 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.68 - ETA: 0s - loss: 0.5826 - accuracy: 0.68 - ETA: 0s - loss: 0.5845 - accuracy: 0.6880\n",
      "Epoch 00035: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5845 - accuracy: 0.6880 - val_loss: 0.7638 - val_accuracy: 0.4747 - lr: 0.0010\n",
      "Epoch 36/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.68 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.70 - ETA: 0s - loss: 0.5872 - accuracy: 0.69 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.6976\n",
      "Epoch 00036: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5806 - accuracy: 0.6976 - val_loss: 0.7673 - val_accuracy: 0.4776 - lr: 0.0010\n",
      "Epoch 37/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.69 - ETA: 0s - loss: 0.5580 - accuracy: 0.70 - ETA: 0s - loss: 0.5666 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5830 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.6957\n",
      "Epoch 00037: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5804 - accuracy: 0.6957 - val_loss: 0.7837 - val_accuracy: 0.4831 - lr: 0.0010\n",
      "Epoch 38/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.69 - ETA: 0s - loss: 0.5598 - accuracy: 0.70 - ETA: 0s - loss: 0.5623 - accuracy: 0.71 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.6968\n",
      "Epoch 00038: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5821 - accuracy: 0.6968 - val_loss: 0.7657 - val_accuracy: 0.4789 - lr: 0.0010\n",
      "Epoch 39/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.69 - ETA: 0s - loss: 0.5537 - accuracy: 0.70 - ETA: 0s - loss: 0.5626 - accuracy: 0.71 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5854 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5837 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5775 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5830 - accuracy: 0.6917\n",
      "Epoch 00039: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5830 - accuracy: 0.6917 - val_loss: 0.7843 - val_accuracy: 0.4857 - lr: 0.0010\n",
      "Epoch 40/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.69 - ETA: 0s - loss: 0.5669 - accuracy: 0.71 - ETA: 0s - loss: 0.5782 - accuracy: 0.70 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5850 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.6965\n",
      "Epoch 00040: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5825 - accuracy: 0.6965 - val_loss: 0.7924 - val_accuracy: 0.4820 - lr: 0.0010\n",
      "Epoch 41/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.71 - ETA: 0s - loss: 0.5703 - accuracy: 0.69 - ETA: 0s - loss: 0.5665 - accuracy: 0.70 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5816 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5844 - accuracy: 0.6915\n",
      "Epoch 00041: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5844 - accuracy: 0.6915 - val_loss: 0.7873 - val_accuracy: 0.4872 - lr: 0.0010\n",
      "Epoch 42/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.69 - ETA: 0s - loss: 0.5611 - accuracy: 0.70 - ETA: 0s - loss: 0.5621 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5676 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5791 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.70 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.6975\n",
      "Epoch 00042: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5826 - accuracy: 0.6975 - val_loss: 0.7690 - val_accuracy: 0.4859 - lr: 0.0010\n",
      "Epoch 43/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.68 - ETA: 0s - loss: 0.5537 - accuracy: 0.70 - ETA: 0s - loss: 0.5643 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.6965\n",
      "Epoch 00043: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5806 - accuracy: 0.6965 - val_loss: 0.7872 - val_accuracy: 0.4886 - lr: 0.0010\n",
      "Epoch 44/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.70 - ETA: 0s - loss: 0.5641 - accuracy: 0.70 - ETA: 0s - loss: 0.5624 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.6975\n",
      "Epoch 00044: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5807 - accuracy: 0.6975 - val_loss: 0.7739 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 45/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.71 - ETA: 0s - loss: 0.5790 - accuracy: 0.70 - ETA: 0s - loss: 0.5807 - accuracy: 0.70 - ETA: 0s - loss: 0.5842 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.6951\n",
      "Epoch 00045: val_loss did not improve from 0.75113\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5820 - accuracy: 0.6951 - val_loss: 0.7635 - val_accuracy: 0.4864 - lr: 0.0010\n",
      "Epoch 46/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.69 - ETA: 0s - loss: 0.5598 - accuracy: 0.70 - ETA: 0s - loss: 0.5587 - accuracy: 0.71 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5681 - accuracy: 0.69 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5754 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.6947\n",
      "Epoch 00046: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5803 - accuracy: 0.6947 - val_loss: 0.7853 - val_accuracy: 0.4892 - lr: 0.0010\n",
      "Epoch 47/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.68 - ETA: 0s - loss: 0.5634 - accuracy: 0.69 - ETA: 0s - loss: 0.5627 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5860 - accuracy: 0.68 - ETA: 0s - loss: 0.5838 - accuracy: 0.68 - ETA: 0s - loss: 0.5745 - accuracy: 0.69 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.6926\n",
      "Epoch 00047: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5810 - accuracy: 0.6926 - val_loss: 0.7707 - val_accuracy: 0.4875 - lr: 0.0010\n",
      "Epoch 48/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5727 - accuracy: 0.69 - ETA: 0s - loss: 0.5678 - accuracy: 0.70 - ETA: 0s - loss: 0.5658 - accuracy: 0.71 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5811 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5814 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.69 - ETA: 0s - loss: 0.5855 - accuracy: 0.6924\n",
      "Epoch 00048: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5855 - accuracy: 0.6924 - val_loss: 0.7684 - val_accuracy: 0.4819 - lr: 0.0010\n",
      "Epoch 49/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.69 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5855 - accuracy: 0.69 - ETA: 0s - loss: 0.5832 - accuracy: 0.68 - ETA: 0s - loss: 0.5732 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5827 - accuracy: 0.6909\n",
      "Epoch 00049: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5827 - accuracy: 0.6909 - val_loss: 0.7712 - val_accuracy: 0.4833 - lr: 0.0010\n",
      "Epoch 50/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.72 - ETA: 0s - loss: 0.5604 - accuracy: 0.71 - ETA: 0s - loss: 0.5580 - accuracy: 0.71 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5840 - accuracy: 0.6943\n",
      "Epoch 00050: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5840 - accuracy: 0.6943 - val_loss: 0.7681 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 51/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5567 - accuracy: 0.70 - ETA: 0s - loss: 0.5674 - accuracy: 0.69 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5827 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.69 - ETA: 0s - loss: 0.5756 - accuracy: 0.69 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5853 - accuracy: 0.6940\n",
      "Epoch 00051: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5853 - accuracy: 0.6940 - val_loss: 0.7761 - val_accuracy: 0.4903 - lr: 0.0010\n",
      "Epoch 52/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.68 - ETA: 0s - loss: 0.5616 - accuracy: 0.69 - ETA: 0s - loss: 0.5644 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.68 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5833 - accuracy: 0.6922\n",
      "Epoch 00052: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5833 - accuracy: 0.6922 - val_loss: 0.7768 - val_accuracy: 0.4850 - lr: 0.0010\n",
      "Epoch 53/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.68 - ETA: 0s - loss: 0.5561 - accuracy: 0.70 - ETA: 0s - loss: 0.5527 - accuracy: 0.71 - ETA: 0s - loss: 0.5685 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.6935\n",
      "Epoch 00053: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5806 - accuracy: 0.6935 - val_loss: 0.7797 - val_accuracy: 0.4809 - lr: 0.0010\n",
      "Epoch 54/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.72 - ETA: 0s - loss: 0.5621 - accuracy: 0.71 - ETA: 0s - loss: 0.5666 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.69 - ETA: 0s - loss: 0.5850 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5801 - accuracy: 0.69 - ETA: 0s - loss: 0.5816 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.6922\n",
      "Epoch 00054: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5838 - accuracy: 0.6922 - val_loss: 0.7835 - val_accuracy: 0.4826 - lr: 0.0010\n",
      "Epoch 55/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5559 - accuracy: 0.71 - ETA: 0s - loss: 0.5620 - accuracy: 0.71 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.69 - ETA: 0s - loss: 0.5784 - accuracy: 0.6979\n",
      "Epoch 00055: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5784 - accuracy: 0.6979 - val_loss: 0.7924 - val_accuracy: 0.4868 - lr: 0.0010\n",
      "Epoch 56/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5505 - accuracy: 0.70 - ETA: 0s - loss: 0.5613 - accuracy: 0.71 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5790 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5681 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.6966\n",
      "Epoch 00056: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5802 - accuracy: 0.6966 - val_loss: 0.7930 - val_accuracy: 0.4918 - lr: 0.0010\n",
      "Epoch 57/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.71 - ETA: 0s - loss: 0.5787 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5662 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.7001\n",
      "Epoch 00057: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5797 - accuracy: 0.7001 - val_loss: 0.7972 - val_accuracy: 0.4835 - lr: 0.0010\n",
      "Epoch 58/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.69 - ETA: 0s - loss: 0.5571 - accuracy: 0.70 - ETA: 0s - loss: 0.5566 - accuracy: 0.71 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5668 - accuracy: 0.71 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5789 - accuracy: 0.6988\n",
      "Epoch 00058: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5789 - accuracy: 0.6988 - val_loss: 0.7932 - val_accuracy: 0.4793 - lr: 0.0010\n",
      "Epoch 59/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5583 - accuracy: 0.70 - ETA: 0s - loss: 0.5555 - accuracy: 0.70 - ETA: 0s - loss: 0.5593 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.69 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.6952\n",
      "Epoch 00059: val_loss did not improve from 0.75113\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5824 - accuracy: 0.6952 - val_loss: 0.7837 - val_accuracy: 0.4868 - lr: 0.0010\n",
      "Epoch 60/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.70 - ETA: 0s - loss: 0.5548 - accuracy: 0.71 - ETA: 0s - loss: 0.5577 - accuracy: 0.71 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5808 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.6948\n",
      "Epoch 00060: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5817 - accuracy: 0.6948 - val_loss: 0.7673 - val_accuracy: 0.4778 - lr: 0.0010\n",
      "Epoch 61/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.71 - ETA: 0s - loss: 0.5424 - accuracy: 0.71 - ETA: 0s - loss: 0.5485 - accuracy: 0.72 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.6949\n",
      "Epoch 00061: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5813 - accuracy: 0.6949 - val_loss: 0.7618 - val_accuracy: 0.4852 - lr: 0.0010\n",
      "Epoch 62/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5740 - accuracy: 0.69 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5830 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5735 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.6937\n",
      "Epoch 00062: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5842 - accuracy: 0.6923 - val_loss: 0.7947 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 63/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.69 - ETA: 0s - loss: 0.5592 - accuracy: 0.70 - ETA: 0s - loss: 0.5598 - accuracy: 0.71 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5796 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5678 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.6957\n",
      "Epoch 00063: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5815 - accuracy: 0.6957 - val_loss: 0.7685 - val_accuracy: 0.4775 - lr: 0.0010\n",
      "Epoch 64/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.69 - ETA: 0s - loss: 0.5570 - accuracy: 0.70 - ETA: 0s - loss: 0.5582 - accuracy: 0.71 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.6948\n",
      "Epoch 00064: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5812 - accuracy: 0.6948 - val_loss: 0.8093 - val_accuracy: 0.4789 - lr: 0.0010\n",
      "Epoch 65/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5777 - accuracy: 0.67 - ETA: 0s - loss: 0.5637 - accuracy: 0.69 - ETA: 0s - loss: 0.5598 - accuracy: 0.71 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5844 - accuracy: 0.69 - ETA: 0s - loss: 0.5801 - accuracy: 0.69 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5842 - accuracy: 0.69 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5861 - accuracy: 0.6914\n",
      "Epoch 00065: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5861 - accuracy: 0.6914 - val_loss: 0.7701 - val_accuracy: 0.4795 - lr: 0.0010\n",
      "Epoch 66/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.71 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5827 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.70 - ETA: 0s - loss: 0.5872 - accuracy: 0.69 - ETA: 0s - loss: 0.5847 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5847 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5839 - accuracy: 0.6956\n",
      "Epoch 00066: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5839 - accuracy: 0.6956 - val_loss: 0.7677 - val_accuracy: 0.4809 - lr: 0.0010\n",
      "Epoch 67/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.70 - ETA: 0s - loss: 0.5579 - accuracy: 0.71 - ETA: 0s - loss: 0.5630 - accuracy: 0.71 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5656 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.71 - ETA: 0s - loss: 0.5669 - accuracy: 0.71 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.7043\n",
      "Epoch 00067: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5750 - accuracy: 0.7043 - val_loss: 0.7643 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 68/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.70 - ETA: 0s - loss: 0.5583 - accuracy: 0.71 - ETA: 0s - loss: 0.5581 - accuracy: 0.72 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5834 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5789 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.6984\n",
      "Epoch 00068: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5800 - accuracy: 0.6984 - val_loss: 0.7903 - val_accuracy: 0.4745 - lr: 0.0010\n",
      "Epoch 69/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.69 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5865 - accuracy: 0.69 - ETA: 0s - loss: 0.5863 - accuracy: 0.69 - ETA: 0s - loss: 0.5893 - accuracy: 0.69 - ETA: 0s - loss: 0.5845 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5847 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5848 - accuracy: 0.6934\n",
      "Epoch 00069: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5848 - accuracy: 0.6934 - val_loss: 0.7612 - val_accuracy: 0.4789 - lr: 0.0010\n",
      "Epoch 70/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.70 - ETA: 0s - loss: 0.5570 - accuracy: 0.69 - ETA: 0s - loss: 0.5589 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5726 - accuracy: 0.69 - ETA: 0s - loss: 0.5726 - accuracy: 0.69 - ETA: 0s - loss: 0.5755 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.6922\n",
      "Epoch 00070: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5818 - accuracy: 0.6922 - val_loss: 0.7681 - val_accuracy: 0.4852 - lr: 0.0010\n",
      "Epoch 71/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.70 - ETA: 0s - loss: 0.5512 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5812 - accuracy: 0.70 - ETA: 0s - loss: 0.5811 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5820 - accuracy: 0.70 - ETA: 0s - loss: 0.5857 - accuracy: 0.69 - ETA: 0s - loss: 0.5835 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5839 - accuracy: 0.6964\n",
      "Epoch 00071: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5839 - accuracy: 0.6964 - val_loss: 0.7971 - val_accuracy: 0.4853 - lr: 0.0010\n",
      "Epoch 72/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5612 - accuracy: 0.70 - ETA: 0s - loss: 0.5628 - accuracy: 0.71 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5668 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5736 - accuracy: 0.69 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.6953\n",
      "Epoch 00072: val_loss did not improve from 0.75113\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5780 - accuracy: 0.6953 - val_loss: 0.7795 - val_accuracy: 0.4853 - lr: 0.0010\n",
      "Epoch 73/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.69 - ETA: 0s - loss: 0.5540 - accuracy: 0.70 - ETA: 0s - loss: 0.5549 - accuracy: 0.71 - ETA: 0s - loss: 0.5665 - accuracy: 0.70 - ETA: 0s - loss: 0.5693 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.69 - ETA: 0s - loss: 0.5736 - accuracy: 0.69 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.69 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5665 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.7017\n",
      "Epoch 00073: val_loss improved from 0.75113 to 0.74497, saving model to xau_usd_cross-360p-high2Tp1Cl100VpConc.0.74.hdf5\n",
      "18/18 [==============================] - 1s 71ms/step - loss: 0.5757 - accuracy: 0.7017 - val_loss: 0.7450 - val_accuracy: 0.4982 - lr: 0.0010\n",
      "Epoch 74/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5828 - accuracy: 0.70 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5823 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5808 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5856 - accuracy: 0.6934\n",
      "Epoch 00074: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5856 - accuracy: 0.6934 - val_loss: 0.7658 - val_accuracy: 0.4888 - lr: 0.0010\n",
      "Epoch 75/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.71 - ETA: 0s - loss: 0.5504 - accuracy: 0.72 - ETA: 0s - loss: 0.5444 - accuracy: 0.73 - ETA: 0s - loss: 0.5598 - accuracy: 0.72 - ETA: 0s - loss: 0.5659 - accuracy: 0.71 - ETA: 0s - loss: 0.5681 - accuracy: 0.71 - ETA: 0s - loss: 0.5667 - accuracy: 0.71 - ETA: 0s - loss: 0.5594 - accuracy: 0.71 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5649 - accuracy: 0.70 - ETA: 0s - loss: 0.5643 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.6988\n",
      "Epoch 00075: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5759 - accuracy: 0.6988 - val_loss: 0.7929 - val_accuracy: 0.4833 - lr: 0.0010\n",
      "Epoch 76/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.71 - ETA: 0s - loss: 0.5580 - accuracy: 0.69 - ETA: 0s - loss: 0.5752 - accuracy: 0.69 - ETA: 0s - loss: 0.5811 - accuracy: 0.69 - ETA: 0s - loss: 0.5871 - accuracy: 0.68 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5700 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.6954\n",
      "Epoch 00076: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5798 - accuracy: 0.6954 - val_loss: 0.7932 - val_accuracy: 0.4868 - lr: 0.0010\n",
      "Epoch 77/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5830 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5862 - accuracy: 0.69 - ETA: 0s - loss: 0.5843 - accuracy: 0.69 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.69 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.6947\n",
      "Epoch 00077: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5786 - accuracy: 0.6947 - val_loss: 0.8044 - val_accuracy: 0.4831 - lr: 0.0010\n",
      "Epoch 78/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.69 - ETA: 0s - loss: 0.5613 - accuracy: 0.70 - ETA: 0s - loss: 0.5593 - accuracy: 0.71 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.70 - ETA: 0s - loss: 0.5843 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.69 - ETA: 0s - loss: 0.5683 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.6968\n",
      "Epoch 00078: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5805 - accuracy: 0.6968 - val_loss: 0.8164 - val_accuracy: 0.4782 - lr: 0.0010\n",
      "Epoch 79/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.71 - ETA: 0s - loss: 0.5566 - accuracy: 0.71 - ETA: 0s - loss: 0.5558 - accuracy: 0.72 - ETA: 0s - loss: 0.5696 - accuracy: 0.71 - ETA: 0s - loss: 0.5702 - accuracy: 0.71 - ETA: 0s - loss: 0.5764 - accuracy: 0.71 - ETA: 0s - loss: 0.5719 - accuracy: 0.71 - ETA: 0s - loss: 0.5641 - accuracy: 0.71 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.6976\n",
      "Epoch 00079: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5799 - accuracy: 0.6976 - val_loss: 0.8124 - val_accuracy: 0.4806 - lr: 0.0010\n",
      "Epoch 80/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5684 - accuracy: 0.69 - ETA: 0s - loss: 0.5576 - accuracy: 0.70 - ETA: 0s - loss: 0.5593 - accuracy: 0.71 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5634 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.6979\n",
      "Epoch 00080: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5790 - accuracy: 0.6979 - val_loss: 0.8128 - val_accuracy: 0.4824 - lr: 0.0010\n",
      "Epoch 81/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.69 - ETA: 0s - loss: 0.5589 - accuracy: 0.70 - ETA: 0s - loss: 0.5560 - accuracy: 0.71 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5607 - accuracy: 0.71 - ETA: 0s - loss: 0.5677 - accuracy: 0.71 - ETA: 0s - loss: 0.5697 - accuracy: 0.71 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.71 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.7006\n",
      "Epoch 00081: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5785 - accuracy: 0.7006 - val_loss: 0.8146 - val_accuracy: 0.4853 - lr: 0.0010\n",
      "Epoch 82/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.68 - ETA: 0s - loss: 0.5732 - accuracy: 0.68 - ETA: 0s - loss: 0.5722 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5848 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.6959\n",
      "Epoch 00082: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5799 - accuracy: 0.6959 - val_loss: 0.8130 - val_accuracy: 0.4791 - lr: 0.0010\n",
      "Epoch 83/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.69 - ETA: 0s - loss: 0.5613 - accuracy: 0.70 - ETA: 0s - loss: 0.5635 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5806 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5742 - accuracy: 0.69 - ETA: 0s - loss: 0.5723 - accuracy: 0.69 - ETA: 0s - loss: 0.5748 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5810 - accuracy: 0.6940\n",
      "Epoch 00083: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5810 - accuracy: 0.6940 - val_loss: 0.8093 - val_accuracy: 0.4780 - lr: 0.0010\n",
      "Epoch 84/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.71 - ETA: 0s - loss: 0.5679 - accuracy: 0.71 - ETA: 0s - loss: 0.5683 - accuracy: 0.71 - ETA: 0s - loss: 0.5815 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5861 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.6979\n",
      "Epoch 00084: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5758 - accuracy: 0.6979 - val_loss: 0.7988 - val_accuracy: 0.4841 - lr: 0.0010\n",
      "Epoch 85/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5445 - accuracy: 0.72 - ETA: 0s - loss: 0.5492 - accuracy: 0.72 - ETA: 0s - loss: 0.5553 - accuracy: 0.72 - ETA: 0s - loss: 0.5685 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.71 - ETA: 0s - loss: 0.5781 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5656 - accuracy: 0.71 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.7001\n",
      "Epoch 00085: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5779 - accuracy: 0.7001 - val_loss: 0.8028 - val_accuracy: 0.4754 - lr: 0.0010\n",
      "Epoch 86/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5503 - accuracy: 0.72 - ETA: 0s - loss: 0.5591 - accuracy: 0.71 - ETA: 0s - loss: 0.5579 - accuracy: 0.71 - ETA: 0s - loss: 0.5676 - accuracy: 0.71 - ETA: 0s - loss: 0.5728 - accuracy: 0.71 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5676 - accuracy: 0.71 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.7021\n",
      "Epoch 00086: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5764 - accuracy: 0.7006 - val_loss: 0.7772 - val_accuracy: 0.4960 - lr: 0.0010\n",
      "Epoch 87/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.71 - ETA: 0s - loss: 0.5646 - accuracy: 0.71 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.71 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5658 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.6995\n",
      "Epoch 00087: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5752 - accuracy: 0.6995 - val_loss: 0.7947 - val_accuracy: 0.4819 - lr: 0.0010\n",
      "Epoch 88/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.71 - ETA: 0s - loss: 0.5605 - accuracy: 0.71 - ETA: 0s - loss: 0.5625 - accuracy: 0.71 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5744 - accuracy: 0.69 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.7002\n",
      "Epoch 00088: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5759 - accuracy: 0.7002 - val_loss: 0.7790 - val_accuracy: 0.4943 - lr: 0.0010\n",
      "Epoch 89/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.69 - ETA: 0s - loss: 0.5589 - accuracy: 0.70 - ETA: 0s - loss: 0.5653 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5801 - accuracy: 0.6976\n",
      "Epoch 00089: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 63ms/step - loss: 0.5801 - accuracy: 0.6976 - val_loss: 0.7934 - val_accuracy: 0.4907 - lr: 0.0010\n",
      "Epoch 90/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.71 - ETA: 0s - loss: 0.5629 - accuracy: 0.70 - ETA: 0s - loss: 0.5627 - accuracy: 0.71 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5788 - accuracy: 0.70 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5805 - accuracy: 0.6975\n",
      "Epoch 00090: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5805 - accuracy: 0.6975 - val_loss: 0.7988 - val_accuracy: 0.4802 - lr: 0.0010\n",
      "Epoch 91/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5402 - accuracy: 0.70 - ETA: 0s - loss: 0.5520 - accuracy: 0.70 - ETA: 0s - loss: 0.5575 - accuracy: 0.71 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5674 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.6980\n",
      "Epoch 00091: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5772 - accuracy: 0.6980 - val_loss: 0.7762 - val_accuracy: 0.4857 - lr: 0.0010\n",
      "Epoch 92/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.68 - ETA: 0s - loss: 0.5574 - accuracy: 0.68 - ETA: 0s - loss: 0.5559 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.69 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.69 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5825 - accuracy: 0.6942\n",
      "Epoch 00092: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5825 - accuracy: 0.6942 - val_loss: 0.7734 - val_accuracy: 0.4908 - lr: 0.0010\n",
      "Epoch 93/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.70 - ETA: 0s - loss: 0.5853 - accuracy: 0.70 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5781 - accuracy: 0.6997\n",
      "Epoch 00093: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5781 - accuracy: 0.6997 - val_loss: 0.7940 - val_accuracy: 0.4861 - lr: 0.0010\n",
      "Epoch 94/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5559 - accuracy: 0.71 - ETA: 0s - loss: 0.5615 - accuracy: 0.71 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5810 - accuracy: 0.70 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5681 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.7008\n",
      "Epoch 00094: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5771 - accuracy: 0.7008 - val_loss: 0.8103 - val_accuracy: 0.4908 - lr: 0.0010\n",
      "Epoch 95/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5476 - accuracy: 0.73 - ETA: 0s - loss: 0.5611 - accuracy: 0.72 - ETA: 0s - loss: 0.5622 - accuracy: 0.72 - ETA: 0s - loss: 0.5689 - accuracy: 0.71 - ETA: 0s - loss: 0.5703 - accuracy: 0.71 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.71 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5693 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.7010\n",
      "Epoch 00095: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5776 - accuracy: 0.7010 - val_loss: 0.7965 - val_accuracy: 0.4776 - lr: 0.0010\n",
      "Epoch 96/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5564 - accuracy: 0.70 - ETA: 0s - loss: 0.5618 - accuracy: 0.70 - ETA: 0s - loss: 0.5662 - accuracy: 0.70 - ETA: 0s - loss: 0.5799 - accuracy: 0.69 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5854 - accuracy: 0.69 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.69 - ETA: 0s - loss: 0.5731 - accuracy: 0.69 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5823 - accuracy: 0.6910\n",
      "Epoch 00096: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5823 - accuracy: 0.6910 - val_loss: 0.7912 - val_accuracy: 0.4775 - lr: 0.0010\n",
      "Epoch 97/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.70 - ETA: 0s - loss: 0.5592 - accuracy: 0.70 - ETA: 0s - loss: 0.5576 - accuracy: 0.71 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.69 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.6949\n",
      "Epoch 00097: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5772 - accuracy: 0.6949 - val_loss: 0.8016 - val_accuracy: 0.4830 - lr: 0.0010\n",
      "Epoch 98/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.72 - ETA: 0s - loss: 0.5479 - accuracy: 0.72 - ETA: 0s - loss: 0.5569 - accuracy: 0.72 - ETA: 0s - loss: 0.5697 - accuracy: 0.71 - ETA: 0s - loss: 0.5752 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5630 - accuracy: 0.71 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.7012\n",
      "Epoch 00098: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5783 - accuracy: 0.7012 - val_loss: 0.7915 - val_accuracy: 0.4888 - lr: 0.0010\n",
      "Epoch 99/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.69 - ETA: 0s - loss: 0.5549 - accuracy: 0.70 - ETA: 0s - loss: 0.5531 - accuracy: 0.72 - ETA: 0s - loss: 0.5667 - accuracy: 0.70 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5611 - accuracy: 0.71 - ETA: 0s - loss: 0.5674 - accuracy: 0.71 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5644 - accuracy: 0.71 - ETA: 0s - loss: 0.5658 - accuracy: 0.71 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5786 - accuracy: 0.7007\n",
      "Epoch 00099: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5786 - accuracy: 0.7007 - val_loss: 0.7930 - val_accuracy: 0.4874 - lr: 0.0010\n",
      "Epoch 100/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.70 - ETA: 0s - loss: 0.5584 - accuracy: 0.70 - ETA: 0s - loss: 0.5656 - accuracy: 0.71 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.69 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.69 - ETA: 0s - loss: 0.5739 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.6939\n",
      "Epoch 00100: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5800 - accuracy: 0.6939 - val_loss: 0.8232 - val_accuracy: 0.4826 - lr: 0.0010\n",
      "Epoch 101/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.73 - ETA: 0s - loss: 0.5588 - accuracy: 0.71 - ETA: 0s - loss: 0.5623 - accuracy: 0.71 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5660 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5661 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.6999\n",
      "Epoch 00101: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5773 - accuracy: 0.6999 - val_loss: 0.8009 - val_accuracy: 0.4824 - lr: 0.0010\n",
      "Epoch 102/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5521 - accuracy: 0.71 - ETA: 0s - loss: 0.5512 - accuracy: 0.72 - ETA: 0s - loss: 0.5674 - accuracy: 0.71 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5724 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.6987\n",
      "Epoch 00102: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5787 - accuracy: 0.6987 - val_loss: 0.7999 - val_accuracy: 0.4841 - lr: 0.0010\n",
      "Epoch 103/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5738 - accuracy: 0.71 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.71 - ETA: 0s - loss: 0.5807 - accuracy: 0.70 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.69 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.6978\n",
      "Epoch 00103: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5793 - accuracy: 0.6978 - val_loss: 0.7993 - val_accuracy: 0.4826 - lr: 0.0010\n",
      "Epoch 104/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5588 - accuracy: 0.70 - ETA: 0s - loss: 0.5602 - accuracy: 0.69 - ETA: 0s - loss: 0.5565 - accuracy: 0.71 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5638 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.69 - ETA: 0s - loss: 0.5708 - accuracy: 0.69 - ETA: 0s - loss: 0.5663 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5709 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.6941\n",
      "Epoch 00104: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5758 - accuracy: 0.6941 - val_loss: 0.7949 - val_accuracy: 0.4831 - lr: 0.0010\n",
      "Epoch 105/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.68 - ETA: 0s - loss: 0.5654 - accuracy: 0.69 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5837 - accuracy: 0.69 - ETA: 0s - loss: 0.5865 - accuracy: 0.69 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.70 - ETA: 0s - loss: 0.5809 - accuracy: 0.70 - ETA: 0s - loss: 0.5781 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.7001\n",
      "Epoch 00105: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5793 - accuracy: 0.7001 - val_loss: 0.7973 - val_accuracy: 0.4835 - lr: 0.0010\n",
      "Epoch 106/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.71 - ETA: 0s - loss: 0.5577 - accuracy: 0.71 - ETA: 0s - loss: 0.5520 - accuracy: 0.72 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.6955\n",
      "Epoch 00106: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5792 - accuracy: 0.6955 - val_loss: 0.7929 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 107/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5574 - accuracy: 0.69 - ETA: 0s - loss: 0.5588 - accuracy: 0.69 - ETA: 0s - loss: 0.5611 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5816 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5794 - accuracy: 0.69 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5725 - accuracy: 0.69 - ETA: 0s - loss: 0.5739 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.6943\n",
      "Epoch 00107: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5812 - accuracy: 0.6943 - val_loss: 0.7938 - val_accuracy: 0.4872 - lr: 0.0010\n",
      "Epoch 108/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5550 - accuracy: 0.70 - ETA: 0s - loss: 0.5571 - accuracy: 0.71 - ETA: 0s - loss: 0.5630 - accuracy: 0.71 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5820 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.70 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.70 - ETA: 0s - loss: 0.5788 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.7014\n",
      "Epoch 00108: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5799 - accuracy: 0.6998 - val_loss: 0.8154 - val_accuracy: 0.4848 - lr: 0.0010\n",
      "Epoch 109/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.70 - ETA: 0s - loss: 0.5582 - accuracy: 0.70 - ETA: 0s - loss: 0.5586 - accuracy: 0.71 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5822 - accuracy: 0.69 - ETA: 0s - loss: 0.5809 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.69 - ETA: 0s - loss: 0.5840 - accuracy: 0.6942\n",
      "Epoch 00109: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5840 - accuracy: 0.6942 - val_loss: 0.7881 - val_accuracy: 0.4965 - lr: 0.0010\n",
      "Epoch 110/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.68 - ETA: 0s - loss: 0.5658 - accuracy: 0.69 - ETA: 0s - loss: 0.5668 - accuracy: 0.70 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.70 - ETA: 0s - loss: 0.5861 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5817 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5807 - accuracy: 0.69 - ETA: 0s - loss: 0.5831 - accuracy: 0.6925\n",
      "Epoch 00110: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5831 - accuracy: 0.6925 - val_loss: 0.7778 - val_accuracy: 0.5060 - lr: 0.0010\n",
      "Epoch 111/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.68 - ETA: 0s - loss: 0.5633 - accuracy: 0.69 - ETA: 0s - loss: 0.5641 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.70 - ETA: 0s - loss: 0.5823 - accuracy: 0.69 - ETA: 0s - loss: 0.5786 - accuracy: 0.69 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5791 - accuracy: 0.69 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5801 - accuracy: 0.6953\n",
      "Epoch 00111: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5801 - accuracy: 0.6953 - val_loss: 0.7850 - val_accuracy: 0.4929 - lr: 0.0010\n",
      "Epoch 112/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.67 - ETA: 0s - loss: 0.5577 - accuracy: 0.70 - ETA: 0s - loss: 0.5612 - accuracy: 0.71 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5821 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.7002\n",
      "Epoch 00112: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5785 - accuracy: 0.7002 - val_loss: 0.7889 - val_accuracy: 0.4947 - lr: 0.0010\n",
      "Epoch 113/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.71 - ETA: 0s - loss: 0.5626 - accuracy: 0.70 - ETA: 0s - loss: 0.5638 - accuracy: 0.71 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5798 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5798 - accuracy: 0.6950\n",
      "Epoch 00113: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5798 - accuracy: 0.6950 - val_loss: 0.7814 - val_accuracy: 0.4874 - lr: 0.0010\n",
      "Epoch 114/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5538 - accuracy: 0.69 - ETA: 0s - loss: 0.5616 - accuracy: 0.69 - ETA: 0s - loss: 0.5585 - accuracy: 0.71 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.69 - ETA: 0s - loss: 0.5720 - accuracy: 0.69 - ETA: 0s - loss: 0.5632 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.69 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.69 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.6971\n",
      "Epoch 00114: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5788 - accuracy: 0.6971 - val_loss: 0.7847 - val_accuracy: 0.4798 - lr: 0.0010\n",
      "Epoch 115/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.72 - ETA: 0s - loss: 0.5450 - accuracy: 0.72 - ETA: 0s - loss: 0.5546 - accuracy: 0.72 - ETA: 0s - loss: 0.5699 - accuracy: 0.71 - ETA: 0s - loss: 0.5704 - accuracy: 0.71 - ETA: 0s - loss: 0.5770 - accuracy: 0.70 - ETA: 0s - loss: 0.5648 - accuracy: 0.71 - ETA: 0s - loss: 0.5713 - accuracy: 0.71 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5765 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.6976\n",
      "Epoch 00115: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5802 - accuracy: 0.6976 - val_loss: 0.7932 - val_accuracy: 0.4798 - lr: 0.0010\n",
      "Epoch 116/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5838 - accuracy: 0.69 - ETA: 0s - loss: 0.5821 - accuracy: 0.70 - ETA: 0s - loss: 0.5858 - accuracy: 0.69 - ETA: 0s - loss: 0.5824 - accuracy: 0.69 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.6984\n",
      "Epoch 00116: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5793 - accuracy: 0.6984 - val_loss: 0.8017 - val_accuracy: 0.4798 - lr: 0.0010\n",
      "Epoch 117/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5671 - accuracy: 0.69 - ETA: 0s - loss: 0.5646 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5805 - accuracy: 0.70 - ETA: 0s - loss: 0.5854 - accuracy: 0.69 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.69 - ETA: 0s - loss: 0.5788 - accuracy: 0.69 - ETA: 0s - loss: 0.5811 - accuracy: 0.6945\n",
      "Epoch 00117: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5811 - accuracy: 0.6945 - val_loss: 0.7983 - val_accuracy: 0.4775 - lr: 0.0010\n",
      "Epoch 118/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.70 - ETA: 0s - loss: 0.5511 - accuracy: 0.70 - ETA: 0s - loss: 0.5491 - accuracy: 0.71 - ETA: 0s - loss: 0.5618 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5730 - accuracy: 0.69 - ETA: 0s - loss: 0.5642 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5678 - accuracy: 0.70 - ETA: 0s - loss: 0.5666 - accuracy: 0.70 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.69 - ETA: 0s - loss: 0.5741 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.6969\n",
      "Epoch 00118: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5769 - accuracy: 0.6969 - val_loss: 0.7885 - val_accuracy: 0.4974 - lr: 0.0010\n",
      "Epoch 119/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.71 - ETA: 0s - loss: 0.5522 - accuracy: 0.71 - ETA: 0s - loss: 0.5598 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.70 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5800 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5793 - accuracy: 0.69 - ETA: 0s - loss: 0.5809 - accuracy: 0.6945\n",
      "Epoch 00119: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5809 - accuracy: 0.6945 - val_loss: 0.8012 - val_accuracy: 0.4762 - lr: 0.0010\n",
      "Epoch 120/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.68 - ETA: 0s - loss: 0.5572 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.70 - ETA: 0s - loss: 0.5831 - accuracy: 0.69 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5761 - accuracy: 0.69 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5803 - accuracy: 0.6944\n",
      "Epoch 00120: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5803 - accuracy: 0.6944 - val_loss: 0.8118 - val_accuracy: 0.4808 - lr: 0.0010\n",
      "Epoch 121/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5560 - accuracy: 0.71 - ETA: 0s - loss: 0.5550 - accuracy: 0.71 - ETA: 0s - loss: 0.5547 - accuracy: 0.71 - ETA: 0s - loss: 0.5665 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5648 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5754 - accuracy: 0.69 - ETA: 0s - loss: 0.5787 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5804 - accuracy: 0.6953\n",
      "Epoch 00121: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5804 - accuracy: 0.6953 - val_loss: 0.8047 - val_accuracy: 0.4914 - lr: 0.0010\n",
      "Epoch 122/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.72 - ETA: 0s - loss: 0.5484 - accuracy: 0.71 - ETA: 0s - loss: 0.5490 - accuracy: 0.72 - ETA: 0s - loss: 0.5627 - accuracy: 0.71 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.7006\n",
      "Epoch 00122: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5792 - accuracy: 0.7006 - val_loss: 0.8097 - val_accuracy: 0.4769 - lr: 0.0010\n",
      "Epoch 123/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.69 - ETA: 0s - loss: 0.5614 - accuracy: 0.70 - ETA: 0s - loss: 0.5549 - accuracy: 0.71 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5811 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.69 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5797 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.6949\n",
      "Epoch 00123: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5818 - accuracy: 0.6949 - val_loss: 0.8110 - val_accuracy: 0.4835 - lr: 0.0010\n",
      "Epoch 124/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5615 - accuracy: 0.71 - ETA: 0s - loss: 0.5632 - accuracy: 0.71 - ETA: 0s - loss: 0.5699 - accuracy: 0.71 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.7026\n",
      "Epoch 00124: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5767 - accuracy: 0.7011 - val_loss: 0.8111 - val_accuracy: 0.4754 - lr: 0.0010\n",
      "Epoch 125/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.70 - ETA: 0s - loss: 0.5565 - accuracy: 0.71 - ETA: 0s - loss: 0.5588 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5647 - accuracy: 0.71 - ETA: 0s - loss: 0.5659 - accuracy: 0.71 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.7038\n",
      "Epoch 00125: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00125: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5729 - accuracy: 0.7038 - val_loss: 0.7962 - val_accuracy: 0.4731 - lr: 0.0010\n",
      "Epoch 126/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.69 - ETA: 0s - loss: 0.5516 - accuracy: 0.70 - ETA: 0s - loss: 0.5580 - accuracy: 0.71 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5689 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5725 - accuracy: 0.69 - ETA: 0s - loss: 0.5620 - accuracy: 0.70 - ETA: 0s - loss: 0.5674 - accuracy: 0.69 - ETA: 0s - loss: 0.5688 - accuracy: 0.69 - ETA: 0s - loss: 0.5730 - accuracy: 0.69 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5656 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.69 - ETA: 0s - loss: 0.5738 - accuracy: 0.69 - ETA: 0s - loss: 0.5760 - accuracy: 0.6984\n",
      "Epoch 00126: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5760 - accuracy: 0.6984 - val_loss: 0.7988 - val_accuracy: 0.4791 - lr: 0.0010\n",
      "Epoch 127/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.68 - ETA: 0s - loss: 0.5612 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.69 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5663 - accuracy: 0.70 - ETA: 0s - loss: 0.5697 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.69 - ETA: 0s - loss: 0.5737 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.6971\n",
      "Epoch 00127: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5763 - accuracy: 0.6971 - val_loss: 0.8016 - val_accuracy: 0.4643 - lr: 0.0010\n",
      "Epoch 128/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5678 - accuracy: 0.71 - ETA: 0s - loss: 0.5609 - accuracy: 0.71 - ETA: 0s - loss: 0.5577 - accuracy: 0.71 - ETA: 0s - loss: 0.5722 - accuracy: 0.71 - ETA: 0s - loss: 0.5736 - accuracy: 0.71 - ETA: 0s - loss: 0.5795 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5802 - accuracy: 0.69 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5812 - accuracy: 0.6969\n",
      "Epoch 00128: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 59ms/step - loss: 0.5812 - accuracy: 0.6969 - val_loss: 0.8324 - val_accuracy: 0.4764 - lr: 0.0010\n",
      "Epoch 129/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.69 - ETA: 0s - loss: 0.5635 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5795 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.6950\n",
      "Epoch 00129: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5819 - accuracy: 0.6950 - val_loss: 0.8065 - val_accuracy: 0.4852 - lr: 0.0010\n",
      "Epoch 130/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.73 - ETA: 0s - loss: 0.5514 - accuracy: 0.72 - ETA: 0s - loss: 0.5648 - accuracy: 0.71 - ETA: 0s - loss: 0.5667 - accuracy: 0.71 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5635 - accuracy: 0.71 - ETA: 0s - loss: 0.5701 - accuracy: 0.71 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.71 - ETA: 0s - loss: 0.5684 - accuracy: 0.71 - ETA: 0s - loss: 0.5706 - accuracy: 0.71 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5792 - accuracy: 0.7041\n",
      "Epoch 00130: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5792 - accuracy: 0.7041 - val_loss: 0.7957 - val_accuracy: 0.4943 - lr: 0.0010\n",
      "Epoch 131/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.71 - ETA: 0s - loss: 0.5488 - accuracy: 0.71 - ETA: 0s - loss: 0.5622 - accuracy: 0.71 - ETA: 0s - loss: 0.5638 - accuracy: 0.71 - ETA: 0s - loss: 0.5660 - accuracy: 0.71 - ETA: 0s - loss: 0.5593 - accuracy: 0.71 - ETA: 0s - loss: 0.5674 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5677 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5730 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.7014\n",
      "Epoch 00131: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5777 - accuracy: 0.7014 - val_loss: 0.7931 - val_accuracy: 0.4802 - lr: 0.0010\n",
      "Epoch 132/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5618 - accuracy: 0.69 - ETA: 0s - loss: 0.5575 - accuracy: 0.71 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5660 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.69 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5783 - accuracy: 0.6988\n",
      "Epoch 00132: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5783 - accuracy: 0.6988 - val_loss: 0.8107 - val_accuracy: 0.4795 - lr: 0.0010\n",
      "Epoch 133/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.72 - ETA: 0s - loss: 0.5481 - accuracy: 0.71 - ETA: 0s - loss: 0.5544 - accuracy: 0.71 - ETA: 0s - loss: 0.5683 - accuracy: 0.71 - ETA: 0s - loss: 0.5699 - accuracy: 0.71 - ETA: 0s - loss: 0.5768 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5676 - accuracy: 0.70 - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.7019\n",
      "Epoch 00133: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5744 - accuracy: 0.7019 - val_loss: 0.8022 - val_accuracy: 0.4804 - lr: 0.0010\n",
      "Epoch 134/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.69 - ETA: 0s - loss: 0.5641 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5800 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5784 - accuracy: 0.70 - ETA: 0s - loss: 0.5827 - accuracy: 0.70 - ETA: 0s - loss: 0.5791 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5794 - accuracy: 0.70 - ETA: 0s - loss: 0.5816 - accuracy: 0.6988\n",
      "Epoch 00134: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5816 - accuracy: 0.6988 - val_loss: 0.8003 - val_accuracy: 0.4903 - lr: 0.0010\n",
      "Epoch 135/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.70 - ETA: 0s - loss: 0.5626 - accuracy: 0.70 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5807 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5645 - accuracy: 0.71 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.6971\n",
      "Epoch 00135: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5776 - accuracy: 0.6971 - val_loss: 0.8075 - val_accuracy: 0.4784 - lr: 0.0010\n",
      "Epoch 136/3000\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.69 - ETA: 0s - loss: 0.5584 - accuracy: 0.70 - ETA: 0s - loss: 0.5537 - accuracy: 0.72 - ETA: 0s - loss: 0.5693 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5796 - accuracy: 0.69 - ETA: 0s - loss: 0.5756 - accuracy: 0.69 - ETA: 0s - loss: 0.5654 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.69 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.6967\n",
      "Epoch 00136: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5785 - accuracy: 0.6948 - val_loss: 0.8048 - val_accuracy: 0.4855 - lr: 0.0010\n",
      "Epoch 137/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5662 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.71 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.71 - ETA: 0s - loss: 0.5787 - accuracy: 0.70 - ETA: 0s - loss: 0.5659 - accuracy: 0.71 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.71 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.7023\n",
      "Epoch 00137: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5755 - accuracy: 0.7023 - val_loss: 0.7951 - val_accuracy: 0.4921 - lr: 0.0010\n",
      "Epoch 138/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.70 - ETA: 0s - loss: 0.5494 - accuracy: 0.71 - ETA: 0s - loss: 0.5667 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5595 - accuracy: 0.71 - ETA: 0s - loss: 0.5651 - accuracy: 0.71 - ETA: 0s - loss: 0.5666 - accuracy: 0.71 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5698 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.71 - ETA: 0s - loss: 0.5637 - accuracy: 0.71 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.7023\n",
      "Epoch 00138: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5756 - accuracy: 0.7023 - val_loss: 0.8025 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 139/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.71 - ETA: 0s - loss: 0.5471 - accuracy: 0.71 - ETA: 0s - loss: 0.5510 - accuracy: 0.72 - ETA: 0s - loss: 0.5649 - accuracy: 0.71 - ETA: 0s - loss: 0.5685 - accuracy: 0.71 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5615 - accuracy: 0.71 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5693 - accuracy: 0.70 - ETA: 0s - loss: 0.5652 - accuracy: 0.70 - ETA: 0s - loss: 0.5637 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.7032\n",
      "Epoch 00139: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5738 - accuracy: 0.7032 - val_loss: 0.7969 - val_accuracy: 0.4778 - lr: 0.0010\n",
      "Epoch 140/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.71 - ETA: 0s - loss: 0.5527 - accuracy: 0.71 - ETA: 0s - loss: 0.5567 - accuracy: 0.71 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.69 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5618 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5648 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.7019\n",
      "Epoch 00140: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5744 - accuracy: 0.7019 - val_loss: 0.7869 - val_accuracy: 0.4875 - lr: 0.0010\n",
      "Epoch 141/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.72 - ETA: 0s - loss: 0.5591 - accuracy: 0.71 - ETA: 0s - loss: 0.5579 - accuracy: 0.72 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5661 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5773 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5784 - accuracy: 0.70 - ETA: 0s - loss: 0.5813 - accuracy: 0.6979\n",
      "Epoch 00141: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5813 - accuracy: 0.6979 - val_loss: 0.7827 - val_accuracy: 0.4918 - lr: 0.0010\n",
      "Epoch 142/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5676 - accuracy: 0.69 - ETA: 0s - loss: 0.5591 - accuracy: 0.70 - ETA: 0s - loss: 0.5679 - accuracy: 0.70 - ETA: 0s - loss: 0.5791 - accuracy: 0.70 - ETA: 0s - loss: 0.5798 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.70 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5763 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.6991\n",
      "Epoch 00142: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5771 - accuracy: 0.6991 - val_loss: 0.7953 - val_accuracy: 0.4831 - lr: 0.0010\n",
      "Epoch 143/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.73 - ETA: 0s - loss: 0.5454 - accuracy: 0.72 - ETA: 0s - loss: 0.5500 - accuracy: 0.72 - ETA: 0s - loss: 0.5668 - accuracy: 0.71 - ETA: 0s - loss: 0.5674 - accuracy: 0.71 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5688 - accuracy: 0.70 - ETA: 0s - loss: 0.5583 - accuracy: 0.71 - ETA: 0s - loss: 0.5644 - accuracy: 0.71 - ETA: 0s - loss: 0.5660 - accuracy: 0.70 - ETA: 0s - loss: 0.5699 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5629 - accuracy: 0.71 - ETA: 0s - loss: 0.5616 - accuracy: 0.71 - ETA: 0s - loss: 0.5647 - accuracy: 0.71 - ETA: 0s - loss: 0.5674 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5736 - accuracy: 0.7028\n",
      "Epoch 00143: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5736 - accuracy: 0.7028 - val_loss: 0.7875 - val_accuracy: 0.4888 - lr: 0.0010\n",
      "Epoch 144/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.71 - ETA: 0s - loss: 0.5604 - accuracy: 0.71 - ETA: 0s - loss: 0.5622 - accuracy: 0.71 - ETA: 0s - loss: 0.5725 - accuracy: 0.71 - ETA: 0s - loss: 0.5761 - accuracy: 0.71 - ETA: 0s - loss: 0.5795 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5784 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.70 - ETA: 0s - loss: 0.5718 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5782 - accuracy: 0.6980\n",
      "Epoch 00144: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5782 - accuracy: 0.6980 - val_loss: 0.7843 - val_accuracy: 0.4780 - lr: 0.0010\n",
      "Epoch 145/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5449 - accuracy: 0.71 - ETA: 0s - loss: 0.5482 - accuracy: 0.71 - ETA: 0s - loss: 0.5553 - accuracy: 0.71 - ETA: 0s - loss: 0.5698 - accuracy: 0.70 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.69 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5641 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.69 - ETA: 0s - loss: 0.5750 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.69 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5667 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5723 - accuracy: 0.69 - ETA: 0s - loss: 0.5746 - accuracy: 0.69 - ETA: 0s - loss: 0.5769 - accuracy: 0.6957\n",
      "Epoch 00145: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5769 - accuracy: 0.6957 - val_loss: 0.7812 - val_accuracy: 0.4820 - lr: 0.0010\n",
      "Epoch 146/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.71 - ETA: 0s - loss: 0.5534 - accuracy: 0.71 - ETA: 0s - loss: 0.5591 - accuracy: 0.71 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5586 - accuracy: 0.71 - ETA: 0s - loss: 0.5669 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5639 - accuracy: 0.71 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.7013\n",
      "Epoch 00146: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5753 - accuracy: 0.7013 - val_loss: 0.7845 - val_accuracy: 0.4936 - lr: 0.0010\n",
      "Epoch 147/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.72 - ETA: 0s - loss: 0.5559 - accuracy: 0.71 - ETA: 0s - loss: 0.5557 - accuracy: 0.71 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.70 - ETA: 0s - loss: 0.5644 - accuracy: 0.71 - ETA: 0s - loss: 0.5706 - accuracy: 0.71 - ETA: 0s - loss: 0.5710 - accuracy: 0.71 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5684 - accuracy: 0.71 - ETA: 0s - loss: 0.5670 - accuracy: 0.71 - ETA: 0s - loss: 0.5700 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.7010\n",
      "Epoch 00147: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5769 - accuracy: 0.7010 - val_loss: 0.7955 - val_accuracy: 0.4745 - lr: 0.0010\n",
      "Epoch 148/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5504 - accuracy: 0.71 - ETA: 0s - loss: 0.5519 - accuracy: 0.71 - ETA: 0s - loss: 0.5493 - accuracy: 0.72 - ETA: 0s - loss: 0.5611 - accuracy: 0.72 - ETA: 0s - loss: 0.5660 - accuracy: 0.71 - ETA: 0s - loss: 0.5733 - accuracy: 0.71 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5621 - accuracy: 0.71 - ETA: 0s - loss: 0.5696 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5656 - accuracy: 0.71 - ETA: 0s - loss: 0.5634 - accuracy: 0.71 - ETA: 0s - loss: 0.5668 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.7021\n",
      "Epoch 00148: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5737 - accuracy: 0.7021 - val_loss: 0.7992 - val_accuracy: 0.4775 - lr: 0.0010\n",
      "Epoch 149/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.71 - ETA: 0s - loss: 0.5438 - accuracy: 0.71 - ETA: 0s - loss: 0.5572 - accuracy: 0.71 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5807 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.70 - ETA: 0s - loss: 0.5666 - accuracy: 0.71 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.7023\n",
      "Epoch 00149: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5767 - accuracy: 0.7023 - val_loss: 0.7766 - val_accuracy: 0.4756 - lr: 0.0010\n",
      "Epoch 150/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5665 - accuracy: 0.69 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5762 - accuracy: 0.70 - ETA: 0s - loss: 0.5793 - accuracy: 0.70 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5690 - accuracy: 0.70 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5779 - accuracy: 0.7014\n",
      "Epoch 00150: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5779 - accuracy: 0.7014 - val_loss: 0.7882 - val_accuracy: 0.4707 - lr: 0.0010\n",
      "Epoch 151/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5661 - accuracy: 0.71 - ETA: 0s - loss: 0.5651 - accuracy: 0.71 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5778 - accuracy: 0.70 - ETA: 0s - loss: 0.5826 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.69 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.69 - ETA: 0s - loss: 0.5770 - accuracy: 0.69 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5681 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5691 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5759 - accuracy: 0.6997\n",
      "Epoch 00151: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5759 - accuracy: 0.6997 - val_loss: 0.8087 - val_accuracy: 0.4758 - lr: 0.0010\n",
      "Epoch 152/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.71 - ETA: 0s - loss: 0.5580 - accuracy: 0.72 - ETA: 0s - loss: 0.5700 - accuracy: 0.71 - ETA: 0s - loss: 0.5710 - accuracy: 0.71 - ETA: 0s - loss: 0.5759 - accuracy: 0.70 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5637 - accuracy: 0.71 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.71 - ETA: 0s - loss: 0.5755 - accuracy: 0.70 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5668 - accuracy: 0.71 - ETA: 0s - loss: 0.5651 - accuracy: 0.71 - ETA: 0s - loss: 0.5682 - accuracy: 0.71 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.7048\n",
      "Epoch 00152: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5742 - accuracy: 0.7048 - val_loss: 0.7923 - val_accuracy: 0.4824 - lr: 0.0010\n",
      "Epoch 153/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.71 - ETA: 0s - loss: 0.5550 - accuracy: 0.72 - ETA: 0s - loss: 0.5566 - accuracy: 0.72 - ETA: 0s - loss: 0.5680 - accuracy: 0.71 - ETA: 0s - loss: 0.5701 - accuracy: 0.71 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.70 - ETA: 0s - loss: 0.5744 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5683 - accuracy: 0.70 - ETA: 0s - loss: 0.5676 - accuracy: 0.70 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5777 - accuracy: 0.6992\n",
      "Epoch 00153: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5777 - accuracy: 0.6992 - val_loss: 0.7882 - val_accuracy: 0.4754 - lr: 0.0010\n",
      "Epoch 154/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.71 - ETA: 0s - loss: 0.5766 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5828 - accuracy: 0.69 - ETA: 0s - loss: 0.5834 - accuracy: 0.69 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.69 - ETA: 0s - loss: 0.5763 - accuracy: 0.69 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5698 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.6994\n",
      "Epoch 00154: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5760 - accuracy: 0.6994 - val_loss: 0.7860 - val_accuracy: 0.4901 - lr: 0.0010\n",
      "Epoch 155/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.68 - ETA: 0s - loss: 0.5668 - accuracy: 0.70 - ETA: 0s - loss: 0.5657 - accuracy: 0.70 - ETA: 0s - loss: 0.5764 - accuracy: 0.69 - ETA: 0s - loss: 0.5773 - accuracy: 0.69 - ETA: 0s - loss: 0.5814 - accuracy: 0.69 - ETA: 0s - loss: 0.5792 - accuracy: 0.69 - ETA: 0s - loss: 0.5666 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.70 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.70 - ETA: 0s - loss: 0.5671 - accuracy: 0.70 - ETA: 0s - loss: 0.5694 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.6996\n",
      "Epoch 00155: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5747 - accuracy: 0.6996 - val_loss: 0.7874 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 156/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.71 - ETA: 0s - loss: 0.5560 - accuracy: 0.71 - ETA: 0s - loss: 0.5559 - accuracy: 0.71 - ETA: 0s - loss: 0.5678 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5628 - accuracy: 0.70 - ETA: 0s - loss: 0.5681 - accuracy: 0.70 - ETA: 0s - loss: 0.5692 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5646 - accuracy: 0.70 - ETA: 0s - loss: 0.5630 - accuracy: 0.70 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.6981\n",
      "Epoch 00156: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5747 - accuracy: 0.6981 - val_loss: 0.7998 - val_accuracy: 0.4879 - lr: 0.0010\n",
      "Epoch 157/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.69 - ETA: 0s - loss: 0.5630 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5753 - accuracy: 0.70 - ETA: 0s - loss: 0.5781 - accuracy: 0.69 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5745 - accuracy: 0.70 - ETA: 0s - loss: 0.5772 - accuracy: 0.69 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.70 - ETA: 0s - loss: 0.5708 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5778 - accuracy: 0.6971\n",
      "Epoch 00157: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5778 - accuracy: 0.6971 - val_loss: 0.7889 - val_accuracy: 0.4765 - lr: 0.0010\n",
      "Epoch 158/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5581 - accuracy: 0.69 - ETA: 0s - loss: 0.5561 - accuracy: 0.70 - ETA: 0s - loss: 0.5615 - accuracy: 0.71 - ETA: 0s - loss: 0.5750 - accuracy: 0.69 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.69 - ETA: 0s - loss: 0.5736 - accuracy: 0.69 - ETA: 0s - loss: 0.5762 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5682 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5754 - accuracy: 0.7019\n",
      "Epoch 00158: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.5754 - accuracy: 0.7019 - val_loss: 0.7743 - val_accuracy: 0.4885 - lr: 0.0010\n",
      "Epoch 159/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.67 - ETA: 0s - loss: 0.5543 - accuracy: 0.70 - ETA: 0s - loss: 0.5618 - accuracy: 0.71 - ETA: 0s - loss: 0.5726 - accuracy: 0.71 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5769 - accuracy: 0.70 - ETA: 0s - loss: 0.5740 - accuracy: 0.70 - ETA: 0s - loss: 0.5648 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5746 - accuracy: 0.70 - ETA: 0s - loss: 0.5716 - accuracy: 0.70 - ETA: 0s - loss: 0.5662 - accuracy: 0.70 - ETA: 0s - loss: 0.5664 - accuracy: 0.70 - ETA: 0s - loss: 0.5698 - accuracy: 0.70 - ETA: 0s - loss: 0.5725 - accuracy: 0.70 - ETA: 0s - loss: 0.5741 - accuracy: 0.70 - ETA: 0s - loss: 0.5768 - accuracy: 0.6994\n",
      "Epoch 00159: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5768 - accuracy: 0.6994 - val_loss: 0.7956 - val_accuracy: 0.4808 - lr: 0.0010\n",
      "Epoch 160/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5617 - accuracy: 0.71 - ETA: 0s - loss: 0.5652 - accuracy: 0.71 - ETA: 0s - loss: 0.5737 - accuracy: 0.70 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5702 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5732 - accuracy: 0.70 - ETA: 0s - loss: 0.5695 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.7016\n",
      "Epoch 00160: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5774 - accuracy: 0.7016 - val_loss: 0.7878 - val_accuracy: 0.4835 - lr: 0.0010\n",
      "Epoch 161/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.69 - ETA: 0s - loss: 0.5621 - accuracy: 0.70 - ETA: 0s - loss: 0.5613 - accuracy: 0.70 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5652 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.69 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5737 - accuracy: 0.69 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5672 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5743 - accuracy: 0.69 - ETA: 0s - loss: 0.5766 - accuracy: 0.6981\n",
      "Epoch 00161: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5766 - accuracy: 0.6981 - val_loss: 0.7984 - val_accuracy: 0.4809 - lr: 0.0010\n",
      "Epoch 162/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.70 - ETA: 0s - loss: 0.5626 - accuracy: 0.71 - ETA: 0s - loss: 0.5590 - accuracy: 0.71 - ETA: 0s - loss: 0.5683 - accuracy: 0.71 - ETA: 0s - loss: 0.5745 - accuracy: 0.71 - ETA: 0s - loss: 0.5775 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.71 - ETA: 0s - loss: 0.5720 - accuracy: 0.70 - ETA: 0s - loss: 0.5715 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.70 - ETA: 0s - loss: 0.5670 - accuracy: 0.71 - ETA: 0s - loss: 0.5652 - accuracy: 0.71 - ETA: 0s - loss: 0.5682 - accuracy: 0.71 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5735 - accuracy: 0.70 - ETA: 0s - loss: 0.5753 - accuracy: 0.7053\n",
      "Epoch 00162: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5753 - accuracy: 0.7053 - val_loss: 0.7928 - val_accuracy: 0.4795 - lr: 0.0010\n",
      "Epoch 163/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.68 - ETA: 0s - loss: 0.5650 - accuracy: 0.69 - ETA: 0s - loss: 0.5644 - accuracy: 0.70 - ETA: 0s - loss: 0.5774 - accuracy: 0.69 - ETA: 0s - loss: 0.5858 - accuracy: 0.68 - ETA: 0s - loss: 0.5807 - accuracy: 0.68 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5780 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5720 - accuracy: 0.69 - ETA: 0s - loss: 0.5709 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.69 - ETA: 0s - loss: 0.5751 - accuracy: 0.69 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5805 - accuracy: 0.6940\n",
      "Epoch 00163: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5805 - accuracy: 0.6940 - val_loss: 0.7898 - val_accuracy: 0.4753 - lr: 0.0010\n",
      "Epoch 164/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.68 - ETA: 0s - loss: 0.5557 - accuracy: 0.70 - ETA: 0s - loss: 0.5590 - accuracy: 0.70 - ETA: 0s - loss: 0.5738 - accuracy: 0.70 - ETA: 0s - loss: 0.5767 - accuracy: 0.70 - ETA: 0s - loss: 0.5803 - accuracy: 0.69 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5675 - accuracy: 0.70 - ETA: 0s - loss: 0.5749 - accuracy: 0.69 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5755 - accuracy: 0.69 - ETA: 0s - loss: 0.5709 - accuracy: 0.69 - ETA: 0s - loss: 0.5701 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.69 - ETA: 0s - loss: 0.5739 - accuracy: 0.69 - ETA: 0s - loss: 0.5760 - accuracy: 0.69 - ETA: 0s - loss: 0.5779 - accuracy: 0.6947\n",
      "Epoch 00164: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.5779 - accuracy: 0.6947 - val_loss: 0.7734 - val_accuracy: 0.4890 - lr: 0.0010\n",
      "Epoch 165/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.71 - ETA: 0s - loss: 0.5712 - accuracy: 0.70 - ETA: 0s - loss: 0.5685 - accuracy: 0.71 - ETA: 0s - loss: 0.5819 - accuracy: 0.70 - ETA: 0s - loss: 0.5818 - accuracy: 0.70 - ETA: 0s - loss: 0.5843 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.70 - ETA: 0s - loss: 0.5680 - accuracy: 0.71 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5750 - accuracy: 0.70 - ETA: 0s - loss: 0.5780 - accuracy: 0.70 - ETA: 0s - loss: 0.5770 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5703 - accuracy: 0.70 - ETA: 0s - loss: 0.5731 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.7001\n",
      "Epoch 00165: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5776 - accuracy: 0.7001 - val_loss: 0.7842 - val_accuracy: 0.5009 - lr: 0.0010\n",
      "Epoch 166/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.67 - ETA: 0s - loss: 0.5646 - accuracy: 0.69 - ETA: 0s - loss: 0.5642 - accuracy: 0.70 - ETA: 0s - loss: 0.5801 - accuracy: 0.69 - ETA: 0s - loss: 0.5832 - accuracy: 0.69 - ETA: 0s - loss: 0.5877 - accuracy: 0.69 - ETA: 0s - loss: 0.5847 - accuracy: 0.69 - ETA: 0s - loss: 0.5722 - accuracy: 0.70 - ETA: 0s - loss: 0.5782 - accuracy: 0.69 - ETA: 0s - loss: 0.5819 - accuracy: 0.69 - ETA: 0s - loss: 0.5818 - accuracy: 0.69 - ETA: 0s - loss: 0.5789 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.70 - ETA: 0s - loss: 0.5790 - accuracy: 0.69 - ETA: 0s - loss: 0.5813 - accuracy: 0.69 - ETA: 0s - loss: 0.5829 - accuracy: 0.69 - ETA: 0s - loss: 0.5853 - accuracy: 0.6943\n",
      "Epoch 00166: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5853 - accuracy: 0.6943 - val_loss: 0.7692 - val_accuracy: 0.4793 - lr: 0.0010\n",
      "Epoch 167/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5546 - accuracy: 0.70 - ETA: 0s - loss: 0.5619 - accuracy: 0.69 - ETA: 0s - loss: 0.5574 - accuracy: 0.71 - ETA: 0s - loss: 0.5663 - accuracy: 0.71 - ETA: 0s - loss: 0.5684 - accuracy: 0.70 - ETA: 0s - loss: 0.5771 - accuracy: 0.69 - ETA: 0s - loss: 0.5768 - accuracy: 0.69 - ETA: 0s - loss: 0.5659 - accuracy: 0.70 - ETA: 0s - loss: 0.5729 - accuracy: 0.69 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5776 - accuracy: 0.69 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5728 - accuracy: 0.70 - ETA: 0s - loss: 0.5706 - accuracy: 0.70 - ETA: 0s - loss: 0.5734 - accuracy: 0.70 - ETA: 0s - loss: 0.5748 - accuracy: 0.70 - ETA: 0s - loss: 0.5760 - accuracy: 0.70 - ETA: 0s - loss: 0.5785 - accuracy: 0.7006\n",
      "Epoch 00167: val_loss did not improve from 0.74497\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5785 - accuracy: 0.7006 - val_loss: 0.7758 - val_accuracy: 0.5024 - lr: 0.0010\n",
      "Epoch 168/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5452 - accuracy: 0.71 - ETA: 0s - loss: 0.5501 - accuracy: 0.70 - ETA: 0s - loss: 0.5560 - accuracy: 0.70 - ETA: 0s - loss: 0.5707 - accuracy: 0.69 - ETA: 0s - loss: 0.5765 - accuracy: 0.69 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5642 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5713 - accuracy: 0.70 - ETA: 0s - loss: 0.5752 - accuracy: 0.69 - ETA: 0s - loss: 0.5717 - accuracy: 0.70 - ETA: 0s - loss: 0.5660 - accuracy: 0.70 - ETA: 0s - loss: 0.5651 - accuracy: 0.70 - ETA: 0s - loss: 0.5686 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.70 - ETA: 0s - loss: 0.5722 - accuracy: 0.69 - ETA: 0s - loss: 0.5743 - accuracy: 0.6979\n",
      "Epoch 00168: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5743 - accuracy: 0.6979 - val_loss: 0.7848 - val_accuracy: 0.4918 - lr: 0.0010\n",
      "Epoch 169/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.68 - ETA: 0s - loss: 0.5620 - accuracy: 0.69 - ETA: 0s - loss: 0.5569 - accuracy: 0.70 - ETA: 0s - loss: 0.5687 - accuracy: 0.69 - ETA: 0s - loss: 0.5726 - accuracy: 0.70 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5732 - accuracy: 0.69 - ETA: 0s - loss: 0.5686 - accuracy: 0.69 - ETA: 0s - loss: 0.5735 - accuracy: 0.69 - ETA: 0s - loss: 0.5757 - accuracy: 0.69 - ETA: 0s - loss: 0.5734 - accuracy: 0.69 - ETA: 0s - loss: 0.5687 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5705 - accuracy: 0.69 - ETA: 0s - loss: 0.5728 - accuracy: 0.69 - ETA: 0s - loss: 0.5753 - accuracy: 0.69 - ETA: 0s - loss: 0.5777 - accuracy: 0.6930\n",
      "Epoch 00169: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5777 - accuracy: 0.6930 - val_loss: 0.7746 - val_accuracy: 0.4872 - lr: 0.0010\n",
      "Epoch 170/3000\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.71 - ETA: 0s - loss: 0.5596 - accuracy: 0.70 - ETA: 0s - loss: 0.5596 - accuracy: 0.71 - ETA: 0s - loss: 0.5680 - accuracy: 0.70 - ETA: 0s - loss: 0.5681 - accuracy: 0.71 - ETA: 0s - loss: 0.5758 - accuracy: 0.70 - ETA: 0s - loss: 0.5742 - accuracy: 0.70 - ETA: 0s - loss: 0.5655 - accuracy: 0.70 - ETA: 0s - loss: 0.5711 - accuracy: 0.70 - ETA: 0s - loss: 0.5710 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5724 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5648 - accuracy: 0.70 - ETA: 0s - loss: 0.5673 - accuracy: 0.70 - ETA: 0s - loss: 0.5704 - accuracy: 0.70 - ETA: 0s - loss: 0.5721 - accuracy: 0.7043\n",
      "Epoch 00170: val_loss did not improve from 0.74497\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.5721 - accuracy: 0.7043 - val_loss: 0.7833 - val_accuracy: 0.4910 - lr: 0.0010\n",
      "Epoch 171/3000\n",
      "10/18 [===============>..............] - ETA: 0s - loss: 0.5574 - accuracy: 0.69 - ETA: 0s - loss: 0.5602 - accuracy: 0.70 - ETA: 0s - loss: 0.5551 - accuracy: 0.71 - ETA: 0s - loss: 0.5657 - accuracy: 0.70 - ETA: 0s - loss: 0.5714 - accuracy: 0.70 - ETA: 0s - loss: 0.5678 - accuracy: 0.70 - ETA: 0s - loss: 0.5585 - accuracy: 0.71 - ETA: 0s - loss: 0.5649 - accuracy: 0.70 - ETA: 0s - loss: 0.5677 - accuracy: 0.7035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1662150a0adb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m                       \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                       \u001b[0muse_multiprocessing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                       callbacks = callbacks_list)\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# model.fit([train_X, train_X2], [train_y, train_y],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \"\"\"\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \"\"\"\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
    "\n",
    "\n",
    "class PerformanceVisualizationCallback(Callback):\n",
    "    def __init__(self, model, validation_data, image_dir):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.validation_data = validation_data\n",
    "        \n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        y_true = self.validation_data[1]             \n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # plot and save confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        chart = plot_confusion_matrix(y_true, y_pred_class, ax=ax)\n",
    "        fig.savefig(os.path.join(self.image_dir, f'confusion_matrix_epoch_{epoch}'))\n",
    "        plt.display(chart)\n",
    "        plt.show()\n",
    "        \n",
    "       # plot and save roc curve\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        plot_roc(y_true, y_pred, ax=ax)\n",
    "        chart = fig.savefig(os.path.join(self.image_dir, f'roc_curve_epoch_{epoch}'))\n",
    "        plt.display(chart)\n",
    "        plt.show()\n",
    "        \n",
    "logdir = \"logs\\\\scalars\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "weight_path=\"xau_usd_cross-360p-high2Tp1Cl100VpConc.{val_loss:.2f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=100) \n",
    "performance_cbk = PerformanceVisualizationCallback(\n",
    "                      model=model,\n",
    "                      validation_data=v_dataset,\n",
    "                      image_dir='performance_vizualizations')\n",
    "\n",
    "callbacks_list = [checkpoint\n",
    "#                   , early\n",
    "                  , reduceLROnPlat\n",
    "                  , tensorboard_callback\n",
    "                  \n",
    "                 ]\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "model.fit(train_dataset,\n",
    "                      validation_data = v_dataset, \n",
    "#                       batch_size = 10,\n",
    "                      epochs = 3000,\n",
    "                      use_multiprocessing = True,\n",
    "                      callbacks = callbacks_list)\n",
    "\n",
    "# model.fit([train_X, train_X2], [train_y, train_y],\n",
    "#                       validation_data = ([valid_X, valid_X2], [valid_y, valid_y]), \n",
    "#                       batch_size = batch_size,\n",
    "#                       epochs = 500,\n",
    "#                       callbacks = callbacks_list)\n",
    "\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b4790925984514e64ca5a9b46de8b309062e0cf",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('xau_usd_cross-360p-high2Tp1Cl100VpConc.1.09.hdf5')\n",
    "lstm_results = model.evaluate(test_dataset, return_dict=True)\n",
    "print(lstm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./trained_model.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3340d6d6ce75585f90c98f1728b1cd664d7f33f"
   },
   "source": [
    "Load and normalize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(ts_length = 150000):\n",
    "    base_dir = 'input/test/'\n",
    "    test_files = [f for f in listdir(base_dir) if isfile(join(base_dir, f))]\n",
    "\n",
    "    ts = np.empty([len(test_files), ts_length])\n",
    "    ids = []\n",
    "    \n",
    "    i = 0\n",
    "    for f in tqdm_notebook(test_files):\n",
    "        ids.append(splitext(f)[0])\n",
    "        t_df = pd.read_csv(base_dir + f, dtype={\"acoustic_data\": np.int8})\n",
    "        ts[i, :] = t_df['acoustic_data'].values\n",
    "        i = i + 1\n",
    "\n",
    "    return ts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f005579ea08913f4f68a3749bd761df6cef2b1b"
   },
   "outputs": [],
   "source": [
    "test_data, test_ids = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c3b7a864a9f53af142a08883def46c3866c5464"
   },
   "outputs": [],
   "source": [
    "X_test = test_data\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf9b36929e5228d4d94b3b7ad1b9011bf088ac44"
   },
   "source": [
    "Load best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "435449fda2bf96635e67d69f56227e140c4cea99"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9aaf9fb44edba5879a75c68820527d9180d2b3c6"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'seg_id': test_ids, 'time_to_failure': y_pred[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b9d5c63161f637de2e39b59e8e4d7c2f3049581"
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"submission.csv\"> Download File </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.transform([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nn = np.array([[1., 0.,2], [2., 1.,3], [0., 0.,4]])\n",
    "print(nn[1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(valid_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(valid_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, \n",
    "                          loss='categorical_crossentropy',\n",
    "                          loss_weights=[1., 1.]\n",
    "#                           loss_weights=[1.]\n",
    "#               \n",
    "                            , metrics=[Recall(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                 , Recall(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                   , Precision(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                  , Precision(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                  ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "model = load_model('./xau_usd_cross-360p-high2Tp1Cl100VpConc.1.13.hdf5')\n",
    "model.save('./modelw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
