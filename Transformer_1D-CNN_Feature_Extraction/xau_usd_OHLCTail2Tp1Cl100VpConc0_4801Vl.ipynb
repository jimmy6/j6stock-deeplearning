{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "e1baa7518f18a7ff1f2767df1b29c051955502ff"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d9e31105725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtop_k_categorical_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCategoricalHinge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction_tf2.2\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, basename, splitext, isfile, exists\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, CategoricalHinge, Recall, Precision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.compat.v2.keras.layers import Input\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random, os, sys\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "tf.compat.v1.disable_eager_execution\n",
    "pd.set_option('precision', 30)\n",
    "np.set_printoptions(precision = 30)\n",
    "\n",
    "\n",
    "#np.random.seed(368)\n",
    "#tf.random.set_seed(368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xfile='C:\\\\workspace\\\\j6stock\\\\xau_usd_OHLC2.0Tp1.0Cl100Vp.txt'\n",
    "\n",
    "seq_len = 60*10 # 3 days + 2 features is enough memory\n",
    "batch_size = 2048       # Batch size\n",
    "# mini_batch_size = 64       # Batch size\n",
    "\n",
    "learning_rate = 0.001  #0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "y_column = 6\n",
    "compute_val_at = 0\n",
    "acc_filtered_r = 0.8\n",
    "\n",
    "\n",
    "upperTailFilter = 0.4\n",
    "lowerTailFilter = 0.4\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "#from keras.layers.recurrent import LSTM\n",
    "#from keras.models import load_model\n",
    "#import keras\n",
    "import pandas as pd ## can be remove once pandas_datareader 0.7 using\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like ## can be remove once pandas_datareader 0.7 using\n",
    "import pandas_datareader.data as web\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    global upperTailFilter, lowerTailFilter\n",
    "    \n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "    #TODO Use previous close instead of open\n",
    "    df['change'] = df['close'] - df['open']\n",
    "    df['tail_upper'] = df['high'].copy()\n",
    "    df['tail_lower'] = df['low'].copy()\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'tail_upper'] = df.at[i, 'high'] - (df.at[i, 'open'] if df.at[i, 'open'] > df.at[i, 'close'] else df.at[i, 'close'])\n",
    "        df.at[i, 'tail_lower'] = (df.at[i, 'close'] if df.at[i, 'close'] < df.at[i, 'open'] else df.at[i, 'open']) - df.at[i, 'low']\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.drop('open', axis=1, inplace=True)\n",
    "    df.drop('high', axis=1, inplace=True)\n",
    "    df.drop('low', axis=1, inplace=True)\n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean() \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        #df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        #df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        #df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "        df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
    "#         df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1)) # no rescale for keep the negative value\n",
    "        df['tail_upper'] = min_max_scaler.fit_transform(df['tail_upper'].values.reshape(-1,1))\n",
    "        upperTailFilter = min_max_scaler.transform([[upperTailFilter]])[0][0] \n",
    "        df['tail_lower'] = min_max_scaler.fit_transform(df['tail_lower'].values.reshape(-1,1))\n",
    "        lowerTailFilter = min_max_scaler.transform([[lowerTailFilter]])[0][0] \n",
    "        \n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))   \n",
    "                #pd.concat([min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1)), df], axis=1)\n",
    "                ma_data = df['{}ma'.format(moving)]\n",
    "                df.drop(labels=['{}ma'.format(moving)], axis=1, inplace=True)\n",
    "                df = pd.concat([ma_data, df], axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = get_stock_data( ma=[50, 100, 200])\n",
    "df = get_stock_data(ma=[240])\n",
    "\n",
    "# amount_of_features = len(df.columns)-1+(input2Length*-1)\n",
    "\n",
    "# def load_data(stock, seq_len):\n",
    "#     print (\"Amount of features = {}\".format(amount_of_features))\n",
    "#     data = stock.as_matrix()\n",
    "#     sequence_length = seq_len + 1 # index starting from 0\n",
    "#     x_result = []\n",
    "#     x_result2 = []\n",
    "#     y_result = []\n",
    "#     for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "#         x_result.append(data[index-seq_len: index,\n",
    "#                              :-1 + (input2Length*-1) # -2 is ignore Input2 features\n",
    "#                             ]) # index : index + 22days\n",
    "#         x_result2.append(data[index, -1 + (input2Length*-1):-1])\n",
    "#         y_result.append(data[index ,-1]);\n",
    "\n",
    "#     x_result, x_result2, y_result = shuffle(x_result, x_result2, y_result , random_state=2)\n",
    "\n",
    "#     #print('---', data[0])\n",
    "#     #print('---', x_result[0])\n",
    "#     #print('---', y_result[0])\n",
    "#     x_result = np.array(x_result)\n",
    "#     x_result2 = np.array(x_result2)\n",
    "#     y_result = np.array(y_result)\n",
    "#     print (\"Amount of data = {}\".format(y_result.shape[0]))\n",
    "\n",
    "#     percentageSplit = 0.5 # 60% split\n",
    "#     row = round(percentageSplit * y_result.shape[0]) \n",
    "#     print (\"Split = {}\".format(row))\n",
    " \n",
    "#     X_train = x_result[:int(row), :] \n",
    "#     X_train2 = x_result2[:int(row), :] \n",
    "#     y_train = y_result[:int(row)] \n",
    "#     print (\"Amount of training data = {}\".format(y_train.shape[0]))\n",
    "#     X_test = x_result[int(row):, :]\n",
    "#     X_test2 = x_result2[int(row):, :]\n",
    "#     y_test = y_result[int(row):]\n",
    "#     # filter for 1 and -1 for validation only\n",
    "#     X_test = X_test[y_test[:]!=0,:]\n",
    "#     X_test2 = X_test2[y_test[:]!=0,:]\n",
    "#     y_test = y_test[y_test[:]!=0]\n",
    "    \n",
    "#     # split 50% again for test and validation set\n",
    "#     row = round(percentageSplit * y_test.shape[0]) \n",
    "#     X_val = X_test[int(row):, :]\n",
    "#     X_val2 = X_test2[int(row):, :]\n",
    "#     y_val = y_test[int(row):]\n",
    "#     print (\"Amount of validation data = {}\".format(y_val.shape[0]))\n",
    "#     X_test = X_test[:int(row), :]\n",
    "#     X_test2 = X_test2[:int(row), :]\n",
    "#     y_test = y_test[:int(row)]\n",
    "#     print (\"Amount of testing data = {}\".format(y_test.shape[0]))\n",
    "#     #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "#     #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "#     #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))    \n",
    "#     return [X_train, X_train2, y_train, X_test, X_test2, y_test, X_val, X_val2, y_val]\n",
    "\n",
    "\n",
    "\n",
    "classes = [1, 0, -1]\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "lb.transform([-1, 0, 1])\n",
    "\n",
    "df_to_dataset = df[['close', 'change', '240ma'\n",
    "                   ]].copy()\n",
    "df_to_dataset_input2 = df[[ 'tail_upper', 'tail_lower', 'change' ,'cross_360p_high', 'cross_1440p_high'\n",
    "                          ]].copy()\n",
    "\n",
    "amount_of_features = len(df_to_dataset.columns)\n",
    "input2Length = len(df_to_dataset_input2.columns)\n",
    "\n",
    "df.drop(labels=['close'], axis=1, inplace=True)\n",
    "df.drop(labels=['change'], axis=1, inplace=True)\n",
    "df.drop(labels=['tail_upper'], axis=1, inplace=True)\n",
    "df.drop(labels=['tail_lower'], axis=1, inplace=True)\n",
    "df.drop(labels=['cross_360p_high'], axis=1, inplace=True)\n",
    "df.drop(labels=['cross_1440p_high'], axis=1, inplace=True)\n",
    "\n",
    "df_to_dataset_y = lb.transform(df[['y_result']].copy())\n",
    "df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "\n",
    "train_data_no = int(len(df_to_dataset_y)/2)\n",
    "test_data_no = int(train_data_no/2)\n",
    "v_data_no = test_data_no\n",
    "\n",
    "train_x_1 = df_to_dataset.iloc[:train_data_no].values\n",
    "train_x_2 = df_to_dataset_input2.iloc[:train_data_no].values\n",
    "train_y = df_to_dataset_y[:train_data_no]                                                   \n",
    "\n",
    "test_x_1 = df_to_dataset.iloc[train_data_no:train_data_no+test_data_no].values\n",
    "test_x_2 = df_to_dataset_input2.iloc[train_data_no:train_data_no+test_data_no].values\n",
    "test_y = df_to_dataset_y[train_data_no:train_data_no+test_data_no]                                                   \n",
    "\n",
    "v_x_1 = df_to_dataset.iloc[train_data_no+test_data_no:].values\n",
    "v_x_2 = df_to_dataset_input2.iloc[train_data_no+test_data_no:].values\n",
    "v_y = df_to_dataset_y[train_data_no+test_data_no:]                                                   \n",
    "\n",
    "\n",
    "def make_window_dataset(ds, window_size=seq_len, shift=1, stride=1):\n",
    "  windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "  def batch(sub):\n",
    "    ret = ()\n",
    "    for index in range(2):\n",
    "      ret = ret + ( sub[index].batch(window_size, drop_remainder=True), )\n",
    "    return ret\n",
    "  def sub_to_batch(sub, sub2): \n",
    "    return tf.data.Dataset.zip((batch(sub), (sub2.batch(window_size, drop_remainder=True))))\n",
    "  \n",
    "  windows = windows.flat_map(sub_to_batch)\n",
    "  windows = windows.map(lambda sub1, sub2: ((sub1[0], sub1[1][-1:][0]), (sub2[-1:][0], sub2[-1:][0])))\n",
    "  return windows\n",
    "\n",
    "def filter_fn(a, b):\n",
    "  return a[1][0]>=upperTailFilter or a[1][1]>=lowerTailFilter or a[1][3]==1 or a[1][4]==1\n",
    "\n",
    "train_dataset_x = tf.data.Dataset.from_tensor_slices(((train_x_1, train_x_2),(train_y)))\n",
    "train_dataset = make_window_dataset(train_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True).repeat(2) \n",
    "\n",
    "test_dataset_x = tf.data.Dataset.from_tensor_slices(((test_x_1, test_x_2),(test_y)))\n",
    "test_dataset = make_window_dataset(test_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True).repeat(2) \n",
    "\n",
    "v_dataset_x = tf.data.Dataset.from_tensor_slices(((v_x_1, v_x_2),(v_y)))\n",
    "v_dataset = make_window_dataset(v_dataset_x).filter(filter_fn).cache().batch(batch_size, drop_remainder=True).repeat(2)  \n",
    "\n",
    "\n",
    "print('Total Size : {}'.format(train_dataset_x))\n",
    "print('Total Filtered Size : {}'.format(v_dataset_x))\n",
    "\n",
    "\n",
    "# X_tr, X_tr2, lab_tr, X_test, X_test2, lab_test, X_vld, X_vld2, lab_vld = load_data(df, seq_len)\n",
    "# y_tr = lb.transform(lab_tr)\n",
    "# y_vld = lb.transform(lab_vld)\n",
    "# y_test = lb.transform(lab_test)\n",
    "\n",
    "\n",
    "# train_X = X_tr\n",
    "# train_X2 = X_tr2\n",
    "# train_y = y_tr\n",
    "# valid_X = X_vld\n",
    "# valid_X2 = X_vld2\n",
    "# valid_y = y_vld\n",
    "# test_X = X_test\n",
    "# test_X2 = X_test2\n",
    "# test_y = y_test\n",
    "\n",
    "# print(train_X.shape)\n",
    "# print(train_y[0])\n",
    "# print(train_y[1])\n",
    "# print(train_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shujian/transformer-with-lstm\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "\n",
    "\n",
    "embed_size = 60\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def CnnTransformerModel():\n",
    "#    i = tf.compat.v2.keras.layers.Flatten(input_shape=(batch_size, amount_of_features))\n",
    "    i = tf.compat.v2.keras.layers.Input(shape = (seq_len, amount_of_features)#, batch_size=mini_batch_size\n",
    "                                       )\n",
    "    \n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64*2, kernel_size = 4, strides = 2)(i)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 4, strides = 2)(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(32, kernel_size = 4, strides = 2)(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(32, kernel_size = 4, strides = 2)(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    \n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(i)\n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    " \n",
    "#    x = (CuDNNLSTM(16, return_sequences = True, return_state = False))(x)\n",
    "    x, slf_attn = MultiHeadAttention(n_head=80, d_model=300, d_k=64, d_v=64, dropout=0.3)(x, x, x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    tailInput = Input(shape=(input2Length,))\n",
    "    tailLayers = Dense(input2Length, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(input2Length*2, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(input2Length*2*2, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(input2Length*2, activation='tanh')(tailInput)\n",
    "    tailLayers = Dense(3, activation='sigmoid')(tailInput)\n",
    "    \n",
    "    concat = concatenate([avg_pool, max_pool, tailLayers])\n",
    "    y = Dense(3,activation = 'softmax')(concat)\n",
    "    \n",
    "\n",
    "    return Model(inputs = [i, tailInput], outputs = [y, tailLayers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnTransformerModel()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.090, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, \n",
    "                          loss='categorical_crossentropy',\n",
    "                          loss_weights=[1., 1.]\n",
    "#                           loss_weights=[1.]\n",
    "#               \n",
    "                            , metrics=[Recall(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                 , Recall(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                   , Precision(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                  , Precision(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                  ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing The Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes = True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bed51171db8bf8c5e1731119fd31205bfd80a82"
   },
   "source": [
    "Train the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "e1061c7cde0687450300f516735aad0cc5dbf08f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "logdir = \"logs\\\\scalars\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=500) \n",
    "callbacks_list = [checkpoint\n",
    "                  , early\n",
    "                  , reduceLROnPlat\n",
    "                  , tensorboard_callback\n",
    "                 ]\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "model.fit(train_dataset,\n",
    "                      validation_data = v_dataset, \n",
    "#                       batch_size = 10,\n",
    "                      epochs = 500,\n",
    "                      callbacks = callbacks_list)\n",
    "\n",
    "# model.fit([train_X, train_X2], [train_y, train_y],\n",
    "#                       validation_data = ([valid_X, valid_X2], [valid_y, valid_y]), \n",
    "#                       batch_size = batch_size,\n",
    "#                       epochs = 500,\n",
    "#                       callbacks = callbacks_list)\n",
    "\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b4790925984514e64ca5a9b46de8b309062e0cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_path)\n",
    "lstm_results = model.evaluate(test_dataset, return_dict=True)\n",
    "print(lstm_results)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[0], 100*lstm_results[1]))\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%, Recall S %2.8f%%, Recall L %2.8f%%, Precision S %2.8f%%, Precision L %2.8f%%' % (100*lstm_results[1], 100*lstm_results[2], lstm_results[3], lstm_results[4], lstm_results[5], lstm_results[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm_results)\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[0], 100*lstm_results[1]))\n",
    "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%, Recall S %2.8f%%, Recall L %2.8f%%, Precision S %2.8f%%, Precision L %2.8f%%' % (100*lstm_results[1], 100*lstm_results[2], lstm_results[3], lstm_results[4], lstm_results[5], lstm_results[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./trained_model.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3340d6d6ce75585f90c98f1728b1cd664d7f33f"
   },
   "source": [
    "Load and normalize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(ts_length = 150000):\n",
    "    base_dir = 'input/test/'\n",
    "    test_files = [f for f in listdir(base_dir) if isfile(join(base_dir, f))]\n",
    "\n",
    "    ts = np.empty([len(test_files), ts_length])\n",
    "    ids = []\n",
    "    \n",
    "    i = 0\n",
    "    for f in tqdm_notebook(test_files):\n",
    "        ids.append(splitext(f)[0])\n",
    "        t_df = pd.read_csv(base_dir + f, dtype={\"acoustic_data\": np.int8})\n",
    "        ts[i, :] = t_df['acoustic_data'].values\n",
    "        i = i + 1\n",
    "\n",
    "    return ts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f005579ea08913f4f68a3749bd761df6cef2b1b"
   },
   "outputs": [],
   "source": [
    "test_data, test_ids = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c3b7a864a9f53af142a08883def46c3866c5464"
   },
   "outputs": [],
   "source": [
    "X_test = test_data\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf9b36929e5228d4d94b3b7ad1b9011bf088ac44"
   },
   "source": [
    "Load best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "435449fda2bf96635e67d69f56227e140c4cea99"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9aaf9fb44edba5879a75c68820527d9180d2b3c6"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'seg_id': test_ids, 'time_to_failure': y_pred[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b9d5c63161f637de2e39b59e8e4d7c2f3049581"
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"submission.csv\"> Download File </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.transform([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nn = np.array([[1., 0.,2], [2., 1.,3], [0., 0.,4]])\n",
    "print(nn[1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(valid_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(valid_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, \n",
    "                          loss='categorical_crossentropy',\n",
    "                          loss_weights=[1., 1.]\n",
    "#                           loss_weights=[1.]\n",
    "#               \n",
    "                            , metrics=[Recall(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                 , Recall(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                   , Precision(thresholds=0.5, class_id=0, top_k=1)\n",
    "                                  , Precision(thresholds=0.5, class_id=2, top_k=1)\n",
    "                                  ]\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
