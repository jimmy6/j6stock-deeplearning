{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "e1baa7518f18a7ff1f2767df1b29c051955502ff"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gc\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, basename, splitext, isfile, exists\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.metrics import top_k_categorical_accuracy, CategoricalHinge, Recall, Precision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.compat.v2.keras.layers import Input\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random, os, sys\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.initializers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "tf.compat.v1.disable_eager_execution\n",
    "pd.set_option('precision', 30)\n",
    "np.set_printoptions(precision = 30)\n",
    "\n",
    "## For float16 ##\n",
    "tf.compat.v2.keras.backend.set_floatx('float16')\n",
    "# default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\n",
    "tf.compat.v2.keras.backend.set_epsilon(1e-4) \n",
    "## End for float16 ##\n",
    "\n",
    "\n",
    "#np.random.seed(368)\n",
    "#tf.random.set_seed(368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction\\lib\\site-packages\\ipykernel_launcher.py:124: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data = 947135\n",
      "Split = 473568\n",
      "Amount of training data = 473568\n",
      "Amount of validation data = 84370\n",
      "Amount of testing data = 84370\n",
      "(473568, 300, 5)\n",
      "[0 1 0]\n",
      "[0 1 0]\n",
      "[1 0 0]\n"
     ]
    }
   ],
   "source": [
    "xfile='C:\\\\workspace\\\\j6stock\\\\xau_usd_OHLC2.0Tp1.0Cl100Vp.txt'\n",
    "\n",
    "seq_len = 60*5 # 3 days + 2 features is enough memory\n",
    "batch_size = 2048*6       # Batch size\n",
    "mini_batch_size = 64       # Batch size\n",
    "\n",
    "learning_rate = 0.001  #0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "y_column = 6\n",
    "compute_val_at = 0\n",
    "acc_filtered_r = 0.8\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers.core import Dense, Dropout, Activation\n",
    "#from keras.layers.recurrent import LSTM\n",
    "#from keras.models import load_model\n",
    "#import keras\n",
    "import pandas as pd ## can be remove once pandas_datareader 0.7 using\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like ## can be remove once pandas_datareader 0.7 using\n",
    "import pandas_datareader.data as web\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "#from keras import backend as K\n",
    "\n",
    "def make_window_dataset(ds, window_size=5, shift=1, stride=1):\n",
    "    windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "\n",
    "    def sub_to_batch(sub, y):\n",
    "        print (sub)\n",
    "        print (y)\n",
    "        s = sub.batch(window_size, drop_remainder=True)\n",
    "        yy = y.batch(window_size, drop_remainder=True)\n",
    "        return (s, yy)\n",
    "\n",
    "    windows = windows.flat_map(sub_to_batch)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "    #TODO Use previous close instead of open\n",
    "    df['change'] = df['close'] - df['open']\n",
    "    df['tail'] = df['low']\n",
    "    for i, row in df.iterrows():\n",
    "        df.at[i, 'high'] = df.at[i, 'high'] - (df.at[i, 'open'] if df.at[i, 'open'] > df.at[i, 'close'] else df.at[i, 'close'])\n",
    "        df.at[i, 'low'] = (df.at[i, 'close'] if df.at[i, 'close'] < df.at[i, 'open'] else df.at[i, 'open']) - df.at[i, 'low']\n",
    "        df.at[i, 'tail'] = df.at[i, 'high'] if df.at[i, 'high'] > df.at[i, 'low'] else ( df.at[i, 'low'] * -1)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop('open', axis=1)\n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            if moving == 5:\n",
    "                df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean()\n",
    "            else:\n",
    "                df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean() - df['close'] # minus close to focus up/lower price\n",
    "            \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "        \n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        #df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        #df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        #df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "#         df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
    "        df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1))\n",
    "        df['tail'] = min_max_scaler.fit_transform(df['tail'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))   \n",
    "                #pd.concat([min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1)), df], axis=1)\n",
    "                ma_data = df['{}ma'.format(moving)]\n",
    "                df.drop(labels=['{}ma'.format(moving)], axis=1, inplace=True)\n",
    "                df = pd.concat([ma_data, df], axis=1)\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "               \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "    \n",
    "    #drop for wavenet and not enough memory\n",
    "    #df.drop(labels=['change'], axis=1, inplace=True)\n",
    "    df.drop(labels=['high'], axis=1, inplace=True)\n",
    "    df.drop(labels=['low'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df = get_stock_data( ma=[50, 100, 200])\n",
    "df = get_stock_data(ma=[5, 240, 240*2]\n",
    "                   )\n",
    "input2Length = 1\n",
    "amount_of_features = len(df.columns)-1+(input2Length*-1)\n",
    "\n",
    "def load_data(stock, seq_len):\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    x_result = []\n",
    "    x_result2 = []\n",
    "    y_result = []\n",
    "    for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "        x_result.append(data[index-seq_len: index,\n",
    "                             :-1 + (input2Length*-1) # -2 is ignore Input2 features\n",
    "                            ]) # index : index + 22days\n",
    "        x_result2.append(data[index, -1 + (input2Length*-1):-1])\n",
    "        y_result.append(data[index ,-1]);\n",
    "\n",
    "    x_result, x_result2, y_result = shuffle(x_result, x_result2, y_result , random_state=2)\n",
    "\n",
    "    del [[stock,data]]\n",
    "    gc.collect()\n",
    "    stock=pd.DataFrame()\n",
    "    data=pd.DataFrame()\n",
    "\n",
    "    #print('---', data[0])\n",
    "    #print('---', x_result[0])\n",
    "    #print('---', y_result[0])\n",
    "    x_result = np.array(x_result)\n",
    "    x_result2 = np.array(x_result2)\n",
    "    y_result = np.array(y_result)\n",
    "    print (\"Amount of data = {}\".format(y_result.shape[0]))\n",
    "\n",
    "    percentageSplit = 0.5 # 60% split\n",
    "    row = round(percentageSplit * y_result.shape[0]) \n",
    "    print (\"Split = {}\".format(row))\n",
    " \n",
    "    X_train = x_result[:int(row), :] \n",
    "    X_train2 = x_result2[:int(row), :] \n",
    "    y_train = y_result[:int(row)] \n",
    "    print (\"Amount of training data = {}\".format(y_train.shape[0]))\n",
    "    X_test = x_result[int(row):, :]\n",
    "    X_test2 = x_result2[int(row):, :]\n",
    "    y_test = y_result[int(row):]\n",
    "    # filter for 1 and -1 for validation only\n",
    "    X_test = X_test[y_test[:]!=0,:]\n",
    "    X_test2 = X_test2[y_test[:]!=0,:]\n",
    "    y_test = y_test[y_test[:]!=0]\n",
    "    \n",
    "    # split 50% again for test and validation set\n",
    "    row = round(percentageSplit * y_test.shape[0]) \n",
    "    X_val = X_test[int(row):, :]\n",
    "    X_val2 = X_test2[int(row):, :]\n",
    "    y_val = y_test[int(row):]\n",
    "    print (\"Amount of validation data = {}\".format(y_val.shape[0]))\n",
    "    X_test = X_test[:int(row), :]\n",
    "    X_test2 = X_test2[:int(row), :]\n",
    "    y_test = y_test[:int(row)]\n",
    "    print (\"Amount of testing data = {}\".format(y_test.shape[0]))\n",
    "    \n",
    "\n",
    "           \n",
    "    #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "    #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))\n",
    "    \n",
    "    return [X_train, X_train2, y_train, X_test, X_test2, y_test, X_val, X_val2, y_val]\n",
    "\n",
    "\n",
    "\n",
    "classes = [1, 0, -1]\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(classes)\n",
    "lb.transform([-1, 0, 1])\n",
    "\n",
    "X_tr, X_tr2, lab_tr, X_test, X_test2, lab_test, X_vld, X_vld2, lab_vld = load_data(df, seq_len)\n",
    "y_tr = lb.transform(lab_tr)\n",
    "y_vld = lb.transform(lab_vld)\n",
    "y_test = lb.transform(lab_test)\n",
    "\n",
    "\n",
    "train_X = X_tr\n",
    "train_X2 = X_tr2\n",
    "train_y = y_tr\n",
    "valid_X = X_vld\n",
    "valid_X2 = X_vld2\n",
    "valid_y = y_vld\n",
    "test_X = X_test\n",
    "test_X2 = X_test2\n",
    "test_y = y_test\n",
    "print(train_X.shape)\n",
    "print(train_y[0])\n",
    "print(train_y[1])\n",
    "print(train_y[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/shujian/transformer-with-lstm\n",
    "\n",
    "try:\n",
    "    from dataloader import TokenList, pad_to_longest\n",
    "    # for transformer\n",
    "except: pass\n",
    "\n",
    "\n",
    "\n",
    "embed_size = 60\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-4, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ScaledDotProductAttention():\n",
    "    def __init__(self, d_model, attn_dropout=0.1):\n",
    "        self.temper = np.sqrt(d_model)\n",
    "        self.dropout = Dropout(attn_dropout)\n",
    "    def __call__(self, q, k, v, mask):\n",
    "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
    "        if mask is not None:\n",
    "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
    "            attn = Add()([attn, mmask])\n",
    "        attn = Activation('softmax')(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention():\n",
    "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
    "        self.mode = mode\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.dropout = dropout\n",
    "        if mode == 0:\n",
    "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
    "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
    "        elif mode == 1:\n",
    "            self.qs_layers = []\n",
    "            self.ks_layers = []\n",
    "            self.vs_layers = []\n",
    "            for _ in range(n_head):\n",
    "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
    "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
    "        self.attention = ScaledDotProductAttention(d_model)\n",
    "        self.layer_norm = LayerNormalization() if use_norm else None\n",
    "        self.w_o = TimeDistributed(Dense(d_model))\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        d_k, d_v = self.d_k, self.d_v\n",
    "        n_head = self.n_head\n",
    "\n",
    "        if self.mode == 0:\n",
    "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
    "            ks = self.ks_layer(k)\n",
    "            vs = self.vs_layer(v)\n",
    "\n",
    "            def reshape1(x):\n",
    "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
    "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
    "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
    "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
    "                return x\n",
    "            qs = Lambda(reshape1)(qs)\n",
    "            ks = Lambda(reshape1)(ks)\n",
    "            vs = Lambda(reshape1)(vs)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
    "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
    "                \n",
    "            def reshape2(x):\n",
    "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
    "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
    "                x = tf.transpose(x, [1, 2, 0, 3])\n",
    "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
    "                return x\n",
    "            head = Lambda(reshape2)(head)\n",
    "        elif self.mode == 1:\n",
    "            heads = []; attns = []\n",
    "            for i in range(n_head):\n",
    "                qs = self.qs_layers[i](q)   \n",
    "                ks = self.ks_layers[i](k) \n",
    "                vs = self.vs_layers[i](v) \n",
    "                head, attn = self.attention(qs, ks, vs, mask)\n",
    "                heads.append(head); attns.append(attn)\n",
    "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
    "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
    "\n",
    "        outputs = self.w_o(head)\n",
    "        outputs = Dropout(self.dropout)(outputs)\n",
    "        if not self.layer_norm: return outputs, attn\n",
    "        # outputs = Add()([outputs, q]) # sl: fix\n",
    "        return self.layer_norm(outputs), attn\n",
    "\n",
    "class PositionwiseFeedForward():\n",
    "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
    "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
    "        self.w_2 = Conv1D(d_hid, 1)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        output = self.w_1(x) \n",
    "        output = self.w_2(output)\n",
    "        output = self.dropout(output)\n",
    "        output = Add()([output, x])\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "class EncoderLayer():\n",
    "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
    "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
    "    def __call__(self, enc_input, mask=None):\n",
    "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
    "        output = self.pos_ffn_layer(output)\n",
    "        return output, slf_attn\n",
    "\n",
    "\n",
    "def GetPosEncodingMatrix(max_len, d_emb):\n",
    "    pos_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
    "        if pos != 0 else np.zeros(d_emb) \n",
    "            for pos in range(max_len)\n",
    "            ])\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "def GetPadMask(q, k):\n",
    "    ones = K.expand_dims(K.ones_like(q, 'float16'), -1)\n",
    "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float16')\n",
    "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
    "    return mask\n",
    "\n",
    "def GetSubMask(s):\n",
    "    len_s = tf.shape(s)[1]\n",
    "    bs = tf.shape(s)[:1]\n",
    "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CnnTransformerModel():\n",
    "#    i = tf.compat.v2.keras.layers.Flatten(input_shape=(batch_size, amount_of_features))\n",
    "    i = tf.compat.v2.keras.layers.Input(shape = (seq_len, amount_of_features)#, batch_size= 500) #mini_batch_size\n",
    "                                        , dtype=tf.compat.v2.keras.backend.floatx()\n",
    "                                       )\n",
    "    \n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 4, strides = 2 , dtype=tf.compat.v2.keras.backend.floatx())(i)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 4, strides = 2 , dtype=tf.compat.v2.keras.backend.floatx())(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    x = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 4, strides = 2 , dtype=tf.compat.v2.keras.backend.floatx())(x)\n",
    "    x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "#     x = tf.compat.v2.keras.layers.Convolution1D(32, kernel_size = 4, strides = 2)(x)\n",
    "#     x = tf.compat.v2.keras.layers.BatchNormalization()(x)\n",
    "#     x = tf.compat.v2.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(i)\n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    "#    x = Convolution1D(16, kernel_size = 10, strides = 10, activation='relu')(x)\n",
    " \n",
    "#    x = (CuDNNLSTM(16, return_sequences = True, return_state = False))(x)\n",
    "    x, slf_attn = MultiHeadAttention(n_head=15, d_model=300, d_k=64, d_v=64, dropout=0.3)(x, x, x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    input2 = Input(shape=(input2Length,), dtype=tf.compat.v2.keras.backend.floatx())\n",
    "    if input2Length != 1:\n",
    "        input2Layers = Dense(input2Length*5, activation='sigmoid')(input2)\n",
    "        input2Layers = Dense(input2Length, activation='sigmoid')(input2Layers)\n",
    "    else:\n",
    "        input2Layers = Dense(input2Length, activation='sigmoid')(input2)\n",
    "\n",
    "    x2 = tf.compat.v2.keras.layers.Convolution1D(64, kernel_size = 4, strides = 2 , dtype=tf.compat.v2.keras.backend.floatx())(i)\n",
    "    x2 = tf.compat.v2.keras.layers.BatchNormalization()(x2)\n",
    "    x2 = tf.compat.v2.keras.layers.Activation('relu')(x2)\n",
    "    \n",
    "    concat = concatenate([avg_pool, max_pool, input2Layers])\n",
    "    y = Dense(3,activation = 'softmax')(concat)\n",
    "    \n",
    "\n",
    "    return Model(inputs = [i, input2], outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 300, 5)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 149, 64)      1344        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 149, 64)      256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 149, 64)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 73, 64)       16448       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 73, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 73, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 35, 64)       16448       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 35, 960)      61440       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 35, 960)      61440       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, None, 64)     0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, None, 64)     0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, None, None)   0           lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None)   0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 35, 960)      61440       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, None)   0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, None, 64)     0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, None, 64)     0           dropout_4[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, None, 960)    0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 300)    288300      lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 300)    0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, None, 300)    600         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 300)          0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 300)          0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            2           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 601)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 3)            1806        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 510,036\n",
      "Trainable params: 509,652\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CnnTransformerModel()\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.010, beta_1=0.9, beta_2=0.9999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer, \n",
    "                          loss='categorical_crossentropy'\n",
    "#                             , metrics=[Recall(thresholds=0.5,class_id=0, top_k=1)\n",
    "#                                  , Recall(thresholds=0.5,class_id=2, top_k=1)\n",
    "#                                    , Precision(thresholds=0.5,class_id=0, top_k=1)\n",
    "#                                   , Precision(thresholds=0.5,class_id=2, top_k=1)\n",
    "#                                   ]\n",
    "             )\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing The Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"2508pt\" viewBox=\"0.00 0.00 1255.00 1881.00\" width=\"1673pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 1877)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1877 1251,-1877 1251,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 3006027028128 -->\n",
       "<g class=\"node\" id=\"node1\"><title>3006027028128</title>\n",
       "<polygon fill=\"none\" points=\"286,-1826.5 286,-1872.5 551,-1872.5 551,-1826.5 286,-1826.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349\" y=\"-1845.8\">input_5: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"412,-1826.5 412,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440\" y=\"-1857.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"412,-1849.5 468,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440\" y=\"-1834.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"468,-1826.5 468,-1872.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"509.5\" y=\"-1857.3\">[(?, 300, 5)]</text>\n",
       "<polyline fill=\"none\" points=\"468,-1849.5 551,-1849.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"509.5\" y=\"-1834.3\">[(?, 300, 5)]</text>\n",
       "</g>\n",
       "<!-- 3006027027624 -->\n",
       "<g class=\"node\" id=\"node2\"><title>3006027027624</title>\n",
       "<polygon fill=\"none\" points=\"287,-1743.5 287,-1789.5 550,-1789.5 550,-1743.5 287,-1743.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-1762.8\">conv1d_7: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"414,-1743.5 414,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442\" y=\"-1774.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"414,-1766.5 470,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442\" y=\"-1751.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"470,-1743.5 470,-1789.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1774.3\">(?, 300, 5)</text>\n",
       "<polyline fill=\"none\" points=\"470,-1766.5 550,-1766.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1751.3\">(?, 149, 64)</text>\n",
       "</g>\n",
       "<!-- 3006027028128&#45;&gt;3006027027624 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>3006027028128-&gt;3006027027624</title>\n",
       "<path d=\"M418.5,-1826.37C418.5,-1818.15 418.5,-1808.66 418.5,-1799.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1799.61 418.5,-1789.61 415,-1799.61 422,-1799.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3028241423216 -->\n",
       "<g class=\"node\" id=\"node3\"><title>3028241423216</title>\n",
       "<polygon fill=\"none\" points=\"220.5,-1660.5 220.5,-1706.5 616.5,-1706.5 616.5,-1660.5 220.5,-1660.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-1679.8\">batch_normalization_7: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-1660.5 480.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"508.5\" y=\"-1691.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-1683.5 536.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"508.5\" y=\"-1668.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"536.5,-1660.5 536.5,-1706.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1691.3\">(?, 149, 64)</text>\n",
       "<polyline fill=\"none\" points=\"536.5,-1683.5 616.5,-1683.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1668.3\">(?, 149, 64)</text>\n",
       "</g>\n",
       "<!-- 3006027027624&#45;&gt;3028241423216 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>3006027027624-&gt;3028241423216</title>\n",
       "<path d=\"M418.5,-1743.37C418.5,-1735.15 418.5,-1725.66 418.5,-1716.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1716.61 418.5,-1706.61 415,-1716.61 422,-1716.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3006027080256 -->\n",
       "<g class=\"node\" id=\"node4\"><title>3006027080256</title>\n",
       "<polygon fill=\"none\" points=\"276.5,-1577.5 276.5,-1623.5 560.5,-1623.5 560.5,-1577.5 276.5,-1577.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-1596.8\">activation_9: Activation</text>\n",
       "<polyline fill=\"none\" points=\"424.5,-1577.5 424.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.5\" y=\"-1608.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"424.5,-1600.5 480.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.5\" y=\"-1585.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-1577.5 480.5,-1623.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"520.5\" y=\"-1608.3\">(?, 149, 64)</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-1600.5 560.5,-1600.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"520.5\" y=\"-1585.3\">(?, 149, 64)</text>\n",
       "</g>\n",
       "<!-- 3028241423216&#45;&gt;3006027080256 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>3028241423216-&gt;3006027080256</title>\n",
       "<path d=\"M418.5,-1660.37C418.5,-1652.15 418.5,-1642.66 418.5,-1633.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1633.61 418.5,-1623.61 415,-1633.61 422,-1633.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029854925656 -->\n",
       "<g class=\"node\" id=\"node5\"><title>3029854925656</title>\n",
       "<polygon fill=\"none\" points=\"287,-1494.5 287,-1540.5 550,-1540.5 550,-1494.5 287,-1494.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-1513.8\">conv1d_8: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"414,-1494.5 414,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442\" y=\"-1525.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"414,-1517.5 470,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442\" y=\"-1502.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"470,-1494.5 470,-1540.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1525.3\">(?, 149, 64)</text>\n",
       "<polyline fill=\"none\" points=\"470,-1517.5 550,-1517.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1502.3\">(?, 73, 64)</text>\n",
       "</g>\n",
       "<!-- 3006027080256&#45;&gt;3029854925656 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>3006027080256-&gt;3029854925656</title>\n",
       "<path d=\"M418.5,-1577.37C418.5,-1569.15 418.5,-1559.66 418.5,-1550.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1550.61 418.5,-1540.61 415,-1550.61 422,-1550.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029855681112 -->\n",
       "<g class=\"node\" id=\"node6\"><title>3029855681112</title>\n",
       "<polygon fill=\"none\" points=\"223.5,-1411.5 223.5,-1457.5 613.5,-1457.5 613.5,-1411.5 223.5,-1411.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-1430.8\">batch_normalization_8: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-1411.5 483.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.5\" y=\"-1442.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-1434.5 539.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.5\" y=\"-1419.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"539.5,-1411.5 539.5,-1457.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1442.3\">(?, 73, 64)</text>\n",
       "<polyline fill=\"none\" points=\"539.5,-1434.5 613.5,-1434.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1419.3\">(?, 73, 64)</text>\n",
       "</g>\n",
       "<!-- 3029854925656&#45;&gt;3029855681112 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>3029854925656-&gt;3029855681112</title>\n",
       "<path d=\"M418.5,-1494.37C418.5,-1486.15 418.5,-1476.66 418.5,-1467.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1467.61 418.5,-1457.61 415,-1467.61 422,-1467.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029855826384 -->\n",
       "<g class=\"node\" id=\"node7\"><title>3029855826384</title>\n",
       "<polygon fill=\"none\" points=\"276,-1328.5 276,-1374.5 561,-1374.5 561,-1328.5 276,-1328.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-1347.8\">activation_10: Activation</text>\n",
       "<polyline fill=\"none\" points=\"431,-1328.5 431,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-1359.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"431,-1351.5 487,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-1336.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"487,-1328.5 487,-1374.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-1359.3\">(?, 73, 64)</text>\n",
       "<polyline fill=\"none\" points=\"487,-1351.5 561,-1351.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-1336.3\">(?, 73, 64)</text>\n",
       "</g>\n",
       "<!-- 3029855681112&#45;&gt;3029855826384 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>3029855681112-&gt;3029855826384</title>\n",
       "<path d=\"M418.5,-1411.37C418.5,-1403.15 418.5,-1393.66 418.5,-1384.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1384.61 418.5,-1374.61 415,-1384.61 422,-1384.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029855680832 -->\n",
       "<g class=\"node\" id=\"node8\"><title>3029855680832</title>\n",
       "<polygon fill=\"none\" points=\"290,-1245.5 290,-1291.5 547,-1291.5 547,-1245.5 290,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-1264.8\">conv1d_9: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"417,-1245.5 417,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"445\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"417,-1268.5 473,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"445\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"473,-1245.5 473,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1276.3\">(?, 73, 64)</text>\n",
       "<polyline fill=\"none\" points=\"473,-1268.5 547,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"510\" y=\"-1253.3\">(?, 35, 64)</text>\n",
       "</g>\n",
       "<!-- 3029855826384&#45;&gt;3029855680832 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>3029855826384-&gt;3029855680832</title>\n",
       "<path d=\"M418.5,-1328.37C418.5,-1320.15 418.5,-1310.66 418.5,-1301.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1301.61 418.5,-1291.61 415,-1301.61 422,-1301.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029856329456 -->\n",
       "<g class=\"node\" id=\"node9\"><title>3029856329456</title>\n",
       "<polygon fill=\"none\" points=\"223.5,-1162.5 223.5,-1208.5 613.5,-1208.5 613.5,-1162.5 223.5,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-1181.8\">batch_normalization_9: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-1162.5 483.5,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"483.5,-1185.5 539.5,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"511.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"539.5,-1162.5 539.5,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1193.3\">(?, 35, 64)</text>\n",
       "<polyline fill=\"none\" points=\"539.5,-1185.5 613.5,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"576.5\" y=\"-1170.3\">(?, 35, 64)</text>\n",
       "</g>\n",
       "<!-- 3029855680832&#45;&gt;3029856329456 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>3029855680832-&gt;3029856329456</title>\n",
       "<path d=\"M418.5,-1245.37C418.5,-1237.15 418.5,-1227.66 418.5,-1218.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1218.61 418.5,-1208.61 415,-1218.61 422,-1218.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029856417440 -->\n",
       "<g class=\"node\" id=\"node10\"><title>3029856417440</title>\n",
       "<polygon fill=\"none\" points=\"276,-1079.5 276,-1125.5 561,-1125.5 561,-1079.5 276,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-1098.8\">activation_11: Activation</text>\n",
       "<polyline fill=\"none\" points=\"431,-1079.5 431,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"431,-1102.5 487,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"459\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"487,-1079.5 487,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-1110.3\">(?, 35, 64)</text>\n",
       "<polyline fill=\"none\" points=\"487,-1102.5 561,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"524\" y=\"-1087.3\">(?, 35, 64)</text>\n",
       "</g>\n",
       "<!-- 3029856329456&#45;&gt;3029856417440 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>3029856329456-&gt;3029856417440</title>\n",
       "<path d=\"M418.5,-1162.37C418.5,-1154.15 418.5,-1144.66 418.5,-1135.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1135.61 418.5,-1125.61 415,-1135.61 422,-1135.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029856528256 -->\n",
       "<g class=\"node\" id=\"node11\"><title>3029856528256</title>\n",
       "<polygon fill=\"none\" points=\"20,-996.5 20,-1042.5 267,-1042.5 267,-996.5 20,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"75.5\" y=\"-1015.8\">dense_12: Dense</text>\n",
       "<polyline fill=\"none\" points=\"131,-996.5 131,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-1019.5 187,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187,-996.5 187,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-1027.3\">(?, 35, 64)</text>\n",
       "<polyline fill=\"none\" points=\"187,-1019.5 267,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-1004.3\">(?, 35, 960)</text>\n",
       "</g>\n",
       "<!-- 3029856417440&#45;&gt;3029856528256 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>3029856417440-&gt;3029856528256</title>\n",
       "<path d=\"M343.718,-1079.47C307.948,-1068.94 264.97,-1056.28 228.019,-1045.39\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"228.848,-1041.99 218.267,-1042.52 226.87,-1048.71 228.848,-1041.99\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857190800 -->\n",
       "<g class=\"node\" id=\"node12\"><title>3029857190800</title>\n",
       "<polygon fill=\"none\" points=\"295,-996.5 295,-1042.5 542,-1042.5 542,-996.5 295,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-1015.8\">dense_13: Dense</text>\n",
       "<polyline fill=\"none\" points=\"406,-996.5 406,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"406,-1019.5 462,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"462,-996.5 462,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502\" y=\"-1027.3\">(?, 35, 64)</text>\n",
       "<polyline fill=\"none\" points=\"462,-1019.5 542,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502\" y=\"-1004.3\">(?, 35, 960)</text>\n",
       "</g>\n",
       "<!-- 3029856417440&#45;&gt;3029857190800 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>3029856417440-&gt;3029857190800</title>\n",
       "<path d=\"M418.5,-1079.37C418.5,-1071.15 418.5,-1061.66 418.5,-1052.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-1052.61 418.5,-1042.61 415,-1052.61 422,-1052.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857191248 -->\n",
       "<g class=\"node\" id=\"node17\"><title>3029857191248</title>\n",
       "<polygon fill=\"none\" points=\"570,-913.5 570,-959.5 817,-959.5 817,-913.5 570,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625.5\" y=\"-932.8\">dense_14: Dense</text>\n",
       "<polyline fill=\"none\" points=\"681,-913.5 681,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"709\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"681,-936.5 737,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"709\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"737,-913.5 737,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"777\" y=\"-944.3\">(?, 35, 64)</text>\n",
       "<polyline fill=\"none\" points=\"737,-936.5 817,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"777\" y=\"-921.3\">(?, 35, 960)</text>\n",
       "</g>\n",
       "<!-- 3029856417440&#45;&gt;3029857191248 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>3029856417440-&gt;3029857191248</title>\n",
       "<path d=\"M475.719,-1079.36C499.44,-1069.37 526.876,-1056.73 550.5,-1043 589.831,-1020.15 631.293,-988.62 659.384,-965.993\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"661.739,-968.59 667.297,-959.57 657.327,-963.155 661.739,-968.59\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857499064 -->\n",
       "<g class=\"node\" id=\"node13\"><title>3029857499064</title>\n",
       "<polygon fill=\"none\" points=\"0,-913.5 0,-959.5 267,-959.5 267,-913.5 0,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"65.5\" y=\"-932.8\">lambda_12: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"131,-913.5 131,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-936.5 187,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187,-913.5 187,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-944.3\">(?, 35, 960)</text>\n",
       "<polyline fill=\"none\" points=\"187,-936.5 267,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227\" y=\"-921.3\">(?, ?, 64)</text>\n",
       "</g>\n",
       "<!-- 3029856528256&#45;&gt;3029857499064 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>3029856528256-&gt;3029857499064</title>\n",
       "<path d=\"M140.767,-996.366C139.753,-988.152 138.581,-978.658 137.478,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"140.928,-969.103 136.229,-959.607 133.981,-969.96 140.928,-969.103\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857741232 -->\n",
       "<g class=\"node\" id=\"node14\"><title>3029857741232</title>\n",
       "<polygon fill=\"none\" points=\"285,-913.5 285,-959.5 552,-959.5 552,-913.5 285,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350.5\" y=\"-932.8\">lambda_13: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"416,-913.5 416,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"416,-936.5 472,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"472,-913.5 472,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"512\" y=\"-944.3\">(?, 35, 960)</text>\n",
       "<polyline fill=\"none\" points=\"472,-936.5 552,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"512\" y=\"-921.3\">(?, ?, 64)</text>\n",
       "</g>\n",
       "<!-- 3029857190800&#45;&gt;3029857741232 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>3029857190800-&gt;3029857741232</title>\n",
       "<path d=\"M418.5,-996.366C418.5,-988.152 418.5,-978.658 418.5,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"422,-969.607 418.5,-959.607 415,-969.607 422,-969.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029858105608 -->\n",
       "<g class=\"node\" id=\"node15\"><title>3029858105608</title>\n",
       "<polygon fill=\"none\" points=\"223,-830.5 223,-876.5 542,-876.5 542,-830.5 223,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"288.5\" y=\"-849.8\">lambda_15: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"354,-830.5 354,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"354,-853.5 410,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"410,-830.5 410,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-861.3\">[(?, ?, 64), (?, ?, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"410,-853.5 542,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-838.3\">(?, ?, ?)</text>\n",
       "</g>\n",
       "<!-- 3029857499064&#45;&gt;3029858105608 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>3029857499064-&gt;3029858105608</title>\n",
       "<path d=\"M201.211,-913.473C233.324,-903.027 271.852,-890.494 305.116,-879.673\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"306.375,-882.944 314.802,-876.522 304.21,-876.287 306.375,-882.944\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857741232&#45;&gt;3029858105608 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>3029857741232-&gt;3029858105608</title>\n",
       "<path d=\"M408.663,-913.366C404.893,-904.884 400.517,-895.037 396.435,-885.853\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"399.585,-884.323 392.325,-876.607 393.188,-887.166 399.585,-884.323\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029858229776 -->\n",
       "<g class=\"node\" id=\"node16\"><title>3029858229776</title>\n",
       "<polygon fill=\"none\" points=\"321.5,-747.5 321.5,-793.5 591.5,-793.5 591.5,-747.5 321.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-766.8\">activation_12: Activation</text>\n",
       "<polyline fill=\"none\" points=\"476.5,-747.5 476.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"476.5,-770.5 532.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"532.5,-747.5 532.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-778.3\">(?, ?, ?)</text>\n",
       "<polyline fill=\"none\" points=\"532.5,-770.5 591.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"562\" y=\"-755.3\">(?, ?, ?)</text>\n",
       "</g>\n",
       "<!-- 3029858105608&#45;&gt;3029858229776 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>3029858105608-&gt;3029858229776</title>\n",
       "<path d=\"M402.721,-830.366C410.96,-821.348 420.607,-810.788 429.438,-801.121\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"432.143,-803.35 436.304,-793.607 426.975,-798.629 432.143,-803.35\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857192536 -->\n",
       "<g class=\"node\" id=\"node18\"><title>3029857192536</title>\n",
       "<polygon fill=\"none\" points=\"359,-664.5 359,-710.5 604,-710.5 604,-664.5 359,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424\" y=\"-683.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"489,-664.5 489,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"489,-687.5 545,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"517\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"545,-664.5 545,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-695.3\">(?, ?, ?)</text>\n",
       "<polyline fill=\"none\" points=\"545,-687.5 604,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-672.3\">(?, ?, ?)</text>\n",
       "</g>\n",
       "<!-- 3029858229776&#45;&gt;3029857192536 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>3029858229776-&gt;3029857192536</title>\n",
       "<path d=\"M463.331,-747.366C465.894,-739.062 468.861,-729.451 471.644,-720.434\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"475.072,-721.194 474.677,-710.607 468.383,-719.13 475.072,-721.194\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857897888 -->\n",
       "<g class=\"node\" id=\"node19\"><title>3029857897888</title>\n",
       "<polygon fill=\"none\" points=\"560,-830.5 560,-876.5 827,-876.5 827,-830.5 560,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625.5\" y=\"-849.8\">lambda_14: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"691,-830.5 691,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"719\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"691,-853.5 747,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"719\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"747,-830.5 747,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-861.3\">(?, 35, 960)</text>\n",
       "<polyline fill=\"none\" points=\"747,-853.5 827,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"787\" y=\"-838.3\">(?, ?, 64)</text>\n",
       "</g>\n",
       "<!-- 3029857191248&#45;&gt;3029857897888 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>3029857191248-&gt;3029857897888</title>\n",
       "<path d=\"M693.5,-913.366C693.5,-905.152 693.5,-895.658 693.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"697,-886.607 693.5,-876.607 690,-886.607 697,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029858596064 -->\n",
       "<g class=\"node\" id=\"node20\"><title>3029858596064</title>\n",
       "<polygon fill=\"none\" points=\"412.5,-581.5 412.5,-627.5 724.5,-627.5 724.5,-581.5 412.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"478\" y=\"-600.8\">lambda_16: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"543.5,-581.5 543.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"543.5,-604.5 599.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"571.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-581.5 599.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662\" y=\"-612.3\">[(?, ?, ?), (?, ?, 64)]</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-604.5 724.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662\" y=\"-589.3\">(?, ?, 64)</text>\n",
       "</g>\n",
       "<!-- 3029857192536&#45;&gt;3029858596064 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>3029857192536-&gt;3029858596064</title>\n",
       "<path d=\"M505.273,-664.366C515.151,-655.169 526.751,-644.369 537.3,-634.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"539.822,-636.983 544.756,-627.607 535.052,-631.859 539.822,-636.983\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857897888&#45;&gt;3029858596064 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>3029857897888-&gt;3029858596064</title>\n",
       "<path d=\"M686.206,-830.443C673.821,-794.34 646.755,-720.949 613.5,-664 607.869,-654.357 600.857,-644.5 594.057,-635.692\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"596.607,-633.275 587.654,-627.61 591.12,-637.622 596.607,-633.275\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029858712096 -->\n",
       "<g class=\"node\" id=\"node21\"><title>3029858712096</title>\n",
       "<polygon fill=\"none\" points=\"438.5,-498.5 438.5,-544.5 698.5,-544.5 698.5,-498.5 438.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504\" y=\"-517.8\">lambda_17: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"569.5,-498.5 569.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"569.5,-521.5 625.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"625.5,-498.5 625.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662\" y=\"-529.3\">(?, ?, 64)</text>\n",
       "<polyline fill=\"none\" points=\"625.5,-521.5 698.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662\" y=\"-506.3\">(?, ?, 960)</text>\n",
       "</g>\n",
       "<!-- 3029858596064&#45;&gt;3029858712096 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>3029858596064-&gt;3029858712096</title>\n",
       "<path d=\"M568.5,-581.366C568.5,-573.152 568.5,-563.658 568.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"572,-554.607 568.5,-544.607 565,-554.607 572,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857193768 -->\n",
       "<g class=\"node\" id=\"node22\"><title>3029857193768</title>\n",
       "<polygon fill=\"none\" points=\"342.5,-415.5 342.5,-461.5 794.5,-461.5 794.5,-415.5 342.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504\" y=\"-434.8\">time_distributed_2(dense_15): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"665.5,-415.5 665.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"665.5,-438.5 721.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"693.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"721.5,-415.5 721.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"758\" y=\"-446.3\">(?, ?, 960)</text>\n",
       "<polyline fill=\"none\" points=\"721.5,-438.5 794.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"758\" y=\"-423.3\">(?, ?, 300)</text>\n",
       "</g>\n",
       "<!-- 3029858712096&#45;&gt;3029857193768 -->\n",
       "<g class=\"edge\" id=\"edge23\"><title>3029858712096-&gt;3029857193768</title>\n",
       "<path d=\"M568.5,-498.366C568.5,-490.152 568.5,-480.658 568.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"572,-471.607 568.5,-461.607 565,-471.607 572,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029858975360 -->\n",
       "<g class=\"node\" id=\"node23\"><title>3029858975360</title>\n",
       "<polygon fill=\"none\" points=\"439,-332.5 439,-378.5 698,-378.5 698,-332.5 439,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504\" y=\"-351.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"569,-332.5 569,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"569,-355.5 625,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"625,-332.5 625,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"661.5\" y=\"-363.3\">(?, ?, 300)</text>\n",
       "<polyline fill=\"none\" points=\"625,-355.5 698,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"661.5\" y=\"-340.3\">(?, ?, 300)</text>\n",
       "</g>\n",
       "<!-- 3029857193768&#45;&gt;3029858975360 -->\n",
       "<g class=\"edge\" id=\"edge24\"><title>3029857193768-&gt;3029858975360</title>\n",
       "<path d=\"M568.5,-415.366C568.5,-407.152 568.5,-397.658 568.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"572,-388.607 568.5,-378.607 565,-388.607 572,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029857192816 -->\n",
       "<g class=\"node\" id=\"node24\"><title>3029857192816</title>\n",
       "<polygon fill=\"none\" points=\"377,-249.5 377,-295.5 760,-295.5 760,-249.5 377,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"504\" y=\"-268.8\">layer_normalization_2: LayerNormalization</text>\n",
       "<polyline fill=\"none\" points=\"631,-249.5 631,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"659\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"631,-272.5 687,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"659\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"687,-249.5 687,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"723.5\" y=\"-280.3\">(?, ?, 300)</text>\n",
       "<polyline fill=\"none\" points=\"687,-272.5 760,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"723.5\" y=\"-257.3\">(?, ?, 300)</text>\n",
       "</g>\n",
       "<!-- 3029858975360&#45;&gt;3029857192816 -->\n",
       "<g class=\"edge\" id=\"edge25\"><title>3029858975360-&gt;3029857192816</title>\n",
       "<path d=\"M568.5,-332.366C568.5,-324.152 568.5,-314.658 568.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"572,-305.607 568.5,-295.607 565,-305.607 572,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029856329680 -->\n",
       "<g class=\"node\" id=\"node26\"><title>3029856329680</title>\n",
       "<polygon fill=\"none\" points=\"113.5,-166.5 113.5,-212.5 569.5,-212.5 569.5,-166.5 113.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"277\" y=\"-185.8\">global_average_pooling1d_2: GlobalAveragePooling1D</text>\n",
       "<polyline fill=\"none\" points=\"440.5,-166.5 440.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"440.5,-189.5 496.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"468.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"496.5,-166.5 496.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-197.3\">(?, ?, 300)</text>\n",
       "<polyline fill=\"none\" points=\"496.5,-189.5 569.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-174.3\">(?, 300)</text>\n",
       "</g>\n",
       "<!-- 3029857192816&#45;&gt;3029856329680 -->\n",
       "<g class=\"edge\" id=\"edge26\"><title>3029857192816-&gt;3029856329680</title>\n",
       "<path d=\"M506.771,-249.473C477.747,-239.117 442.974,-226.709 412.828,-215.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"413.811,-212.587 403.217,-212.522 411.459,-219.179 413.811,-212.587\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029859249064 -->\n",
       "<g class=\"node\" id=\"node27\"><title>3029859249064</title>\n",
       "<polygon fill=\"none\" points=\"587.5,-166.5 587.5,-212.5 1003.5,-212.5 1003.5,-166.5 587.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"731\" y=\"-185.8\">global_max_pooling1d_2: GlobalMaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-166.5 874.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-189.5 930.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"902.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"930.5,-166.5 930.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"967\" y=\"-197.3\">(?, ?, 300)</text>\n",
       "<polyline fill=\"none\" points=\"930.5,-189.5 1003.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"967\" y=\"-174.3\">(?, 300)</text>\n",
       "</g>\n",
       "<!-- 3029857192816&#45;&gt;3029859249064 -->\n",
       "<g class=\"edge\" id=\"edge27\"><title>3029857192816-&gt;3029859249064</title>\n",
       "<path d=\"M630.229,-249.473C659.253,-239.117 694.026,-226.709 724.172,-215.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"725.541,-219.179 733.783,-212.522 723.189,-212.587 725.541,-219.179\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029859249568 -->\n",
       "<g class=\"node\" id=\"node25\"><title>3029859249568</title>\n",
       "<polygon fill=\"none\" points=\"1010,-249.5 1010,-295.5 1247,-295.5 1247,-249.5 1010,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1073\" y=\"-268.8\">input_6: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1136,-249.5 1136,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1136,-272.5 1192,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1164\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1192,-249.5 1192,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1219.5\" y=\"-280.3\">[(?, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"1192,-272.5 1247,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1219.5\" y=\"-257.3\">[(?, 1)]</text>\n",
       "</g>\n",
       "<!-- 3029859894664 -->\n",
       "<g class=\"node\" id=\"node28\"><title>3029859894664</title>\n",
       "<polygon fill=\"none\" points=\"1022,-166.5 1022,-212.5 1235,-212.5 1235,-166.5 1022,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1077.5\" y=\"-185.8\">dense_16: Dense</text>\n",
       "<polyline fill=\"none\" points=\"1133,-166.5 1133,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1133,-189.5 1189,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1161\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1189,-166.5 1189,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1212\" y=\"-197.3\">(?, 1)</text>\n",
       "<polyline fill=\"none\" points=\"1189,-189.5 1235,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1212\" y=\"-174.3\">(?, 1)</text>\n",
       "</g>\n",
       "<!-- 3029859249568&#45;&gt;3029859894664 -->\n",
       "<g class=\"edge\" id=\"edge28\"><title>3029859249568-&gt;3029859894664</title>\n",
       "<path d=\"M1128.5,-249.366C1128.5,-241.152 1128.5,-231.658 1128.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1132,-222.607 1128.5,-212.607 1125,-222.607 1132,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029860245968 -->\n",
       "<g class=\"node\" id=\"node29\"><title>3029860245968</title>\n",
       "<polygon fill=\"none\" points=\"603.5,-83.5 603.5,-129.5 987.5,-129.5 987.5,-83.5 603.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"690\" y=\"-102.8\">concatenate_2: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"776.5,-83.5 776.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"776.5,-106.5 832.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"804.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"832.5,-83.5 832.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-114.3\">[(?, 300), (?, 300), (?, 1)]</text>\n",
       "<polyline fill=\"none\" points=\"832.5,-106.5 987.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"910\" y=\"-91.3\">(?, 601)</text>\n",
       "</g>\n",
       "<!-- 3029856329680&#45;&gt;3029860245968 -->\n",
       "<g class=\"edge\" id=\"edge29\"><title>3029856329680-&gt;3029860245968</title>\n",
       "<path d=\"M464.958,-166.473C526.022,-155.579 599.808,-142.414 662.161,-131.289\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"662.837,-134.724 672.067,-129.522 661.607,-127.833 662.837,-134.724\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029859249064&#45;&gt;3029860245968 -->\n",
       "<g class=\"edge\" id=\"edge30\"><title>3029859249064-&gt;3029860245968</title>\n",
       "<path d=\"M795.5,-166.366C795.5,-158.152 795.5,-148.658 795.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"799,-139.607 795.5,-129.607 792,-139.607 799,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029859894664&#45;&gt;3029860245968 -->\n",
       "<g class=\"edge\" id=\"edge31\"><title>3029859894664-&gt;3029860245968</title>\n",
       "<path d=\"M1037.95,-166.473C993.986,-155.78 941.037,-142.901 895.851,-131.91\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"896.58,-128.485 886.036,-129.522 894.925,-135.287 896.58,-128.485\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3029860648496 -->\n",
       "<g class=\"node\" id=\"node30\"><title>3029860648496</title>\n",
       "<polygon fill=\"none\" points=\"682.5,-0.5 682.5,-46.5 908.5,-46.5 908.5,-0.5 682.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"738\" y=\"-19.8\">dense_17: Dense</text>\n",
       "<polyline fill=\"none\" points=\"793.5,-0.5 793.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"821.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"793.5,-23.5 849.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"821.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"849.5,-0.5 849.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879\" y=\"-31.3\">(?, 601)</text>\n",
       "<polyline fill=\"none\" points=\"849.5,-23.5 908.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"879\" y=\"-8.3\">(?, 3)</text>\n",
       "</g>\n",
       "<!-- 3029860245968&#45;&gt;3029860648496 -->\n",
       "<g class=\"edge\" id=\"edge32\"><title>3029860245968-&gt;3029860648496</title>\n",
       "<path d=\"M795.5,-83.3664C795.5,-75.1516 795.5,-65.6579 795.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"799,-56.6068 795.5,-46.6068 792,-56.6069 799,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model,show_shapes = True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5bed51171db8bf8c5e1731119fd31205bfd80a82"
   },
   "source": [
    "Train the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "e1061c7cde0687450300f516735aad0cc5dbf08f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 19:36:02.156150 13272 callbacks.py:1781] `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473568 samples, validate on 84370 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 19:36:03.415449 13272 summary_ops_v2.py:1110] Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "466944/473568 [============================>.] - ETA: 2:34 - loss: 1.502 - ETA: 1:33 - loss: 1.457 - ETA: 1:09 - loss: nan  - ETA: 56s - loss: nan - ETA: 49s - loss: na - ETA: 43s - loss: na - ETA: 39s - loss: na - ETA: 36s - loss: na - ETA: 34s - loss: na - ETA: 32s - loss: na - ETA: 30s - loss: na - ETA: 29s - loss: na - ETA: 27s - loss: na - ETA: 26s - loss: na - ETA: 25s - loss: na - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:991: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1224: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\Admin\\Anaconda3\\envs\\Transformer_1D-CNN_Feature_Extraction\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py:1805: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473568/473568 [==============================] - 29s 62us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00002: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00003: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00004: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00005: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 55us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00006: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00007: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00008: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 27s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/500\n",
      "466944/473568 [============================>.] - ETA: 25s - loss: na - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00009: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00010: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00011: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00012: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00013: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00014: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00015: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00016: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00017: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/500\n",
      "466944/473568 [============================>.] - ETA: 24s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00018: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 27s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/500\n",
      "466944/473568 [============================>.] - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00019: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/500\n",
      "466944/473568 [============================>.] - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00020: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 26s 54us/sample - loss: nan - val_loss: nan\n",
      "Epoch 21/500\n",
      "466944/473568 [============================>.] - ETA: 25s - loss: na - ETA: 23s - loss: na - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00021: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 25s 52us/sample - loss: nan - val_loss: nan\n",
      "Epoch 22/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00022: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 23/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00023: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 24/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00024: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.006399999558925629.\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 25/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00025: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 26/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00026: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 27/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00027: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 28/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00028: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 29/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00029: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 30/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00030: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 31/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00031: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 32/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00032: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 33/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00033: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 34/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00034: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 35/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00035: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 36/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00036: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00037: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 38/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00038: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0051199994981288915.\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 39/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00039: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 40/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00040: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 41/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00041: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00042: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 43/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00043: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 44/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00044: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 45/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00045: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 46/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00046: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00047: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 48/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00048: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 49/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00049: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 50/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00050: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 51/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00051: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 52/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00052: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.004095999523997307.\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 53/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00053: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 54/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00054: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 55/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00055: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 56/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00056: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00057: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 58/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00058: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 59/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00059: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 60/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00060: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 61/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00061: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00062: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 63/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00063: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 64/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00064: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 65/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00065: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 66/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00066: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0032767996191978457.\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00067: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 68/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00068: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 69/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00069: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 70/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00070: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 71/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00071: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00072: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 73/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00073: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 74/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00074: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 75/500\n",
      "466944/473568 [============================>.] - ETA: 22s - loss: na - ETA: 22s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00075: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 24s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 76/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00076: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00077: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 78/500\n",
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00078: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 79/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00079: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 80/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00080: val_loss did not improve from inf\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0026214396581053737.\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 81/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00081: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 24s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00082: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 83/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 21s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00083: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 24s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 84/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00084: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 85/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00085: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 24s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 86/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00086: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 87/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00087: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 88/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00088: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 89/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00089: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 49us/sample - loss: nan - val_loss: nan\n",
      "Epoch 90/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00090: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 91/500\n",
      "466944/473568 [============================>.] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 17s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 16s - loss: na - ETA: 15s - loss: na - ETA: 15s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 14s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 13s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 12s - loss: na - ETA: 11s - loss: na - ETA: 11s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 10s - loss: na - ETA: 9s - loss: na - ETA: 9s - loss: n - ETA: 9s - loss: n - ETA: 8s - loss: n - ETA: 8s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 7s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 6s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 5s - loss: n - ETA: 4s - loss: n - ETA: 4s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 3s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 2s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 1s - loss: n - ETA: 0s - loss: n - ETA: 0s - loss: nan\n",
      "Epoch 00091: val_loss did not improve from inf\n",
      "473568/473568 [==============================] - 23s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 92/500\n",
      " 73728/473568 [===>..........................] - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 20s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 19s - loss: na - ETA: 18s - loss: na - ETA: 18s - loss: na - ETA: 17s - loss: nan"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "logdir = \"logs\\\\scalars\\\\\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "weight_path=\"{}_weights.best.hdf5\".format('stroke_lstm_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=500) \n",
    "callbacks_list = [checkpoint\n",
    "                  , early\n",
    "                  , reduceLROnPlat\n",
    "                  , tensorboard_callback\n",
    "                 ]\n",
    "\n",
    "from IPython.display import clear_output\n",
    "model.fit([train_X, train_X2], train_y,\n",
    "                      validation_data = ([valid_X, valid_X2], valid_y), \n",
    "                      batch_size = 2048*4,\n",
    "                      epochs = 500,\n",
    "                      callbacks = callbacks_list)\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b4790925984514e64ca5a9b46de8b309062e0cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_path)\n",
    "lstm_results = model.evaluate([test_X, test_X2], test_y, batch_size = batch_size)\n",
    "print(lstm_results)\n",
    "# print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[0], 100*lstm_results[1]))\n",
    "print('Recall S %2.8f%%, Recall L %2.8f%%, Precision S %2.8f%%, Precision L %2.8f%%' % ( lstm_results[1], lstm_results[2], lstm_results[3], lstm_results[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "del train_gen\n",
    "del valid_gen\n",
    "del X_train\n",
    "del y_train\n",
    "del train_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./trained_model.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3340d6d6ce75585f90c98f1728b1cd664d7f33f"
   },
   "source": [
    "Load and normalize the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(ts_length = 150000):\n",
    "    base_dir = 'input/test/'\n",
    "    test_files = [f for f in listdir(base_dir) if isfile(join(base_dir, f))]\n",
    "\n",
    "    ts = np.empty([len(test_files), ts_length])\n",
    "    ids = []\n",
    "    \n",
    "    i = 0\n",
    "    for f in tqdm_notebook(test_files):\n",
    "        ids.append(splitext(f)[0])\n",
    "        t_df = pd.read_csv(base_dir + f, dtype={\"acoustic_data\": np.int8})\n",
    "        ts[i, :] = t_df['acoustic_data'].values\n",
    "        i = i + 1\n",
    "\n",
    "    return ts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f005579ea08913f4f68a3749bd761df6cef2b1b"
   },
   "outputs": [],
   "source": [
    "test_data, test_ids = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c3b7a864a9f53af142a08883def46c3866c5464"
   },
   "outputs": [],
   "source": [
    "X_test = test_data\n",
    "X_test = np.expand_dims(X_test, 2)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf9b36929e5228d4d94b3b7ad1b9011bf088ac44"
   },
   "source": [
    "Load best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "435449fda2bf96635e67d69f56227e140c4cea99"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9aaf9fb44edba5879a75c68820527d9180d2b3c6"
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'seg_id': test_ids, 'time_to_failure': y_pred[:, 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b9d5c63161f637de2e39b59e8e4d7c2f3049581"
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"submission.csv\"> Download File </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.transform([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nn = np.array([[1., 0.,2], [2., 1.,3], [0., 0.,4]])\n",
    "print(nn[1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(valid_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(valid_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
