{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xfile = 'F:\\workspace\\j6stock\\XOpenHighLowCloseVol_tp10_cl10.txt'\n",
    "lstm_size = 120         # 3 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 2048       # Batch size\n",
    "learning_rate = 0.001  #0.0001  # Learning rate (default is 0.001)\n",
    "epochs = 200\n",
    "\n",
    "# Fixed\n",
    "#amount_of_features_cvs = 5 # cvs with prefix with feature column\n",
    "#n_channels = amount_of_features\n",
    "seq_len = lstm_size\n",
    "y_column = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py\n",
    "import os\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stock_data(normalize=True, ma=[]):\n",
    "    \"\"\"\n",
    "    Return a dataframe of that stock and normalize all the values. \n",
    "    (Optional: create moving average)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath_or_buffer  = xfile )\n",
    "\n",
    "    df['change'] = df['open'] - df['close']\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Moving Average    \n",
    "    if ma != []:\n",
    "        for moving in ma:\n",
    "            df['{}ma'.format(moving)] = df['close'].rolling(window=moving).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    if normalize:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df['open'] = min_max_scaler.fit_transform(df.open.values.reshape(-1,1))\n",
    "        df['high'] = min_max_scaler.fit_transform(df.high.values.reshape(-1,1))\n",
    "        df['low'] = min_max_scaler.fit_transform(df.low.values.reshape(-1,1))\n",
    "        #df['vol'] = min_max_scaler.fit_transform(df.vol.values.reshape(-1,1))\n",
    "        df['close'] = min_max_scaler.fit_transform(df['close'].values.reshape(-1,1))\n",
    "        df['change'] = min_max_scaler.fit_transform(df['change'].values.reshape(-1,1))\n",
    "        if ma != []:\n",
    "            for moving in ma:\n",
    "                df['{}ma'.format(moving)] = min_max_scaler.fit_transform(df['{}ma'.format(moving)].values.reshape(-1,1))  \n",
    "    df.dropna(inplace=True)\n",
    "               \n",
    "    # Move y_result to the rightmost for the ease of training\n",
    "    adj_close = df['y_result']\n",
    "    df.drop(labels=['y_result'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, adj_close], axis=1)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = get_stock_data( ma=[50, 100, 200])\n",
    "amount_of_features = len(df.columns)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stock(df):\n",
    "    print(df.head())\n",
    "    plt.subplot(211)\n",
    "    plt.plot(df['close'], color='red', label='Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(212)\n",
    "    plt.plot(df['change'], color='blue', label='Percentage change')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         open      high       low     close    change      50ma     100ma  \\\n",
      "199  0.358586  0.355870  0.358374  0.357085  0.410872  0.351429  0.351070   \n",
      "200  0.357245  0.354622  0.357608  0.356606  0.399494  0.351433  0.351063   \n",
      "201  0.356623  0.354862  0.357799  0.356893  0.387484  0.351445  0.351060   \n",
      "202  0.357054  0.354478  0.358422  0.357133  0.390013  0.351502  0.351082   \n",
      "203  0.357293  0.355774  0.358709  0.358138  0.379899  0.351587  0.351141   \n",
      "\n",
      "        200ma  y_result  \n",
      "199  0.346428         0  \n",
      "200  0.346426         0  \n",
      "201  0.346424         0  \n",
      "202  0.346433         0  \n",
      "203  0.346448         0  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXeY1dTWh38bGBgEFBgQR5EqgkNx\nYIamKCBSRMUuYAGxgCKgXr0qFlQs2BsWinJR8CKI5aKCIDp8NhAG6SADIujQYZA+lJn1/bETUk6S\nk3NOcup6n+c8SXZ2dtY+SfbadS1BRGAYhmFSjzKxFoBhGIaJDawAGIZhUhRWAAzDMCkKKwCGYZgU\nhRUAwzBMisIKgGEYJkVhBcAwDJOisAJgGIZJUVgBMAzDpCjlYnXjGjVqUL169WJ1e4ZhmIRk8eLF\nu4iophdpBVUAQogJAC4DsIOImlmcFwDeANATwCEAtxDRb8HSrVevHvLz80OXmGEYJoURQmzyKi03\nXUATAfRwOH8JgEbKbyCAdyMXi2EYhvGboAqAiH4AUOQQ5QoAH5JkAYCqQohMrwRkGIZh/MGLQeAz\nAPytOy5Uwhgm9SguBoQAJk+OtSQMExQvBoGFRZiljWkhxEDIbiLUqVPHg1szjA/s2wdUrgyUCaN+\n9NNPcnvzzcBNN3krV4py7NgxFBYWori4ONaiRJX09HTUrl0baWlpvt3DCwVQCOBM3XFtAFusIhLR\nOADjACA3N5cdETDxx8GDwCmnAJ06AXl5oV8fjtJgHCksLESVKlVQr149yDknyQ8RYffu3SgsLET9\n+vV9u48Xb+sMAP2EpB2AvUS01YN0GSb67N0rt/PmhXd9ihRQ0aS4uBgZGRkpU/gDgBACGRkZvrd6\n3EwDnQKgE4AaQohCAE8ASAMAIhoDYCbkFND1kNNAB/glLMP4TklJ+NeOHQvcead3sjAnSKXCXyUa\neQ6qAIiob5DzBOBuzyRimFiiVwC7dgE1ari/lgt/JsHgDkuG0aNXADk5sZODiSu2bduGPn36oGHD\nhsjKykLPnj1RUFCAZs0C1sYmFKwAGEbPsWPa/l9/yT79VatiJ48fvPOOzNeyZbGWJCEgIlx11VXo\n1KkT/vjjD6xevRrPPfcctm/fHmvRIoYVAMPoWbAgMKxXr+DX7dkTGBbJeIJfHDoE3K302E6ZEltZ\nEoS8vDykpaXhTl0XX3Z2Ns48U5v8WFxcjAEDBqB58+Zo2bIl8pQZZKtWrUKbNm2QnZ2NFi1aYN26\ndQCAyZMnnwgfNGgQSmL0rsTMGBzDxCUDLOYwbNgQ/LqdOwPDFiwAzj8/cplKSoDNm4Gffwb6Og7J\nBee117T99PTI0ooF994LLF3qbZrZ2cDrr9ueXrlyJXKCdAe+/fbbAIAVK1bg999/R7du3VBQUIAx\nY8bgnnvuwY033oijR4+ipKQEa9aswdSpU/Hzzz8jLS0NgwcPxkcffYR+/fp5mi03sAJgGDcQOU/x\ntKpN9+kD/P13YHgo/Pwz0KGDMc1IZoc89pi236RJ+OkwBn766ScMHToUANCkSRPUrVsXBQUFaN++\nPZ599lkUFhbi6quvRqNGjfDdd99h8eLFaN26NQDg8OHDOPXUU2MiNysAhnFDz57ArFn25zdvltsq\nVYD9++V+YSHw+efAVVeFd08iY+EPeLvOoG9foH17oG5d79L0G4eaul80bdoU06dPd4wjJ0MGcsMN\nN6Bt27b4+uuv0b17d7z33nsgIvTv3x+jRo3yQ9yQ4DEAhlHZYrmAXfLNN87XZir2D196CfjxRy38\n6qtlQR4OwaagHjokFcKnn4aXPgB47ZOja1fg5Ze9TTPGXHTRRThy5AjGjx9/ImzRokXYtEmzynzh\nhRfio48+AgAUFBTgr7/+QuPGjbFhwwY0aNAAw4YNQ69evbB8+XJ06dIF06dPx44dOwAARUVFhrSi\nCSsAhlEx17bNCAEcOWJ9buRIuT12zDiTCAAeeSQ8eYqcjPACqFRJbq+9Nrz0/WDuXODf/wZWrgS6\ndQPeey/WEkWMEAKff/45vv32WzRs2BBNmzbFk08+idNPP/1EnMGDB6OkpATNmzdH7969MXHiRFSo\nUAFTp05Fs2bNkJ2djd9//x39+vVDVlYWnnnmGXTr1g0tWrRA165dsXVrbIwnCLumi9/k5uYSO4Rh\n4gpz98prrwH33WcMKywEzjAZu925E1D7cAcNkgOV55xjjBPOd2bV3fPPP9JWkfl8URFQrZp1OitX\nAhUrAg0bWqdZWhp615Ia//BhOZhMZG8HKcIyZs2aNTjH/H+mCFZ5F0IsJqJcL9LnFgDD2NGnT2DY\ngQOBYS++qO2feaYcXLWaThoqVgV6aal13OrVpSlqK5o3B846y76La9eu0OT67jttv2JFuXVqrRw6\nFFr6TNRgBcAwdlj1wc+eHRim7/POypLbtm3Dv+8PPwDbtskuntNPN9agzd1Lei66KDDsyy+1fXPL\nRaVCBfeyLVsGXHyxMWz6dOA//7G/5s8/3afPRBVWAAxj5t//lrN+yllMkqte3Xh8+LDxuEULbf+L\nL7T9bt1kwfnLL/L4v/8FRo+2vn/HjnJQubBQq7WrhbuTApg/X26vv1520Xz7rf0iNiJtjcLKlfZp\nmtm2LTDs+eflf2bHXXe5T9+GWHVVx5Ko5JmIYvLLyckhhokrZLFoDNuwQQu3Ov/UU8Zzhw9bp6n/\nrVplPD50yPkaIqIJE+T+n38SzZ1rna4a1+6c+vvhB2O8smXlcUkJ0UMPyTzb8dFHwdO3+i1f7vox\nmNmwYQPt3LmTSktLw04j0SgtLaWdO3fSBotnASCfPCqHeRCYYQBp90edD2/+JswDpPrzVatqPgQA\n4OhRQO/Byc3gqt75zOLFQK5pfI9IW4dQoULgTKSvvwYuvTT4fczyO8nWtm3gOMbmzUDt2trxySdL\n72lWdO5sdKgzYgTw1FPuZdTBHsGMHsG8HATmhWBM6pCXB9Svbz33PdzpivrCHwDKlg09Db3zGXPh\nr8qqDv5aTUPt2dM+7bPPBgoKrM899BDwwgvW5379Fdi+HahVSwvTF/6A7HJq2tQY9tdfclC5ZUtp\nsqFlSxkeQeGdlpbmq1esVIbHAJjU4aKLpAKw4umn7a8zF3JOmKdCLl/u/lor1AFUq8FnQM67d8Jc\n+H//vbYfzNfsaacBPXpYn3vpJaBmTWPY8uVyFpRa6GdnS8ujgOYrmYkrWAEwyc2+fcCttwbWQA8c\nAMaMcTdHfcUKY8174kR5XaNGwa9t3hyYMyckkS258cbAsJkzgS5dQkunc2dt/7LLgsdXFY+55TFk\niFEBjBgh82rmww/lVh38ZuILrwYTQv3xIDATFfr2DRyQvOceoquukvtffUVUWqqd27TJPi19GtWr\n2w/CBrvW7rp9+5zTM5/TDzjPmxf8HnfeGSjXgAHa+fnzra9r1sx4XFSkXZ+fT7R9u32+ly7Vrrvi\nCvt4jGvAg8AM45Jgg7CNGwMHD8opl4Bzi8Aprfr1ZXeN3fXB5CAKjPPuu4FuJoWQ8/nnz5fdLW7v\n8fTTwIMPAuXLB55r21aanM7PB4YNs5+eqpfVLUTGbrEYlTfJBK8EZhi31KnjfH7tWq3wD5c77pAz\nZtR5+FboB4d37ZImHYjkuoIhQwLjX3GF7LoyM3s2sGhRYOEPAN2729//scesC39ADviqlbE337RP\nIxxS0Jl7IsEKgEluomFD5s47pS2gdu3s4+jNIWRkaPZ8ioqAt94KjP/FF9YFdrdumuVRM+qAK2Af\nJ1LCWdR1xRXafnGxvUE9JuqwAmCSGy+dnphXAau0ahX82vLltV50KyZP1vadWhJONGig3ePjj7Xw\nUE0xHDggWwX//BN4Tq9k3KJfEV2xojQed/x46OkwnsNjAExyE0oXxDXXSLs2oaYXyTfkdXp6srOl\niQarGURucVoEF0k6NWpYu9FkgsILwRjGD8JZDBbPg5pe+861M/ccDqFaIGV8gbuAmORl0SLn87fd\nJrd5ebIgr1o1eJqPP64tdPIC80rieGPfPpnnQYPkbCkvqVZNGsVjYgZ3ATHJy8SJwIAB9ucjefcX\nLZIrabOzw09Dxatulnhm2jSgd2/rc8mYXx/haaAM4wa9mePt27Wpjl7QurU3hT8gFVWyc/31sZaA\nsYAVAJO8qE5KypSR0zRzcrRzr70WG5ms6N/f2RZRsrB0qVwzwcQNrACY5EV11qJ3mj58uNzee2/0\n5XFi2LBYS+A/554LjBvn/WIzJmxYATDJi6oA9P5qn3suPvucTz5ZLpiaMiXWkvjP0KFyQRsAXH65\nfB5Ons4Y3+BpoEzyc/RorCVwh37BVLIze7Yc/K5WTZteWlQkj5mowS0AJvl59tlYS8DYoZqLBqTD\neSaqsAJgkp8OHWItAeMGva8CJiqwAmAYhklRkkcBrFsn+xRnzoy1JEw0eO89OXB66JD9oG7NmoH2\n9BmGOUHyKADVvd2ll8ZWDsY/6tbVVpPecQewfz9QqRIwdqx1/J07ZcWAiU+sFLcbcxyMZ7hSAEKI\nHkKItUKI9UKIhy3O3yKE2CmEWKr8bvde1CDwNLLk56+/pEkBs+kEK3v6Tz0lt999579cjHfEu22k\nJCOoAhBClAXwNoBLAGQB6CuEyLKIOpWIspVfGGYVI6C4OHSb50zysGoVUFqqHR8/Djz5ZMzEYcKg\nf/9YS5CSuGkBtAGwnog2ENFRAB8DuCLINdHlk0+Mx+oCICZ5KClxPj9rlra/b5+/sjDesWcPsGZN\nathDikPcKIAzAPytOy5UwsxcI4RYLoSYLoSwcFgaRU46KT5XezLh8/LLzuf1nqr+1r2u8+b5Ig7j\nEVWrBnptE0IqBsZ33CgAK5dK5tL1SwD1iKgFgLkAPrBMSIiBQoh8IUT+Ti+9AVmt9AzmDOPvv+WL\ndtpp3snB+MfDAUNPRtaskdv/+z9g4UItvGNH/2Ri/MPO/SbjKW4UQCEAfY2+NoAt+ghEtJuIVE/P\n4wHkwAIiGkdEuUSUW7NmzXDktWbDhsAwfSFg5vvvgTp15P727dJs8I8/AgUF3snERAfVi5c6BtSp\nEzBwoNznboXERgg5vsf4hhsFsAhAIyFEfSFEeQB9AMzQRxBCZOoOewFY452ILrDyVPTVV7JboFat\nwHNduhiP164FLrwQaNzYH/mYyNAr5iNHtP19+4Bbb7W/rhybukp4KlaMtQRJTVAFQETHAQwBMBuy\nYJ9GRKuEECOFEL2UaMOEEKuEEMsADANwi18CW/L229p+jRpy+9VXwN13Azt2AA0aAP/8I2sUP/0U\neH2LFtGRkwmPiy/W9suXl+M7RECVKs5O38uW9V82xn/uvts4y4vxDFfrAIhoJhGdTUQNiehZJWwE\nEc1Q9ocTUVMiOpeIOhPR734KHcDx49r+Cy8Env/zT2CG0mi54ILA84cO+SMX4w1qd53q4MUOJ2XA\nxD+rV1uHv/OOdWuuqAj4+Wd/ZUpykmclMADUqwfceKP1ubQ0++ueeUbb55qGxn33AUOGxFaGTZu0\nj/yqq0K7dv167+Vh/OOcc+xn71mFZ2RIQ3/79/srVxKT+ApAX+u78kqgQgXreFazSG66SW6nT9fC\nHnrIO9kSmVWrgNdfN3avxYJ69bT9ypWt49gtIrJ7F5jEZ9Uqbb9du9jJkeAkrgI4cgRo1swY9txz\n9vH/+st4PH680UesSrD55qlAaWngf6uydatUuuee678c5sVfdn36drN99C07JvEwu44cMEBuDx82\nvp92XUdMUBJTAXz/vZz+p68FANqMATeLwNq04TUAduhn2gDA7t1ye/QocPrpcn/5cv9X3M6fr+3X\nrx/69eb3g0kM8vJk6/N2k0mxiRNld8+WLcZwbrWHTWIqgC5dgvdN33STZiHUisaN7QcN9YPKqciu\nXcbjGjXkf2Lua/V7teby5dq+1VoPPXon7//7n1zoV7u2P3Ix/tKpE3DPPdbjcStWAAcOGMMaNoyK\nWMlI4ikAO5sw11xjPJ40ydrH6ldfyQKuQoXAmq5KqjcprQbVdu0KXG8xd64395s4MXDAds8eOf0P\n0BZ2OfHaa7IgmDED6NWLC/9koGJFoGdPY9j8+cDGjcawYHaiGHuIKCa/nJwcCotp09RZ4Npv9277\n+Oa4es44QwuvX1/b/89/wpMtWVi7NvB/A4hWr3b+P8NFTWvxYi3sggvcPV8m+bn0Uuv3Uf2NHh1r\nCaMKgHzyqBxOvBbA9dcHhrm1G2Lu2tDPLV6xQttXB5siYfNm2cX08cfS1EQi+StQ/6dPPzWGZ1lY\nARcC+O238O+lH6/JyZHG24SQpjlU2C5MavPVV8B119mfT/Uu2whIPAUwe7bxOBTTz1WqGI/1JiEq\nVTKei9SaqNoF0bcv0Lw58OCDkaUXTc4/X26XLJH96cGwmk3lFnPz3ewYnO0zMYDz98NdQGGTeAqg\nWzdZOO/dKwcJ09Od4+s9DJUvbzx39dVyq9Yu9LNG1NWn4WCllF5/3XhMJGu6QkSvddCiRfDVsnof\nulOnOg+kh0JxsXTXqf+PiYAJE5yva9TIm/sziU21aoFho0fLLbcAwibxFIDKySfLmrWbeHZceqks\nhKZNk8f6Lo7CwvBlczJQpqIfaDW3avxC381lBZHRv+7ChUAZl69IMMVSsSIwc6Zx9taLLwKDBtlf\n0769u3szyY+Vr+BbbpFbLxXAP//YTwLZvVu+52p5kQQkrgIIhQcfjMzl3NGjwA03uHcw/vHHwePo\nVx9nZBjPrVghX7RRo9zL6AWTJhmPnRx0Wy3KsptVpW/hqA5aLrzQ2cb//PnAL7/Yn2dSi1NOMR73\n6CEdPwGyC2j58sDFnuFQrRrQtKl1F7Daen388cjvEy94NZoc6i/sWUB+89RTgTNcPvzQ/ayXd96x\nn63w8cdavLPPtp9N4/UsGyKi0lItzUWL5HbXLvv7/vSTFt6okfHc888THTxonEXlJK955taRI4HX\nHT2q7Z9zjnf5ZpKHY8cC3zMhiB5/3LvvRU1n7FiiBx6Q7675nJffZRjAw1lArACsUB/y/v1Ex4+H\n9uD1cSdPDizo1q0zFsbmdA8c8OdFs5o+CxA9+mig7P/3f4HXDxminV+wwDq/6m/HDqJ+/TT5Fy+2\nV4oAUVGRdo9vvvEuz0zy8fXXRAUF2nG5ckTDh3uvAPS/Ll1CLwd8hBWA36gPOTubqGNH44PfutXd\ntWphX1AQ+EK9/LK9AvjhB39eNKcCOC/PGMeOLVuIHntMKjCVnJzA9D7/XNv/6SeiMWOc719S4l0+\nmdQiPd2776VzZ+f3VP09/LA3soeJlwogNcYAwmXpUuljVk9+vlyifsEFwOWXG1fH6vvAe/UCzjpL\nzmIx2yx/4IHAe61YIccFLrxQC8vMDIznBx984G4qXWYm8PTTxgHfn38GpkwxxvvmG22/QwfjzCIr\n3A40M4wZs5+ASHxC5OW5i5dEq8zZZ16o3H679COsUrmydExRrZpxXYF+/nzr1sHTtfJKFi1jdb/+\nKherhUOFCkCfPnK9g4p+JlEw1Km4DBMOZrtA4RLK+0/kzT3jAK56hYq+8FcZNkxuly61viYtzfql\nCWZ6esmS0GSzI9hHctllQHa23G/ZMrx7FBcDXbuGfp15tTHDREo4juTdTClXSSKnUawArHjxxcCw\nf/6xj3/smGw+qt1B48e7u8/99weatvUDfRfNhx8Gnn/pJW0/3FZHhQrAnDnOcT77zHicRDUpJo74\n4ANjV6oTO3cGFui1ajlfwwogyfn3vwPDnBaUTZ0KXHSRdmy2TKpiVaO36ufv0cNZvlBR+yxPOw24\n+WZZ8NrZ8p81y7v7mheeXXlloBJgGK+5805pSyrYAss//gBOPdW4GLF3b2e7QwArgJTEanDp1Vet\n41otWwekz1OzOQrA2HWyZ4+3hTAgHegAcjFbMP7805t7dugAnH22MUwIOXAOBC46Y5hIsLJZ1aOH\nNOmin4Rx9Kj2Pahexd57Tzs/YIB8dwG5QNPqW0yilisrADv69NH2e/eW288/18KOHLF2RKG3YmlG\n9UGwZIl0WKIyZ46stYwe7bz6NlzUsYaiIi3MShEdOWL0wRsOx48DTzwhZ0/p76EOspUrJz8g1R8z\nw0SCaqvq1FOtz//9t1agA9KVaZcuQJMm1mMFtWrJiQmDB8vVxT16yBly8+ZpFT63LQDV1lc82yry\naj5pqL+4XgdAJOe6n3020a+/Guep6+cbFxUZ5wf/9pt391fTPHrUu7T09vaJiA4dknkDiKpWjfw+\nVowdS5Sf70/aDLNzJ9ELL1gvrrRaHxBsjr9+jYuZgwdlnOefDy7XE09oaZpX3EcIeB1AFBACWLtW\n+g62m6du7uoJdwaNE07mro8cCfTSZYU6ptC0qTG8YkWZNyL/3DsOHBiZuWiGcaJGDWnrSwhZa4+E\nCROc1xGo5YCbLqCnntL2ly2LTC4fYQUQKhs2GBeM/PCDP/dRp2VazT46eFC+qOnpch2CHXPmADt2\naP50K1TwXk6GiRdGjnQ+H6wrpmZN5/OqAnjySed45mnX+rHCVavktxvuuhuPYQUQKvXrS6fVKhdc\n4M991DUFVpZFn346+PX79wPdu8s+TXaqwqQCqlXdypWBbdsCzz/yiPP1dpM3VNSav53VW0CObZkd\nT339tdYiUAeeQ1l34COsALxg7VpgwQJv02zVSm43bJDdQF26aINKL7zgfO3+/c7TVhkmWSkuli5N\nrVrG+vUuVgRTAOqkBv2gspmPPrIOf/JJ49RrfSUyhrAC8IKzzwbatvU2TXVWw9ixclaCOnXNDVaF\nv1WNiGGSjQoV5M/sKdBu5k5mpjabL9gMOHV84Kef5LakBOjXz3mRqB79OJvqzSzGsAKIV559VtvX\nG1cLl2CrGxkmmShb1uiZz8qBESC9f61fL7t3VAczbklLk+tZ1JaD3sufFfrzaWmh3csnWAHEK2oX\nkBvmzrWv4Qwa5H33FMMkAu+/bx2+fXugh7FQESJwNlDHjoHx9Pa+9F1A9etHdn+PYAWQSNi5teza\nVdZwrr8eyM3VwkeOBMaM8b57imESmYwMuVDx/vsjVwR69KZe7rhD2gi7/34tTG/3y2ohZgwQFKNl\nzbm5uZSfnx+TeycM778vzU+rqM3Uw4flNDJ1RoEdSbRknWHC4oYbAv1V7N/vPH3aCbt1Au3aGVva\n+m/P6poIvk0hxGIiyg0eMzjcAohnmjTR9tWZPwcPyu6epk39W4PAMMnCf/8bGOZH7Vtf+CfQDDxW\nAPGMfmHKgw/KrToVFJBrEOxmE8Tx6kOGiSl+d7+YF3lVqmQ8Ns9QiiGsAOIZ1Zrmww/bxxkyBPjl\nF+142jRg715rD2MMw/iP2cS7agFXJRyHNT7BCiDeIQJGjXKO0769tn/ttQnVBGWYqGJnNdQtbux9\nmf0UDxliPLYyXR0jXCkAIUQPIcRaIcR6IURAdVQIUUEIMVU5/6sQop7XgjJBePVVuQoxEqfYDJPs\n7NgR2fULF2r7bgdyzTa4evWKTAYPCaoAhBBlAbwN4BIAWQD6CiGyTNFuA7CHiM4C8BqAILYKGM+5\n7z53Dl8YhgmfcuVkN+tvvxnDnVbq61sNwQzJRRk3LYA2ANYT0QYiOgrgYwBXmOJcAeADZX86gC5C\ncFWUYZg44J57vE3vuuu0Qv2994ARI4DOne3jly0rZwllZAAPPeStLBHiRgGcAUDnvgqFSphlHCI6\nDmAvgAwvBGQYhokIveE1r21i3Xab0fa/HW3bSiN1cTQDCADKBY8Cq5q8ufPLTRwIIQYCGAgAderU\ncXFrhmGYCLnySlnw+20Pa8WKhFt86aYFUAjgTN1xbQBb7OIIIcoBOAVAkSkOiGgcEeUSUW7NYM4X\nGIZhvCIaxhCbNYsbO/9ucaMAFgFoJISoL4QoD6APgBmmODMAqIZqrgXwPcXKxgTDMAzjiqBdQER0\nXAgxBMBsAGUBTCCiVUKIkZDOiWcAeB/AJCHEesiafx8/hWYYhmEiJ2bG4IQQOwFsCvPyGgB2eShO\nrEm2/ADJlyfOT3yTbPkB7PNUl4g86UOPmQKIBCFEvlfW8OKBZMsPkHx54vzEN8mWHyA6eWJTEAzD\nMCkKKwCGYZgUJVEVwLhYC+AxyZYfIPnyxPmJb5ItP0AU8pSQYwAMwzBM5CRqC4BhGIaJEFYADMMw\nKUrCKYBgvgliiRBioxBihRBiqRAiXwmrLoT4VgixTtlWU8KFEOJNJR/LhRCtdOn0V+KvE0L014Xn\nKOmvV6713OKqEGKCEGKHEGKlLsz3PNjdw6f8PCmE2Kw8p6VCiJ66c8MV2dYKIbrrwi3fO2WF/K+K\n3FOV1fK++cgQQpwphMgTQqwRQqwSQtyjhCfkM3LITyI/o3QhxEIhxDIlT0+FK4dXebWFiBLmB7kS\n+Q8ADQCUB7AMQFas5dLJtxFADVPYiwAeVvYfBvCCst8TwCxIQ3rtAPyqhFcHsEHZVlP2qynnFgJo\nr1wzC8AlPuThQgCtAKyMZh7s7uFTfp4E8IBF3CzlnaoAoL7yrpV1eu8ATAPQR9kfA+AuZX8wgDHK\nfh8AUz3KTyaAVsp+FQAFitwJ+Ywc8pPIz0gAqKzspwH4VfnvQ5LDy7zayupFhqP1U17K2brj4QCG\nx1ounTwbEagA1gLI1L3sa5X9sQD6muMB6AtgrC58rBKWCeB3Xbghnsf5qAdjgel7Huzu4VN+noR1\n4WJ4nyDNn7S3e++UD30XgHLm91O9Vtkvp8QTPjyr/wHomujPyCI/SfGMAJwE4DcAbUOVw8u82v0S\nrQvIjW+CWEIA5gghFgtp+hoAahHRVgBQtqpTUru8OIUXWoRHg2jkwe4efjFE6RKZoOvKCDU/GQD+\nIekDQx9uSIt88pGhdBW0hKxhJvwzMuUHSOBnJIQoK4RYCmAHgG8ha+yhyuFlXi1JNAXgyu9ADDmf\niFpBus+8WwhxoUNcu7yEGh5LEjUP7wJoCCAbwFYAryjhXubH17wKISoD+BTAvUS0zymqjRxx9Yws\n8pPQz4iISogoG9J8fhsA54Qhh+/PLtEUgBvfBDGDiLYo2x0APod88NuFEJkAoGxVr9R2eXEKr20R\nHg2ikQe7e3gOEW1XPtBSAOOPNj86AAAgAElEQVQhnxOCyG0VvgtAVSF9YOjDDWkJBx8Z4SCESIMs\nLD8ios+U4IR9Rlb5SfRnpEJE/wCYBzkGEKocXubVkkRTAG58E8QEIUQlIUQVdR9ANwArYfSV0B+y\njxNKeD9llkY7AHuVZvVsAN2EENWUZm83yH68rQD2CyHaKbMy+unS8pto5MHuHp6jFmIKV0E+J1WG\nPsqsjPoAGkEOiFq+dyQ7WvMgfWCY5fbFR4byv70PYA0Rvao7lZDPyC4/Cf6Magohqir7FQFcDGBN\nGHJ4mVdrvB7E8fsHOauhALJP7dFYy6OTqwHkaPwyAKtU2SD75b4DsE7ZVlfCBYC3lXysAJCrS+tW\nAOuV3wBdeC7kh/AHgLfgz6DiFMgm9zHImsZt0ciD3T18ys8kRd7lykeWqYv/qCLbWuhmWdm9d8pz\nX6jk8xMAFZTwdOV4vXK+gUf56QDZrF8OYKny65moz8ghP4n8jFoAWKLIvhLAiHDl8Cqvdj82BcEw\nDJOiJFoXEMMwDOMRrAAYhmFSFFYADMMwKUpQp/B+UaNGDapXr16sbs8wDJOQLF68eBd55BM4qAIQ\nQkwAcBmAHUTUzOK8APAG5Kj0IQC3ENFvwdKtV68e8vPzQ5eYYRgmhRFCbPIqLTddQBMB9HA4fwnk\n/NRGAAZCruBjGIZh4pygCoCIfoDz6rgrAHxIkgWQK9EyHeJHnV27gG3bYi0FwzBMfOHFGICdYaKt\n5oiKgbSBAFCnTh0Pbu2OmkpvGS95YBiG0fBCAbg2QERE46A4Os7NzQ2Ic+zYMRQWFqK4uNgDsTRm\nzZLbNWs8TZbxmfT0dNSuXRtpaWmxFiUo990HvP46VzKYxMILBeCZgbbCwkJUqVIF9erVg/DQ2dXB\ng3J7jpU9PiYuISLs3r0bhYWFqF+/fqzFCcrrr8daAoYJHS/WAdgZmwqZ4uJiZGRkeFr4M4mJEAIZ\nGRmetwYZhtFwMw10CoBOAGoIIQoBPAHp5gxENAbATMgpoOshp4EOiEQgLvwZFX4XGMZf3MwC6ktE\nmUSURkS1ieh9IhqjFP5QZv/cTUQNiag5ESX05P6yZcsiOzsbzZo1w3XXXYdDhw7FRI7XX389Zvfu\n1KkTr9FgmBSATUGYqFixIpYuXYqVK1eifPnyGDNmjOtrS0pKPJMjlgqAYZjUgBWAAxdccAHWr18P\nAJg8eTLatGmD7OxsDBo06ERhX7lyZYwYMQJt27bF/PnzsWjRIpx33nk499xz0aZNG+zfvx8lJSX4\n97//jdatW6NFixYYO3YsAGDevHno1KkTrr32WjRp0gQ33ngjiAhvvvkmtmzZgs6dO6Nz584AgLvu\nugu5ublo2rQpnnjiiRMyzpw5E02aNEGHDh0wbNgwXHbZZQCAgwcP4tZbb0Xr1q3RsmVL/O9/1n4h\nXnzxRTRv3hznnnsuHn744RPhn3zyCdq0aYOzzz4bP/74IwBg48aNuOCCC9CqVSu0atUKv/zyi2M+\nvJCPYRgf8cIBQji/nJwcMrN69eoT+/fcQ9Sxoze/Vq3k7557Am4ZQKVKlYiI6NixY9SrVy965513\naPXq1XTZZZfR0aNHiYjorrvuog8++IBIlnI0depUIiI6cuQI1a9fnxYuXEhERHv37qVjx47R2LFj\n6emnnyYiouLiYsrJyaENGzZQXl4enXzyyfT3339TSUkJtWvXjn788UciIqpbty7t3LnzhFy7d+8m\nIqLjx49Tx44dadmyZXT48GGqXbs2bdiwgYiI+vTpQ5deeikREQ0fPpwmTZpERER79uyhRo0a0YED\nBwx5nTlzJrVv354OHjxouEfHjh3pX//6FxERff3119SlSxciIjp48CAdPnyYiIgKCgpIfYZ2+YhU\nPiLjOxHPyAmgsZaCSQUA5JNH5XDMjMHFK4cPH0Z2djYA2QK47bbbMG7cOCxevBitW7c+EefUU08F\nIMcMrrnmGgDA2rVrkZmZeSLeySefDACYM2cOli9fjunTpwMA9u7di3Xr1qF8+fJo06YNateWLliz\ns7OxceNGdOjQIUCuadOmYdy4cTh+/Di2bt2K1atXo7S0FA0aNDgxTbJv374YN27ciXvOmDEDL7/8\nMgA5w+qvv/7CObq5sHPnzsWAAQNw0kknAQCqV69+4tzVV18NAMjJycHGjRsByHUaQ4YMwdKlS1G2\nbFkUFBSciG+Vj8qVK0ckH8Mw/hK3CsDLedXqeGZubvC46hiAHiJC//79MWrUqID46enpKFu27Il4\nVjNXiAijR49G9+7dDeHz5s1DhQoVThyXLVsWx48fD7j+zz//xMsvv4xFixahWrVquOWWW1BcXHyi\nm8UKIsKnn36Kxo0bO8axm2mjyqWX6bXXXkOtWrWwbNkylJaWIj09PSC+/ppI5WMYxl94DMAFXbp0\nwfTp07Fjxw4AQFFRETZtCjTI16RJE2zZsgWLFi0CAOzfvx/Hjx9H9+7d8e677+LYsWMAgIKCAhxU\nV6fZUKVKFezfvx8AsG/fPlSqVAmnnHIKtm/fjlnK0uYmTZpgw4YNJ2roU6dOPXF99+7dMXr06BOF\n8JIlSwLu0a1bN0yYMOHEYHNRkZPJJ9lyyczMRJkyZTBp0qSgg96RysdEzv33A126xFqK5OHbb4Fk\nmpsRty2AeCIrKwvPPPMMunXrhtLSUqSlpeHtt99G3bp1DfHKly+PqVOnYujQoTh8+DAqVqyIuXPn\n4vbbb8fGjRvRqlUrEBFq1qyJL774wvGeAwcOxCWXXILMzEzk5eWhZcuWaNq0KRo0aIDzzz8fgGyt\nvPPOO+jRowdq1KiBNm3anLj+8ccfx7333osWLVqAiFCvXj189dVXhnv06NEDS5cuRW5uLsqXL4+e\nPXviueees5Vp8ODBuOaaa/DJJ5+gc+fOqFSpkmMeIpWPiZxXX421BMnDmjVAt25A//7AxImxlsYb\nYuYUPjc3l8xzzdesWeNLH3AoXUCJxoEDB1C5cmUQEe6++240atQI9913X6zFOkGk8vn1TniN2pMW\nb7aA4lWuRGT+fOC884B27eR+rBBCLCYiT0oz7gJKcMaPH4/s7Gw0bdoUe/fuxaBBg2ItkoF4l49h\nUhnuAkpw7rvvvriq8ZuJd/kYJpXhFgDDMEwIJFN3WtwpgFiNSTDxB78LTDyRjLYJ40oBpKenY/fu\n3fzhMyf8AejXGjAM4y1xNQZQu3ZtFBYWYufOnZ6mu2uX3LJHsMRC9QjGMIw/xJUCSEtL88X7U1aW\n3HLDgmGSl7/+AsaPB0aO9Le7JpnKkbjqAmIYhgmX664DnnkGWLHCn/R5DIBhUoDSUvljEgvVe2gy\n1dD9hhUAw5ioUweoVi3WUjDhwgrAPXE1BsAw8cDmzbGWgIlnkknBpHQLoLgYYEdUDMO4gccAkowH\nHgCuvBJQPBsyDJPAhFJAT5oEVK8OWLjfSClSWgH8+afc7tkTWzkYhokuQ4bI7z6IW44TFBQAbdv6\nK1MsSGkFwDCJwrFj2oJGJnL27Qst/owZ2j6PATAME1XuuAOoWVMqAsY7vOrX373bm3SiDSsAxKdG\n79IFUHzTM2FSXAwsWxZrKbxB9aaZ6n3Wbgjle3arAMo4lJQTJwI1agAmV+IJQVIoACJg8uTQa0fx\nPKr//ffJU3jFittvl0pUceXMMAEEKwOGDQPGjXOO9+23crtqlXdyRYukUABTpwI33ww8/3ysJWHi\nCXV214EDsZXDC+KplfqvfwFTpsRaCntCqdgFizt6NDBokLEFEE/PIlKSQgGog2PbtsVWDi8wF1aT\nJmlL3JnQSKYPNZ547TXghhtiLYU9fnQBxXNvQSQkhQJQH3iiP6S5c4EqVWT3j0q/fsDw4bGTKZFJ\nlvcCSI48hAqRfPfXr3cXP5z/yIv/NZErGqwAEPkDfOQRuaAsUn74QW5/+skYvnVr5GmnInv3ym0y\nFJ6JXMiEyx9/yG7dyy+PtSTOg8AqifieJZUCOHo0tOu8emCjRnlrUsL8safSx//ss5r/BjdMn64V\n9Gb++UduhQBKSoAjRyKXLxh+P6tELGQiJdTvOhTU/7O01Hmdhf5/T6bvMSkUwPz5cjtunLv4hw4B\njz/uvkA4eBDIyAC++SY8+dySih+3mccec++5raBA2oDv3z943HLlADfeJeP94453+bxE/R68yvPh\nw0BenvW5xx+X6yzsZow5fZtm+Z59Fvjuu/BkjDYJqwCIgKefls1E/SKMdeuCX/v889JxhDp9y8yx\nY8CGDdpxQQFQVBS9vvhotABKSoCLLpLjDonKoUNyu3Gjc7wvv3SfZklJ2OIA8OZZ7doVWOuNZuVg\n3jzgzjvDv37bNuCKK0JfbWsmXAVAJL9XIYA5c7Twu+6S73xBQeA1ags+HAVgjvPYY8DFF4cmc6xI\nWAWwdSswYgTQrZvxBTn77ODXqgWHivkFGzYMaNgQUF0TR6vWFc2PvKhI1ob69g392oICrdUVCnPn\nBv730WDoUPdx9c+6Wzdv5Vi2THZZBaNmTaBPH3u5/KZzZ2Ds2PCvf+YZaTrhww+d4/33v9YF7vz5\nwNdfR9YCWLJEbl94ITAsHMWUrK3zhFUAqsem4uLwaghOqLVis5G4aL0EwVoAdn3ekdzDDYcOyQ+7\ncWPgvPNCu3bdOqBrV2DgQPs4+fmhyxQqau2vTx/gvfcCz+un4dq1EIORny9rgWays2WXlRs+/zy8\ne/tJhQqyFu0FO3YAN94IXHZZ4LnzzpPh6sBrOO+q/hoiYMECYPny8GQF3K0DKCgw9hwkAgmrACKZ\n+RPM3V8kL14kuMnLpElA1aqRvcyh3lNl8GDZtFc5cEC2IIKlsWyZ1jL7/Xf7eK1bu5fFjqNHgbfe\nsu/KadxYdhlNnSrt65jp1Suy+xPJfDz7bGTpmFHHq/bt889d5eLFwNq19uePHgXGjHGfntP3oz6f\nv/6yj6MfoHWD1XsohBwbbN/eXRpu0lZbEgBw7rnAxx/L/SeflD0HTkyZ4t4CaTRwpQCEED2EEGuF\nEOuFEA9bnL9FCLFTCLFU+d3uvahGHn1UvXfo19oN/paWAtdco9USzS/ekiWyAPPSKuO2bYHyOLUA\nZs6U25UrgU2brB1gr1plXEtgRTjKzTwfe88e7eV3Yvz40O8VCvq8vPKK7PJxuucff1iHf/ZZ4BTc\n6dNlS+CPP6Ti2L/fvVx6z2KjRlnHyc6WC6tUHnjAOc3TTgNuu829DG4pKQFyc4EmTbQw/f8aipkV\nN103Zctq940kHT128ZwqHW7TMJcz774rt6FUxObPlwvorCoesSKoAhBClAXwNoBLAGQB6CuEsJqo\nN5WIspWfRePaWyZN0vbtHhoR8OKLgeHqw9MzbZosPD77TAuzqnmcc46sRUbKkiXyZcjMBK69VobZ\nKTP9rBg1rwcOAPXqAS1aBMZv1kwak3NC34ISQprSCIbbWVP79wc2wVW87EZT09KPK6jdY04+HuwG\n6K65JjDsuuvkWMBZZwH16wPnn+8s0yOPaPu1a2uVBX24nmXLpGkFlVdecU4fkMbHAOnQZPTo4PHd\nYFUQq89t/36gfHnr67ZsMR7//LOmRN0ogF275DVW1KkTPB0rtm93fuesWpp6ZTN9euDkCHMagweH\nJtPhw8CsWXI/nsxouGkBtAGwnog2ENFRAB8DuCLINb5ifrjmgrpzZzmI9MYbwEMPaeGLFlkX6kRA\n797Agw8aw+2ankVFoct89Kgc9FJlv/Za7UX46qtAefSsXh14btCg0GW4+WageXNjOuqLPXly8OvN\nhYRVYb5xI3DyybIbRsUvBaAay9O3TNTuO7+6SaxaXHrMFQ67d4UochMfe/bICQtWlJaGlr5VIav+\nh3amjj/+GDjjDK1gA4AOHYJbxdy0SVv0qF4DyIrO4487yzZ7tnyH8/O1dR4qaotL30353XfGFlYw\nPvhAKv2uXY3jQZG+t4MHy1mL8YYbBXAGgL91x4VKmJlrhBDLhRDThRBnWiUkhBgohMgXQuTvVKfY\nhIG+OW31YObNk4NI991nDG/TBqhY0f191A/A6uMYMkSrPQejsBC4/3456KV3LGFGTctqPYNa+P7f\n/wW/nx2TJ8uuo+PHtTxt324fv2NH4KOPtGPz/6CveavKTJ2GO2yYzI/+XoDx/zp4MLKxDHMBAGgF\nrtUgrBWvvy4LjlAW8qnP/d573V9jZuTIwHdx0SLjMZEs8IIpM6uWWe/exvQ3b5YtkhEjtLCJE2U+\njh61fsdLSmRL+7ffrO/7xhty27On9Xk1zW+/le/FhAlSaTRpAlx9dWD8rCw5g8gunWnTgB495Dvc\nujXQqZMxntpNdexY6K0GtcX46qtamP5/t1oJHEzRLVgg/99ffgm07OtXBSVkiMjxB+A6AO/pjm8G\nMNoUJwNABWX/TgDfB0s3JyeHwuWcc4jkI5a/atWMx6H+vvjCOnzAAKLx44ny892lY2bzZqLq1Y1x\n/vMfec7q2qefDp6+Vfgbb2j33LTJXh79Nf36BZdfDc/LI3rttcD4EyYYj9etI/rmG2PYgQNEgwZp\nx23baulfeqkMO3gwUD4rvv9eyqHyxhta/OXL7f+fYL/y5cN/d9avd77vtGlECxYE5q1Bg+DPdsQI\nuX3nHed3oWpVoj59iD7/nOiqq4LLvG6d8fiRR6zjHTrknE5Wlrb/2GNEy5YZz7/6KlFpqbv/cfr0\n8N59lX37jOGzZ7u7b69eRMXF1ue2b9f2zzgj9Hfjqafszx0+bP2OuwFAPpFz+er2FzwC0B7AbN3x\ncADDHeKXBbA3WLqRKIBwP1a7n50CUH8//uguHSKio0eJCgqIjh8n6tIlME7z5tZ5WLIkuAIYO9b+\n3J9/yntafRxu/7eFC4nWrAn/f168mGjWLGPYmjVEHTpox7VqEc2YYUy/qCjwfioTJ8qPT/+R/vyz\n/L/KlAm8xut3w83v99/9SVet2Fx9deC5jRvDT3fUKHfx9u93Pq9XAABRerp///Gzz1qHq5gL6Jtu\ncp/2yJHW4Vbfr1e/Q4fCLv4o2gqgHIANAOoDKA9gGYCmpjiZuv2rACwIlm48KQCvfm5lW7QoMKxn\nz8juPWBAoKJS2bw5tLRKStzX3PS/fv2Ivv469P9q1y7r86tXx/6ZpvJvyhTn82edFXsZr7km9jKE\n85s8Oezij6KqAOT90BNAAYA/ADyqhI0E0EvZHwVglaIc8gA0CZZmMiqAFStiL4P+t327/L8+/DD0\nazt39le2885zPr90aez/P/7xz6+fvisz9PLPOwUgZHrRJzc3l/LDWPp57Jj9lDSGYZhE4Ikn5MKx\ncBBCLCaiXC/kSLiVwOH+aQzDMPHCU0/FWgJJwimA556LtQQMwzDJQcIpAIZhGMYbWAEwDMOkKKwA\nGIZhUhRWAAzDMCkKKwCGYZgUJeEUQKSOHRiGYWLNqafGWgJJwikAr+yfMwzDxIrrr4+1BJKEUwAx\nWrjsGZHagE9FateOtQQM4y2qD4RYk3AKoFGjWEtgj5W3KL3nsssuk461o0monovikXnzjMdWfgDq\n1YuGJKHx44/Ru9eBA8Cll4butF3vAlLFjZtPJ0aOjOz6VCBeKrIJpwBOOUVzMG6H3q2j15x8sv05\ns6u3sWM1Byg33AB8+aXcD6ewatky9Gvy80PzhqTn+PHAMLPrvu3bpZOZWrXcp+v0/9nRsKFRkVWq\nFBgnPz98Zzm33hredcHw0vtZMNLTpWe5l1+Wx2lpgT6czTz4oHTWYqZ3b/f3JQo0z6J6nbPihx+A\nX381egQLl9NPl5WqeCY93Trc6vuKBQmnAIDgH1aWlcdihaZNI7u3k/26M01+0Pr31/b1Gt9O/tq1\ngSuvDAxv0QIYM0buX3ihs3xEwMKFUhnl5FgbznNTSzTLeNJJQLVqxrCMDOnlbOHC4OmpnHOO9K0L\nAP/5j3v/qG+/DQwfLvfN3pkaN5ayBPtvrKhdW5MnXOzGpRo31uw/6tG7KfUK8/MqU8b6PSstlS2F\nqVOBF16wV96tWmn7+kJMrcRkZACLF8v9J57QujSsvNnpueAC6ZnPysMW4O5ZqG45ze+jHQ0a2J/L\nygLatrU+16eP0R+3mWbNgt/b3HpVKVcu+LXRIOkUQJs28sO79FJ5rHfxBjjXpLOygJde0o7Xrw/s\nf9Z/zE4j+VOm2Hf36F/+00+X2w8/lO4R33/f+po2bYBvvpE1tmAtnNat5ctrx9tvO1/frVvgf1yh\nAnDaadbx69SR/4veRV7r1jLsnXeMccuU0boIrrtOyqnvKlm7Vts3f7jPPSfTNBceVt0YbqlRI/IC\n2ao5P3y4TNuKAQOMx7feKn0p6+nYMTQZzM/Lzl2pELKloA5CpqVZp6cWxJ98ormqzMrSwk891agk\nVCfvTl20+pZB1aravvp/vPUWMHCgFn777dq+nRFIK0UydKjRL/C119rL9OOPxhb51VcDd94pn+mU\nKfaKCnA3NtWmjXW42b92rEhIBTBhgizIX3890LfmGYq34q++kg9R7xf4qqtkrcfMkCEynVWrgAce\n0Gqa1asDf/+tNav1NGpk3YxVH7iTktI7TH/iCWD/fumwvVo1ec8NG6yv695d1uivvFI6r1aZOdP+\nXgDw3nvGY7NsRMD8+VKOVaukgrGKE6z75txzgblz5X7lynJrbm2UKQPcdJNMT+3K0Q+I6bv3li4F\ntmyxvpe+ZRNJfyqR+9qYuW+8d28ZZnV/J5nq1jUeV6pkDPvpJ2PNcf/+4LKpz0u9b5kyxsJr5Ej7\n7sDLLw9Mxyptc/pWcZx83epbiur7Ubu2rPR8953s5tMr43/9S26bNJHfyQMPyO9az5gxgeNcrVpp\n38czzzgX4vo8AUDfvsC77zrHt8LcbTZ0KJCXZ18OxItP4IRUAO3bS0fV99xjXVDZ8dlnWo1bz/XX\nG9N55hnpJNqqiVmnjmw2v/KK9b3q1JFbNT2rF6BHD+COOzR51Y9BxdwMNt9HCKBfP+34kksC76HH\nqeDu1k1u27WTcmRlBfax165t/VE4KTm7c3bheXlyzASQtbInngCqVAEyM63jHzkCfPGF/f3dEory\n6N5d23/rLVn49+5tTEN9rk4feHq6zOMff0gn7c8+azxvnkxgfj8Ae6Wv3tfcAnj8cXsn9o8/ru07\nPVMiLX1zoaoeO/2fVv3halfVRRfZt1pUXnpJa9mrZGZat2hPOUXK8uijwd9Tvcxm5XzWWbIwHzHC\nPo3zzgvsyjzrrECn9R9+qO3HiwKIk54o73DzQS9ZAlx8MbB7txzEvOAC4/kyZYxNVLV5C8iXeNs2\nuW/VP2hXQ7IqxKOFWsPNzJSKCwD27ZMtDTezqv7+W27dNFuD/f92tbFOnbQPpkOH0KbJRdoCcEvV\nqlLZzpplfH76Zn6LFsatHWr+zHbh9YOaGzcCRUWB1157rbEw1edB//65fcdat5aziCpXNioDFX3r\nQq9g9KjPtaREKm4AuOUWWVmyanWfdJLcZme7k1FPqM872P+gT8/czVmmDPDmm3LMxI7HHnNXEdV3\nt+V64s4lcpJOAbghOxvYvFl2e5i1tBU1a8qtuVCy0uJ2H0gkRDpl7PLLZfP5oYe0fukqVWSXTTBO\nOSU0OdQ4dvnXdzdESrj/cd26wKZNcj/U/1atIervrV+dftllsnLRuHHocpWUGNOtWzewRpqVBXz0\nkXx3rbBrAQSjUiX7/0KvAOyer6oASkuBrl2BiRPlGM9JJ0kFYB57q1lTdnW5eQft0Muwaxdw992y\noI6ksmX3H9iNf4Wajkqkk1G8IiG7gJwIVvNSqVDBXeGvxzx906nvN9hLp/Z1e7XIaehQ+8HMcuVk\n89luUNKODz4wznoKpbC0yv+IEcD994cmgxtCLcTVLpdOneRMpFBQBxfPO8/6vBCyzzoc5eSm1j53\nrhz/sJtgoG8B6FuukWA1BmCWs3Vruc3MlOf699dq+du2ycLezPnnW3dvhUNGBlCxovW5oUPloLrd\nrB2rFpSZjh0D312n/1c/I23cOFkxiJe5/3qSQgHk5Gj7DRt6n746Tcw8s8ZJAag1IrsP+r77gO+/\nD+zTtMLNi/Pmm8DzzwePFwr9+sm+TJVy5YzjDaEOfp5+urctI/PAp1tuvFFek5cnm+LmfmAnevSQ\nce1qrsEGHCNFHROxm7qobwGEWmsNhr4FYM7nU0/JcTmr/6VWLU0ZxIJateSgulUFyPzsnd4D81jb\nuHFyALpr18C4+i66O+6Q02ftFFQsSYouIL8161lnuS/szF1AdrKVKQN07uyNfNFCCDn4WLUqsHev\ndd7UAWd1MFyPXwuu9MyZIweHzdNPo0X16tG5j50iVWulZ57pnbK1GgMwK4CyZcNbrGjFN99Ed5qk\nWwVg5vTT7adUW3UPq602/WSCWJMULYBYYfWQ3XYBhUI0B4zdsGCBXF9hNX2yXTvZD2u1OMpuzrmX\ndO0avsVY88cfjt0mq1XKgPWKWz+oWlWOEcye7V2a+vfPjzEuM927Az17hn6d+t9bLX4EtOcbzvgM\n4M1YQqjp+E1StAD0OC2A8hr9TCEVswII92HPny9nm4wcGX99h02aOC++ipalQ7tWllf/l5d2my6+\nGFi3Tht89gqrWVw33ODtPVScBoGjid3zff552d1jZ8pCvW7MGNmV9umn8hsOtwXgRLxM8wxGUigA\n/UOLprG1evXk4pb0dODQIaMskfYFq/Py2bCWPX4WQn6kfdZZxjGVSJkzx/2kh0hwMwsoFphlsJvG\nqqKXvXFj4JFHjOHm/WD3cyLeKm12JIUCiCXq7AeVeJwGmuyk6v9jNfjoB1YKwO/Bbj9wo7wieZea\nNAF+/13uJ0oLIAEfY3yjzhQw1/TCebHioZYVz/jdBRQK334rFz4lC+psupo1rccAkkkBeNUC0E91\nTZRKSQI+xkDUObdeNq/DZehQYMeO4CarQyFRXiY78vP9MdEdTwry4otDX1MQzzzzjJzxpTdK57QS\nOBFwowDcYvWsMzK0KYr3fsoAAAijSURBVOncAogi6qj+xRfHVg5AvlzqymEv0koGcnKkIT6/8FJB\nZmTIrVqb168xSUTCXQuQlqa1ZuN1DCBcIm0BXHihfWtPvyI6EUgKBRDPJPKHEu/40QWkmr5QBwh/\n/TX8tOKBdeukmYRIUKfvVquW2AogFFMm4cZR14F4tQrbb1gBMAmLH4WQuYBLlA/ZjsqVtVZNuJx5\nplxp/uWX8TEIrLZqnOz8W+H3GAAgXcC+8kritBxTYhbQli3SfHCikuhjAH4T7P8ZP15Oq3VyVaiS\nkQH8+adx0dpJJ0nzEanM0KFyu3Wr3JqN1PmBaj7CbB791FPlSvRQ7QhFqgDcxKlZU/NjkAikhAKw\nsykf7yRiMzuauP1/9J6lgvG//0lTEnpTFgcPhiZXMpObC0ybFt5K3VCpWxf4/HNro43h+JaOdBA4\nGb9H7gKKEtGyWZ+KOI0B6B3FV6kiTQY7cfrpgR6m4oF4sR8PSDPPdiYvvObKK61X3IeDV+sAkul7\nTIkWQKKSjDUOL1G9u5kX46n07280y7tvn/8y+UVeHrBzZ6ylSA7M39W990qzK0Di2PDxClYACYDd\nS5meHp7BsmShWTPpNzhenGv4SeXK3tnOT1XsvqNu3aQButmzrd1WJjOsAHwmklpDsGu3bEltBQBY\n259PpiY64x1OXUBTpkgF4MafSDK9X67GAIQQPYQQa4UQ64UQD1ucryCEmKqc/1UIUc9rQRMVdV65\nHwPR1aol7gB3NEjGJjsTPqr/jVq1As9VqxbcknCk79Opp8ptVlZk6XhJUAUghCgL4G0AlwDIAtBX\nCGHOwm0A9hDRWQBeA2DhBto/1BXA8ThVr3t34MMPw/PW1aCB7MN+/33v5WKYVGPUKGD9emtnRW44\n91zgjDOA554L7/qcHOmZzGvPfZHgpguoDYD1RLQBAIQQHwO4AsBqXZwrADyp7E8H8JYQQhBFp7HU\nqFH8NsuEAG6+Obxr09KMs1gYhgmfcuUicxlbuTJQWBiZDHrbSvGAmy6gMwD8rTsuVMIs4xDRcQB7\nAQSsPxRCDBRC5Ash8nfylAbGJ84/X26j6RyIYRIRNy0Aq54vc33bTRwQ0TgA4wAgNzc3TuvsTKJz\n9tnx2yJkmHjCTQugEMCZuuPaALbYxRFClANwCoAiLwRkGIZh/MGNAlgEoJEQor4QojyAPgBmmOLM\nANBf2b8WwPfR6v9nGIZhwiNoFxARHRdCDAEwG0BZABOIaJUQYiSAfCKaAeB9AJOEEOsha/7c+8ow\nDBPniFhV1IUQOwFsCvPyGgAitHIeVyRbfoDkyxPnJ75JtvwA9nmqS0SeuJ2KmQKIBCFEPhHFkXms\nyEi2/ADJlyfOT3yTbPkBopMntgbKMAyTorACYBiGSVESVQGMi7UAHpNs+QGSL0+cn/gm2fIDRCFP\nCTkGwDAMw0ROorYAGIZhmAhJOAUQzDR1LBFCbBRCrBBCLBVC5Cth1YUQ3woh1inbakq4EEK8qeRj\nuRCilS6d/kr8dUKI/rrwHCX99cq1nhs8FkJMEELsEEKs1IX5nge7e/iUnyeFEJuV57RUCNFTd264\nIttaIUR3Xbjle6cskPxVkXuqsljSNxPpQogzhRB5Qog1QohVQoh7lPCEfEYO+UnkZ5QuhFgohFim\n5OmpcOXwKq+2EFHC/CAXov0BoAGA8gCWAciKtVw6+TYCqGEKexHAw8r+wwBeUPZ7ApgFaUepHYBf\nlfDqADYo22rKfjXl3EIA7ZVrZgG4xIc8XAigFYCV0cyD3T18ys+TAB6wiJulvFMVANRX3rWyTu8d\ngGkA+ij7YwDcpewPBjBG2e8DYKpH+ckE0ErZrwKgQJE7IZ+RQ34S+RkJAJWV/TQAvyr/fUhyeJlX\nW1m9yHC0fspLOVt3PBzA8FjLpZNnIwIVwFoAmbqXfa2yPxZAX3M8AH0BjNWFj1XCMgH8rgs3xPM4\nH/VgLDB9z4PdPXzKz5OwLlwM7xPk6vf2du+d8qHvAlDO/H6q1yr75ZR4wodn9T8AXRP9GVnkJyme\nEYCTAPwGoG2ocniZV7tfonUBuTFNHUsIwBwhxGIhxEAlrBYRbQUAZav4BbLNi1N4oUV4NIhGHuzu\n4RdDlC6RCbqujFDzkwHgH5Im0PXhhrTIwUR6JChdBS0ha5gJ/4xM+QES+BkJIcoKIZYC2AHgW8ga\ne6hyeJlXSxJNAbgyOx1DzieiVpDe0+4WQlzoENcuL6GGx5JEzcO7ABoCyAawFcArSriX+fE1r0KI\nygA+BXAvEe1zimojR1w9I4v8JPQzIqISIsqGtJ7cBsA5Ycjh+7NLNAXgxjR1zCCiLcp2B4DPIR/8\ndiFEJgAo2x1KdLu8OIXXtgiPBtHIg909PIeItisfaCmA8ZDPCUHktgrfBaCqkCbQ9eGGtITHJtKF\nEGmQheVHRPSZEpywz8gqP4n+jFSI6B8A8yDHAEKVw8u8WpJoCsCNaeqYIISoJISoou4D6AZgJYym\nsvtD9nFCCe+nzNJoB2Cv0qyeDaCbEKKa0uztBtmPtxXAfiFEO2VWRj9dWn4TjTzY3cNz1EJM4SrI\n56TK0EeZlVEfQCPIAVHL945kR2sepAl0s9y+mEhX/rf3Aawhold1pxLyGdnlJ8GfUU0hRFVlvyKA\niwGsCUMOL/NqjdeDOH7/IGc1FED2qT0aa3l0cjWAHI1fBmCVKhtkv9x3ANYp2+pKuADwtpKPFQBy\ndWndCmC98hugC8+F/BD+APAW/BlUnALZ5D4GWdO4LRp5sLuHT/mZpMi7XPnIMnXxH1VkWwvdLCu7\n90557guVfH4CoIISnq4cr1fON/AoPx0gm/XLASxVfj0T9Rk55CeRn1ELAEsU2VcCGBGuHF7l1e7H\nK4EZhmFSlETrAmIYhmE8ghUAwzBMisIKgGEYJkVhBcAwDJOisAJgGIZJUVgBMAzDpCisABiGYVIU\nVgAMwzApyv8DIQwPWM9f/a0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEaCAYAAADwlvf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "ax = sns.heatmap(corr, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    print (\"Amount of features = {}\".format(amount_of_features))\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    x_result = []\n",
    "    y_result = []\n",
    "    for index in range(seq_len, len(data) ): # maxmimum date = lastest date - sequence length\n",
    "        x_result.append(data[index-seq_len: index,:-1]) # index : index + 22days\n",
    "        y_result.append(data[index ,amount_of_features]);\n",
    "\n",
    "    #print('---', data[0])\n",
    "    #print('---', x_result[0])\n",
    "    #print('---', y_result[0])\n",
    "    x_result = np.array(x_result)\n",
    "    y_result = np.array(y_result)\n",
    "    row = round(0.6 * y_result.shape[0]) # 80% split\n",
    "    print (\"Amount of training data = {}\".format(0.9 * x_result.shape[0]))\n",
    "    print (\"Amount of testing data = {}\".format(0.1 * y_result.shape[0]))\n",
    "     \n",
    "    X_train = x_result[:int(row), :] # 90% date\n",
    "    y_train = y_result[:int(row)] # 90% date\n",
    "        \n",
    "\n",
    "    X_test = x_result[int(row):, :]\n",
    "    y_test = y_result[int(row):]\n",
    "    # filter for 1 and -1 for validation only\n",
    "    X_test = X_test[y_test[:]!=0,:]\n",
    "    y_test = y_test[y_test[:]!=0]\n",
    "    #print(result.shape[0], len(y_result), int(row), y_result[int(row):])\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features-1))\n",
    "    #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features-1))\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features = 8\n",
      "Amount of training data = 261918.9\n",
      "Amount of testing data = 29102.100000000002\n",
      "(174613, 120, 8) (174613,) (70626, 120, 8) (70626,)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([1, 0, -1])\n",
    "\n",
    "X_tr, lab_tr, X_vld, lab_vld = load_data(df, seq_len)\n",
    "y_tr = lb.transform(lab_tr)\n",
    "y_vld = lb.transform(lab_vld)\n",
    "print(X_tr.shape, lab_tr.shape, X_vld.shape, lab_vld.shape)\n",
    "print(amount_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(X, y, batch_size = 100):\n",
    "\t\"\"\" Return a generator for batches \"\"\"\n",
    "\tn_batches = len(X) // batch_size\n",
    "\tX, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "\t# Loop over batches and yield\n",
    "\tfor b in range(0, len(X), batch_size):\n",
    "\t\tyield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "n_channels = amount_of_features\n",
    "n_classes = lb.transform([1]).shape[1]\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct inputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define forward pass, cost function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200 Iteration: 5 Train loss: 1.089436 Train acc: 0.352539\n",
      "Epoch: 0/200 Iteration: 10 Train loss: 1.105273 Train acc: 0.260254\n",
      "Epoch: 0/200 Iteration: 15 Train loss: 1.096695 Train acc: 0.348145\n",
      "Epoch: 0/200 Iteration: 20 Train loss: 1.090095 Train acc: 0.410645\n",
      "Epoch: 0/200 Iteration: 25 Train loss: 1.141691 Train acc: 0.312988\n",
      "Epoch: 0/200 Iteration: 30 Train loss: 1.062420 Train acc: 0.467773\n",
      "Epoch: 0/200 Iteration: 35 Train loss: 0.995617 Train acc: 0.579102\n",
      "Epoch: 0/200 Iteration: 40 Train loss: 0.985014 Train acc: 0.584961\n",
      "Epoch: 0/200 Iteration: 45 Train loss: 1.136933 Train acc: 0.278320\n",
      "Epoch: 0/200 Iteration: 50 Train loss: 1.043005 Train acc: 0.631836\n",
      "Epoch: 0/200 Iteration: 55 Train loss: 1.064301 Train acc: 0.494629\n",
      "Epoch: 0/200 Iteration: 60 Train loss: 1.096161 Train acc: 0.400879\n",
      "Epoch: 0/200 Iteration: 65 Train loss: 1.259961 Train acc: 0.204590\n",
      "Epoch: 0/200 Iteration: 70 Train loss: 1.108186 Train acc: 0.281250\n",
      "Epoch: 0/200 Iteration: 75 Train loss: 1.044770 Train acc: 0.431152\n",
      "Epoch: 0/200 Iteration: 80 Train loss: 1.272083 Train acc: 0.289062\n",
      "Epoch: 0/200 Iteration: 85 Train loss: 1.141488 Train acc: 0.333008\n",
      "Epoch: 1/200 Iteration: 90 Train loss: 1.113953 Train acc: 0.207031\n",
      "Epoch: 1/200 Iteration: 95 Train loss: 1.106015 Train acc: 0.334473\n",
      "Epoch: 1/200 Iteration: 100 Train loss: 1.099459 Train acc: 0.346680\n",
      "Epoch: 1/200 Iteration: 105 Train loss: 1.087114 Train acc: 0.406738\n",
      "Epoch: 1/200 Iteration: 110 Train loss: 1.111940 Train acc: 0.312500\n",
      "Epoch: 1/200 Iteration: 115 Train loss: 1.071104 Train acc: 0.467773\n",
      "Epoch: 1/200 Iteration: 120 Train loss: 1.004837 Train acc: 0.579102\n",
      "Epoch: 1/200 Iteration: 125 Train loss: 0.989743 Train acc: 0.584961\n",
      "Epoch: 1/200 Iteration: 130 Train loss: 1.139703 Train acc: 0.278320\n",
      "Epoch: 1/200 Iteration: 135 Train loss: 1.038746 Train acc: 0.631836\n",
      "Epoch: 1/200 Iteration: 140 Train loss: 1.065920 Train acc: 0.494629\n",
      "Epoch: 1/200 Iteration: 145 Train loss: 1.091079 Train acc: 0.400879\n",
      "Epoch: 1/200 Iteration: 150 Train loss: 1.196783 Train acc: 0.204590\n",
      "Epoch: 1/200 Iteration: 155 Train loss: 1.120126 Train acc: 0.279785\n",
      "Epoch: 1/200 Iteration: 160 Train loss: 1.104888 Train acc: 0.245117\n",
      "Epoch: 1/200 Iteration: 165 Train loss: 1.110974 Train acc: 0.315430\n",
      "Epoch: 1/200 Iteration: 170 Train loss: 1.138205 Train acc: 0.320312\n",
      "Epoch: 2/200 Iteration: 175 Train loss: 1.138240 Train acc: 0.212402\n",
      "Epoch: 2/200 Iteration: 180 Train loss: 1.111905 Train acc: 0.236328\n",
      "Epoch: 2/200 Iteration: 185 Train loss: 1.103316 Train acc: 0.284180\n",
      "Epoch: 2/200 Iteration: 190 Train loss: 1.096563 Train acc: 0.344238\n",
      "Epoch: 2/200 Iteration: 195 Train loss: 1.114172 Train acc: 0.297852\n",
      "Epoch: 2/200 Iteration: 200 Train loss: 1.078435 Train acc: 0.458496\n",
      "Epoch: 2/200 Iteration: 205 Train loss: 1.048047 Train acc: 0.579102\n",
      "Epoch: 2/200 Iteration: 210 Train loss: 1.030866 Train acc: 0.584961\n",
      "Epoch: 2/200 Iteration: 215 Train loss: 1.137741 Train acc: 0.278320\n",
      "Epoch: 2/200 Iteration: 220 Train loss: 0.984824 Train acc: 0.631836\n",
      "Epoch: 2/200 Iteration: 225 Train loss: 1.050289 Train acc: 0.494629\n",
      "Epoch: 2/200 Iteration: 230 Train loss: 1.136144 Train acc: 0.400879\n",
      "Epoch: 2/200 Iteration: 235 Train loss: 1.192744 Train acc: 0.204590\n",
      "Epoch: 2/200 Iteration: 240 Train loss: 1.127743 Train acc: 0.243652\n",
      "Epoch: 2/200 Iteration: 245 Train loss: 1.013507 Train acc: 0.459961\n",
      "Epoch: 2/200 Iteration: 250 Train loss: 1.240664 Train acc: 0.288574\n",
      "Epoch: 2/200 Iteration: 255 Train loss: 1.167340 Train acc: 0.253418\n",
      "Epoch: 3/200 Iteration: 260 Train loss: 1.120201 Train acc: 0.265137\n",
      "Epoch: 3/200 Iteration: 265 Train loss: 1.100343 Train acc: 0.312500\n",
      "Epoch: 3/200 Iteration: 270 Train loss: 1.100009 Train acc: 0.348145\n",
      "Epoch: 3/200 Iteration: 275 Train loss: 1.091984 Train acc: 0.411133\n",
      "Epoch: 3/200 Iteration: 280 Train loss: 1.113059 Train acc: 0.312988\n",
      "Epoch: 3/200 Iteration: 285 Train loss: 1.071988 Train acc: 0.467773\n",
      "Epoch: 3/200 Iteration: 290 Train loss: 1.030454 Train acc: 0.579102\n",
      "Epoch: 3/200 Iteration: 295 Train loss: 1.012598 Train acc: 0.584961\n",
      "Epoch: 3/200 Iteration: 300 Train loss: 1.148502 Train acc: 0.278320\n",
      "Epoch: 3/200 Iteration: 305 Train loss: 0.980553 Train acc: 0.631836\n",
      "Epoch: 3/200 Iteration: 310 Train loss: 1.050120 Train acc: 0.494629\n",
      "Epoch: 3/200 Iteration: 315 Train loss: 1.104832 Train acc: 0.400879\n",
      "Epoch: 3/200 Iteration: 320 Train loss: 1.219004 Train acc: 0.204590\n",
      "Epoch: 3/200 Iteration: 325 Train loss: 1.113838 Train acc: 0.268555\n",
      "Epoch: 3/200 Iteration: 330 Train loss: 1.075235 Train acc: 0.463867\n",
      "Epoch: 3/200 Iteration: 335 Train loss: 1.120315 Train acc: 0.337891\n",
      "Epoch: 3/200 Iteration: 340 Train loss: 1.162453 Train acc: 0.321777\n",
      "Epoch: 4/200 Iteration: 345 Train loss: 1.156142 Train acc: 0.221191\n",
      "Epoch: 4/200 Iteration: 350 Train loss: 1.111338 Train acc: 0.296387\n",
      "Epoch: 4/200 Iteration: 355 Train loss: 1.101037 Train acc: 0.331055\n",
      "Epoch: 4/200 Iteration: 360 Train loss: 1.097179 Train acc: 0.284180\n",
      "Epoch: 4/200 Iteration: 365 Train loss: 1.118481 Train acc: 0.312988\n",
      "Epoch: 4/200 Iteration: 370 Train loss: 1.066790 Train acc: 0.467773\n",
      "Epoch: 4/200 Iteration: 375 Train loss: 1.017761 Train acc: 0.579102\n",
      "Epoch: 4/200 Iteration: 380 Train loss: 1.015333 Train acc: 0.584961\n",
      "Epoch: 4/200 Iteration: 385 Train loss: 1.120229 Train acc: 0.278320\n",
      "Epoch: 4/200 Iteration: 390 Train loss: 0.980875 Train acc: 0.631836\n",
      "Epoch: 4/200 Iteration: 395 Train loss: 1.050787 Train acc: 0.494629\n",
      "Epoch: 4/200 Iteration: 400 Train loss: 1.097377 Train acc: 0.400879\n",
      "Epoch: 4/200 Iteration: 405 Train loss: 1.204999 Train acc: 0.204590\n",
      "Epoch: 4/200 Iteration: 410 Train loss: 1.105404 Train acc: 0.266113\n",
      "Epoch: 4/200 Iteration: 415 Train loss: 1.061280 Train acc: 0.452148\n",
      "Epoch: 4/200 Iteration: 420 Train loss: 1.133169 Train acc: 0.352539\n",
      "Epoch: 4/200 Iteration: 425 Train loss: 1.173196 Train acc: 0.310059\n",
      "Epoch: 5/200 Iteration: 430 Train loss: 1.154381 Train acc: 0.221191\n",
      "Epoch: 5/200 Iteration: 435 Train loss: 1.110995 Train acc: 0.317383\n",
      "Epoch: 5/200 Iteration: 440 Train loss: 1.104943 Train acc: 0.348633\n",
      "Epoch: 5/200 Iteration: 445 Train loss: 1.097555 Train acc: 0.388672\n",
      "Epoch: 5/200 Iteration: 450 Train loss: 1.128378 Train acc: 0.312988\n",
      "Epoch: 5/200 Iteration: 455 Train loss: 1.067730 Train acc: 0.467773\n",
      "Epoch: 5/200 Iteration: 460 Train loss: 1.007425 Train acc: 0.579102\n",
      "Epoch: 5/200 Iteration: 465 Train loss: 1.022364 Train acc: 0.584961\n",
      "Epoch: 5/200 Iteration: 470 Train loss: 1.112350 Train acc: 0.278320\n",
      "Epoch: 5/200 Iteration: 475 Train loss: 1.007357 Train acc: 0.631836\n",
      "Epoch: 5/200 Iteration: 480 Train loss: 1.058962 Train acc: 0.494629\n",
      "Epoch: 5/200 Iteration: 485 Train loss: 1.093848 Train acc: 0.400879\n",
      "Epoch: 5/200 Iteration: 490 Train loss: 1.167250 Train acc: 0.204590\n",
      "Epoch: 5/200 Iteration: 495 Train loss: 1.104694 Train acc: 0.284668\n",
      "Epoch: 5/200 Iteration: 500 Train loss: 1.068359 Train acc: 0.450195\n",
      "Epoch: 5/200 Iteration: 505 Train loss: 1.130805 Train acc: 0.347168\n",
      "Epoch: 5/200 Iteration: 510 Train loss: 1.169281 Train acc: 0.310547\n",
      "Epoch: 6/200 Iteration: 515 Train loss: 1.136935 Train acc: 0.238281\n",
      "Epoch: 6/200 Iteration: 520 Train loss: 1.105505 Train acc: 0.282715\n",
      "Epoch: 6/200 Iteration: 525 Train loss: 1.120594 Train acc: 0.348633\n",
      "Epoch: 6/200 Iteration: 530 Train loss: 1.096558 Train acc: 0.410156\n",
      "Epoch: 6/200 Iteration: 535 Train loss: 1.138515 Train acc: 0.312988\n",
      "Epoch: 6/200 Iteration: 540 Train loss: 1.070199 Train acc: 0.467773\n",
      "Epoch: 6/200 Iteration: 545 Train loss: 1.001270 Train acc: 0.579102\n",
      "Epoch: 6/200 Iteration: 550 Train loss: 1.019774 Train acc: 0.584961\n",
      "Epoch: 6/200 Iteration: 555 Train loss: 1.110912 Train acc: 0.278320\n",
      "Epoch: 6/200 Iteration: 560 Train loss: 1.006067 Train acc: 0.631836\n",
      "Epoch: 6/200 Iteration: 565 Train loss: 1.063696 Train acc: 0.494629\n",
      "Epoch: 6/200 Iteration: 570 Train loss: 1.090428 Train acc: 0.400879\n",
      "Epoch: 6/200 Iteration: 575 Train loss: 1.146169 Train acc: 0.204590\n",
      "Epoch: 6/200 Iteration: 580 Train loss: 1.102925 Train acc: 0.264160\n",
      "Epoch: 6/200 Iteration: 585 Train loss: 1.050795 Train acc: 0.453613\n",
      "Epoch: 6/200 Iteration: 590 Train loss: 1.170550 Train acc: 0.313477\n",
      "Epoch: 6/200 Iteration: 595 Train loss: 1.190806 Train acc: 0.297363\n",
      "Epoch: 7/200 Iteration: 600 Train loss: 1.113912 Train acc: 0.282227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/200 Iteration: 605 Train loss: 1.094045 Train acc: 0.406738\n",
      "Epoch: 7/200 Iteration: 610 Train loss: 1.140905 Train acc: 0.348633\n",
      "Epoch: 7/200 Iteration: 615 Train loss: 1.095391 Train acc: 0.410156\n",
      "Epoch: 7/200 Iteration: 620 Train loss: 1.137013 Train acc: 0.312988\n",
      "Epoch: 7/200 Iteration: 625 Train loss: 1.064102 Train acc: 0.467773\n",
      "Epoch: 7/200 Iteration: 630 Train loss: 1.000078 Train acc: 0.579102\n",
      "Epoch: 7/200 Iteration: 635 Train loss: 1.007759 Train acc: 0.584961\n",
      "Epoch: 7/200 Iteration: 640 Train loss: 1.112316 Train acc: 0.278320\n",
      "Epoch: 7/200 Iteration: 645 Train loss: 0.988617 Train acc: 0.631836\n",
      "Epoch: 7/200 Iteration: 650 Train loss: 1.062258 Train acc: 0.494629\n",
      "Epoch: 7/200 Iteration: 655 Train loss: 1.092666 Train acc: 0.400391\n",
      "Epoch: 7/200 Iteration: 660 Train loss: 1.137960 Train acc: 0.204590\n",
      "Epoch: 7/200 Iteration: 665 Train loss: 1.099841 Train acc: 0.312988\n",
      "Epoch: 7/200 Iteration: 670 Train loss: 1.035450 Train acc: 0.465332\n",
      "Epoch: 7/200 Iteration: 675 Train loss: 1.181838 Train acc: 0.310059\n",
      "Epoch: 7/200 Iteration: 680 Train loss: 1.204357 Train acc: 0.288086\n",
      "Epoch: 8/200 Iteration: 685 Train loss: 1.102055 Train acc: 0.296875\n",
      "Epoch: 8/200 Iteration: 690 Train loss: 1.092524 Train acc: 0.430176\n",
      "Epoch: 8/200 Iteration: 695 Train loss: 1.160575 Train acc: 0.348633\n",
      "Epoch: 8/200 Iteration: 700 Train loss: 1.097499 Train acc: 0.410156\n",
      "Epoch: 8/200 Iteration: 705 Train loss: 1.127615 Train acc: 0.312988\n",
      "Epoch: 8/200 Iteration: 710 Train loss: 1.059818 Train acc: 0.467773\n",
      "Epoch: 8/200 Iteration: 715 Train loss: 1.006230 Train acc: 0.579102\n",
      "Epoch: 8/200 Iteration: 720 Train loss: 1.011941 Train acc: 0.584961\n",
      "Epoch: 8/200 Iteration: 725 Train loss: 1.106431 Train acc: 0.277344\n",
      "Epoch: 8/200 Iteration: 730 Train loss: 0.992875 Train acc: 0.631836\n",
      "Epoch: 8/200 Iteration: 735 Train loss: 1.067097 Train acc: 0.494629\n",
      "Epoch: 8/200 Iteration: 740 Train loss: 1.095869 Train acc: 0.394043\n",
      "Epoch: 8/200 Iteration: 745 Train loss: 1.120634 Train acc: 0.208984\n",
      "Epoch: 8/200 Iteration: 750 Train loss: 1.097405 Train acc: 0.329102\n",
      "Epoch: 8/200 Iteration: 755 Train loss: 1.047434 Train acc: 0.444336\n",
      "Epoch: 8/200 Iteration: 760 Train loss: 1.164133 Train acc: 0.336914\n",
      "Epoch: 8/200 Iteration: 765 Train loss: 1.186502 Train acc: 0.279785\n",
      "Epoch: 9/200 Iteration: 770 Train loss: 1.085876 Train acc: 0.452637\n",
      "Epoch: 9/200 Iteration: 775 Train loss: 1.092721 Train acc: 0.419922\n",
      "Epoch: 9/200 Iteration: 780 Train loss: 1.165625 Train acc: 0.348633\n",
      "Epoch: 9/200 Iteration: 785 Train loss: 1.101135 Train acc: 0.410156\n",
      "Epoch: 9/200 Iteration: 790 Train loss: 1.128544 Train acc: 0.312988\n",
      "Epoch: 9/200 Iteration: 795 Train loss: 1.061809 Train acc: 0.467773\n",
      "Epoch: 9/200 Iteration: 800 Train loss: 1.003504 Train acc: 0.579102\n",
      "Epoch: 9/200 Iteration: 805 Train loss: 1.012977 Train acc: 0.584961\n",
      "Epoch: 9/200 Iteration: 810 Train loss: 1.104822 Train acc: 0.290039\n",
      "Epoch: 9/200 Iteration: 815 Train loss: 0.994797 Train acc: 0.631836\n",
      "Epoch: 9/200 Iteration: 820 Train loss: 1.067988 Train acc: 0.494629\n",
      "Epoch: 9/200 Iteration: 825 Train loss: 1.099512 Train acc: 0.355469\n",
      "Epoch: 9/200 Iteration: 830 Train loss: 1.118850 Train acc: 0.231445\n",
      "Epoch: 9/200 Iteration: 835 Train loss: 1.100345 Train acc: 0.291504\n",
      "Epoch: 9/200 Iteration: 840 Train loss: 1.035405 Train acc: 0.458984\n",
      "Epoch: 9/200 Iteration: 845 Train loss: 1.177428 Train acc: 0.326172\n",
      "Epoch: 9/200 Iteration: 850 Train loss: 1.193139 Train acc: 0.271973\n",
      "Epoch: 10/200 Iteration: 855 Train loss: 1.074294 Train acc: 0.501465\n",
      "Epoch: 10/200 Iteration: 860 Train loss: 1.091920 Train acc: 0.402344\n",
      "Epoch: 10/200 Iteration: 865 Train loss: 1.153903 Train acc: 0.348633\n",
      "Epoch: 10/200 Iteration: 870 Train loss: 1.096972 Train acc: 0.410156\n",
      "Epoch: 10/200 Iteration: 875 Train loss: 1.136224 Train acc: 0.312988\n",
      "Epoch: 10/200 Iteration: 880 Train loss: 1.058153 Train acc: 0.467773\n",
      "Epoch: 10/200 Iteration: 885 Train loss: 0.998105 Train acc: 0.579102\n",
      "Epoch: 10/200 Iteration: 890 Train loss: 1.001482 Train acc: 0.584961\n",
      "Epoch: 10/200 Iteration: 895 Train loss: 1.105910 Train acc: 0.284180\n",
      "Epoch: 10/200 Iteration: 900 Train loss: 0.981444 Train acc: 0.631836\n",
      "Epoch: 10/200 Iteration: 905 Train loss: 1.067769 Train acc: 0.494629\n",
      "Epoch: 10/200 Iteration: 910 Train loss: 1.102148 Train acc: 0.352051\n",
      "Epoch: 10/200 Iteration: 915 Train loss: 1.111999 Train acc: 0.275391\n",
      "Epoch: 10/200 Iteration: 920 Train loss: 1.096325 Train acc: 0.304688\n",
      "Epoch: 10/200 Iteration: 925 Train loss: 1.034245 Train acc: 0.435059\n",
      "Epoch: 10/200 Iteration: 930 Train loss: 1.175837 Train acc: 0.324219\n",
      "Epoch: 10/200 Iteration: 935 Train loss: 1.190938 Train acc: 0.270508\n",
      "Epoch: 11/200 Iteration: 940 Train loss: 1.063900 Train acc: 0.503418\n",
      "Epoch: 11/200 Iteration: 945 Train loss: 1.092536 Train acc: 0.402832\n",
      "Epoch: 11/200 Iteration: 950 Train loss: 1.156453 Train acc: 0.348633\n",
      "Epoch: 11/200 Iteration: 955 Train loss: 1.100201 Train acc: 0.410156\n",
      "Epoch: 11/200 Iteration: 960 Train loss: 1.133884 Train acc: 0.312988\n",
      "Epoch: 11/200 Iteration: 965 Train loss: 1.062093 Train acc: 0.467773\n",
      "Epoch: 11/200 Iteration: 970 Train loss: 1.001224 Train acc: 0.579102\n",
      "Epoch: 11/200 Iteration: 975 Train loss: 1.010688 Train acc: 0.584961\n",
      "Epoch: 11/200 Iteration: 980 Train loss: 1.104091 Train acc: 0.291992\n",
      "Epoch: 11/200 Iteration: 985 Train loss: 0.998731 Train acc: 0.631836\n",
      "Epoch: 11/200 Iteration: 990 Train loss: 1.069681 Train acc: 0.494629\n",
      "Epoch: 11/200 Iteration: 995 Train loss: 1.104588 Train acc: 0.347168\n",
      "Epoch: 11/200 Iteration: 1000 Train loss: 1.106680 Train acc: 0.276367\n",
      "Epoch: 11/200 Iteration: 1005 Train loss: 1.097887 Train acc: 0.294434\n",
      "Epoch: 11/200 Iteration: 1010 Train loss: 1.039086 Train acc: 0.460938\n",
      "Epoch: 11/200 Iteration: 1015 Train loss: 1.168703 Train acc: 0.317383\n",
      "Epoch: 11/200 Iteration: 1020 Train loss: 1.183334 Train acc: 0.252441\n",
      "Epoch: 12/200 Iteration: 1025 Train loss: 1.062765 Train acc: 0.503418\n",
      "Epoch: 12/200 Iteration: 1030 Train loss: 1.089830 Train acc: 0.399902\n",
      "Epoch: 12/200 Iteration: 1035 Train loss: 1.144400 Train acc: 0.348633\n",
      "Epoch: 12/200 Iteration: 1040 Train loss: 1.095683 Train acc: 0.410156\n",
      "Epoch: 12/200 Iteration: 1045 Train loss: 1.140007 Train acc: 0.312988\n",
      "Epoch: 12/200 Iteration: 1050 Train loss: 1.059435 Train acc: 0.467773\n",
      "Epoch: 12/200 Iteration: 1055 Train loss: 1.001137 Train acc: 0.579102\n",
      "Epoch: 12/200 Iteration: 1060 Train loss: 1.000899 Train acc: 0.584961\n",
      "Epoch: 12/200 Iteration: 1065 Train loss: 1.110212 Train acc: 0.278809\n",
      "Epoch: 12/200 Iteration: 1070 Train loss: 0.979507 Train acc: 0.631836\n",
      "Epoch: 12/200 Iteration: 1075 Train loss: 1.067147 Train acc: 0.494629\n",
      "Epoch: 12/200 Iteration: 1080 Train loss: 1.108138 Train acc: 0.314453\n",
      "Epoch: 12/200 Iteration: 1085 Train loss: 1.105050 Train acc: 0.311523\n",
      "Epoch: 12/200 Iteration: 1090 Train loss: 1.097651 Train acc: 0.331055\n",
      "Epoch: 12/200 Iteration: 1095 Train loss: 1.026055 Train acc: 0.445801\n",
      "Epoch: 12/200 Iteration: 1100 Train loss: 1.185158 Train acc: 0.323730\n",
      "Epoch: 12/200 Iteration: 1105 Train loss: 1.194945 Train acc: 0.258301\n",
      "Epoch: 13/200 Iteration: 1110 Train loss: 1.058018 Train acc: 0.503418\n",
      "Epoch: 13/200 Iteration: 1115 Train loss: 1.091279 Train acc: 0.400391\n",
      "Epoch: 13/200 Iteration: 1120 Train loss: 1.148763 Train acc: 0.348633\n",
      "Epoch: 13/200 Iteration: 1125 Train loss: 1.096920 Train acc: 0.410156\n",
      "Epoch: 13/200 Iteration: 1130 Train loss: 1.138288 Train acc: 0.312988\n",
      "Epoch: 13/200 Iteration: 1135 Train loss: 1.060442 Train acc: 0.467773\n",
      "Epoch: 13/200 Iteration: 1140 Train loss: 1.002486 Train acc: 0.579102\n",
      "Epoch: 13/200 Iteration: 1145 Train loss: 1.004503 Train acc: 0.584961\n",
      "Epoch: 13/200 Iteration: 1150 Train loss: 1.109031 Train acc: 0.278320\n",
      "Epoch: 13/200 Iteration: 1155 Train loss: 0.987652 Train acc: 0.631836\n",
      "Epoch: 13/200 Iteration: 1160 Train loss: 1.066485 Train acc: 0.494629\n",
      "Epoch: 13/200 Iteration: 1165 Train loss: 1.107688 Train acc: 0.315430\n",
      "Epoch: 13/200 Iteration: 1170 Train loss: 1.101173 Train acc: 0.327637\n",
      "Epoch: 13/200 Iteration: 1175 Train loss: 1.097106 Train acc: 0.324219\n",
      "Epoch: 13/200 Iteration: 1180 Train loss: 1.033805 Train acc: 0.431152\n",
      "Epoch: 13/200 Iteration: 1185 Train loss: 1.174018 Train acc: 0.326660\n",
      "Epoch: 13/200 Iteration: 1190 Train loss: 1.186291 Train acc: 0.268555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/200 Iteration: 1195 Train loss: 1.056727 Train acc: 0.503418\n",
      "Epoch: 14/200 Iteration: 1200 Train loss: 1.090792 Train acc: 0.399902\n",
      "Epoch: 14/200 Iteration: 1205 Train loss: 1.143493 Train acc: 0.348633\n",
      "Epoch: 14/200 Iteration: 1210 Train loss: 1.095486 Train acc: 0.410156\n",
      "Epoch: 14/200 Iteration: 1215 Train loss: 1.141695 Train acc: 0.312988\n",
      "Epoch: 14/200 Iteration: 1220 Train loss: 1.060116 Train acc: 0.467773\n",
      "Epoch: 14/200 Iteration: 1225 Train loss: 0.999934 Train acc: 0.579102\n",
      "Epoch: 14/200 Iteration: 1230 Train loss: 1.000967 Train acc: 0.584961\n",
      "Epoch: 14/200 Iteration: 1235 Train loss: 1.113355 Train acc: 0.278320\n",
      "Epoch: 14/200 Iteration: 1240 Train loss: 0.980844 Train acc: 0.631836\n",
      "Epoch: 14/200 Iteration: 1245 Train loss: 1.065757 Train acc: 0.494629\n",
      "Epoch: 14/200 Iteration: 1250 Train loss: 1.112788 Train acc: 0.290039\n",
      "Epoch: 14/200 Iteration: 1255 Train loss: 1.099602 Train acc: 0.346680\n",
      "Epoch: 14/200 Iteration: 1260 Train loss: 1.096134 Train acc: 0.352051\n",
      "Epoch: 14/200 Iteration: 1265 Train loss: 1.024950 Train acc: 0.454102\n",
      "Epoch: 14/200 Iteration: 1270 Train loss: 1.184575 Train acc: 0.332031\n",
      "Epoch: 14/200 Iteration: 1275 Train loss: 1.194885 Train acc: 0.265625\n",
      "Epoch: 15/200 Iteration: 1280 Train loss: 1.058629 Train acc: 0.503418\n",
      "Epoch: 15/200 Iteration: 1285 Train loss: 1.092802 Train acc: 0.399902\n",
      "Epoch: 15/200 Iteration: 1290 Train loss: 1.144801 Train acc: 0.348633\n",
      "Epoch: 15/200 Iteration: 1295 Train loss: 1.095613 Train acc: 0.410156\n",
      "Epoch: 15/200 Iteration: 1300 Train loss: 1.140797 Train acc: 0.312988\n",
      "Epoch: 15/200 Iteration: 1305 Train loss: 1.061506 Train acc: 0.467773\n",
      "Epoch: 15/200 Iteration: 1310 Train loss: 1.000684 Train acc: 0.579102\n",
      "Epoch: 15/200 Iteration: 1315 Train loss: 1.002167 Train acc: 0.584961\n",
      "Epoch: 15/200 Iteration: 1320 Train loss: 1.113583 Train acc: 0.278320\n",
      "Epoch: 15/200 Iteration: 1325 Train loss: 0.983407 Train acc: 0.631836\n",
      "Epoch: 15/200 Iteration: 1330 Train loss: 1.064552 Train acc: 0.494629\n",
      "Epoch: 15/200 Iteration: 1335 Train loss: 1.111517 Train acc: 0.308594\n",
      "Epoch: 15/200 Iteration: 1340 Train loss: 1.097855 Train acc: 0.351074\n",
      "Epoch: 15/200 Iteration: 1345 Train loss: 1.097854 Train acc: 0.325195\n",
      "Epoch: 15/200 Iteration: 1350 Train loss: 1.028495 Train acc: 0.440430\n",
      "Epoch: 15/200 Iteration: 1355 Train loss: 1.174613 Train acc: 0.321777\n",
      "Epoch: 15/200 Iteration: 1360 Train loss: 1.185396 Train acc: 0.262695\n",
      "Epoch: 16/200 Iteration: 1365 Train loss: 1.055563 Train acc: 0.503418\n",
      "Epoch: 16/200 Iteration: 1370 Train loss: 1.091971 Train acc: 0.399902\n",
      "Epoch: 16/200 Iteration: 1375 Train loss: 1.142541 Train acc: 0.348633\n",
      "Epoch: 16/200 Iteration: 1380 Train loss: 1.094593 Train acc: 0.410156\n",
      "Epoch: 16/200 Iteration: 1385 Train loss: 1.140723 Train acc: 0.312988\n",
      "Epoch: 16/200 Iteration: 1390 Train loss: 1.058163 Train acc: 0.467773\n",
      "Epoch: 16/200 Iteration: 1395 Train loss: 0.999657 Train acc: 0.579102\n",
      "Epoch: 16/200 Iteration: 1400 Train loss: 0.999479 Train acc: 0.584961\n",
      "Epoch: 16/200 Iteration: 1405 Train loss: 1.116622 Train acc: 0.278320\n",
      "Epoch: 16/200 Iteration: 1410 Train loss: 0.979261 Train acc: 0.631836\n",
      "Epoch: 16/200 Iteration: 1415 Train loss: 1.064466 Train acc: 0.494629\n",
      "Epoch: 16/200 Iteration: 1420 Train loss: 1.111532 Train acc: 0.311035\n",
      "Epoch: 16/200 Iteration: 1425 Train loss: 1.098118 Train acc: 0.366699\n",
      "Epoch: 16/200 Iteration: 1430 Train loss: 1.096728 Train acc: 0.364258\n",
      "Epoch: 16/200 Iteration: 1435 Train loss: 1.025248 Train acc: 0.434570\n",
      "Epoch: 16/200 Iteration: 1440 Train loss: 1.181021 Train acc: 0.312012\n",
      "Epoch: 16/200 Iteration: 1445 Train loss: 1.195506 Train acc: 0.246582\n",
      "Epoch: 17/200 Iteration: 1450 Train loss: 1.057701 Train acc: 0.503418\n",
      "Epoch: 17/200 Iteration: 1455 Train loss: 1.091428 Train acc: 0.399902\n",
      "Epoch: 17/200 Iteration: 1460 Train loss: 1.144484 Train acc: 0.348633\n",
      "Epoch: 17/200 Iteration: 1465 Train loss: 1.095148 Train acc: 0.410156\n",
      "Epoch: 17/200 Iteration: 1470 Train loss: 1.140424 Train acc: 0.312988\n",
      "Epoch: 17/200 Iteration: 1475 Train loss: 1.060126 Train acc: 0.467773\n",
      "Epoch: 17/200 Iteration: 1480 Train loss: 0.999988 Train acc: 0.579102\n",
      "Epoch: 17/200 Iteration: 1485 Train loss: 1.001198 Train acc: 0.584961\n",
      "Epoch: 17/200 Iteration: 1490 Train loss: 1.116319 Train acc: 0.278320\n",
      "Epoch: 17/200 Iteration: 1495 Train loss: 0.984623 Train acc: 0.631836\n",
      "Epoch: 17/200 Iteration: 1500 Train loss: 1.063140 Train acc: 0.494629\n",
      "Epoch: 17/200 Iteration: 1505 Train loss: 1.112755 Train acc: 0.297363\n",
      "Epoch: 17/200 Iteration: 1510 Train loss: 1.096621 Train acc: 0.361816\n",
      "Epoch: 17/200 Iteration: 1515 Train loss: 1.094197 Train acc: 0.370117\n",
      "Epoch: 17/200 Iteration: 1520 Train loss: 1.025926 Train acc: 0.436523\n",
      "Epoch: 17/200 Iteration: 1525 Train loss: 1.175633 Train acc: 0.332520\n",
      "Epoch: 17/200 Iteration: 1530 Train loss: 1.191752 Train acc: 0.258301\n",
      "Epoch: 18/200 Iteration: 1535 Train loss: 1.056930 Train acc: 0.503418\n",
      "Epoch: 18/200 Iteration: 1540 Train loss: 1.092766 Train acc: 0.399902\n",
      "Epoch: 18/200 Iteration: 1545 Train loss: 1.140809 Train acc: 0.348633\n",
      "Epoch: 18/200 Iteration: 1550 Train loss: 1.094921 Train acc: 0.410156\n",
      "Epoch: 18/200 Iteration: 1555 Train loss: 1.143191 Train acc: 0.312988\n",
      "Epoch: 18/200 Iteration: 1560 Train loss: 1.058230 Train acc: 0.467773\n",
      "Epoch: 18/200 Iteration: 1565 Train loss: 1.000546 Train acc: 0.579102\n",
      "Epoch: 18/200 Iteration: 1570 Train loss: 0.998936 Train acc: 0.584961\n",
      "Epoch: 18/200 Iteration: 1575 Train loss: 1.118315 Train acc: 0.278320\n",
      "Epoch: 18/200 Iteration: 1580 Train loss: 0.979120 Train acc: 0.631836\n",
      "Epoch: 18/200 Iteration: 1585 Train loss: 1.063133 Train acc: 0.494629\n",
      "Epoch: 18/200 Iteration: 1590 Train loss: 1.112378 Train acc: 0.281738\n",
      "Epoch: 18/200 Iteration: 1595 Train loss: 1.097425 Train acc: 0.374023\n",
      "Epoch: 18/200 Iteration: 1600 Train loss: 1.095158 Train acc: 0.370117\n",
      "Epoch: 18/200 Iteration: 1605 Train loss: 1.023931 Train acc: 0.436523\n",
      "Epoch: 18/200 Iteration: 1610 Train loss: 1.179106 Train acc: 0.331055\n",
      "Epoch: 18/200 Iteration: 1615 Train loss: 1.189117 Train acc: 0.250488\n",
      "Epoch: 19/200 Iteration: 1620 Train loss: 1.055546 Train acc: 0.503418\n",
      "Epoch: 19/200 Iteration: 1625 Train loss: 1.091850 Train acc: 0.399902\n",
      "Epoch: 19/200 Iteration: 1630 Train loss: 1.141612 Train acc: 0.348633\n",
      "Epoch: 19/200 Iteration: 1635 Train loss: 1.093433 Train acc: 0.410156\n",
      "Epoch: 19/200 Iteration: 1640 Train loss: 1.142537 Train acc: 0.312988\n",
      "Epoch: 19/200 Iteration: 1645 Train loss: 1.061181 Train acc: 0.467773\n",
      "Epoch: 19/200 Iteration: 1650 Train loss: 1.000616 Train acc: 0.579102\n",
      "Epoch: 19/200 Iteration: 1655 Train loss: 1.001537 Train acc: 0.584961\n",
      "Epoch: 19/200 Iteration: 1660 Train loss: 1.118915 Train acc: 0.278320\n",
      "Epoch: 19/200 Iteration: 1665 Train loss: 0.980906 Train acc: 0.631836\n",
      "Epoch: 19/200 Iteration: 1670 Train loss: 1.061341 Train acc: 0.494629\n",
      "Epoch: 19/200 Iteration: 1675 Train loss: 1.113368 Train acc: 0.276855\n",
      "Epoch: 19/200 Iteration: 1680 Train loss: 1.094949 Train acc: 0.390625\n",
      "Epoch: 19/200 Iteration: 1685 Train loss: 1.097244 Train acc: 0.371582\n",
      "Epoch: 19/200 Iteration: 1690 Train loss: 1.018717 Train acc: 0.452637\n",
      "Epoch: 19/200 Iteration: 1695 Train loss: 1.178834 Train acc: 0.317871\n",
      "Epoch: 19/200 Iteration: 1700 Train loss: 1.194573 Train acc: 0.236328\n",
      "Epoch: 20/200 Iteration: 1705 Train loss: 1.055485 Train acc: 0.503418\n",
      "Epoch: 20/200 Iteration: 1710 Train loss: 1.093152 Train acc: 0.399902\n",
      "Epoch: 20/200 Iteration: 1715 Train loss: 1.139774 Train acc: 0.348633\n",
      "Epoch: 20/200 Iteration: 1720 Train loss: 1.094642 Train acc: 0.410156\n",
      "Epoch: 20/200 Iteration: 1725 Train loss: 1.140667 Train acc: 0.312988\n",
      "Epoch: 20/200 Iteration: 1730 Train loss: 1.057065 Train acc: 0.467773\n",
      "Epoch: 20/200 Iteration: 1735 Train loss: 1.000682 Train acc: 0.579102\n",
      "Epoch: 20/200 Iteration: 1740 Train loss: 0.999166 Train acc: 0.584961\n",
      "Epoch: 20/200 Iteration: 1745 Train loss: 1.120261 Train acc: 0.278320\n",
      "Epoch: 20/200 Iteration: 1750 Train loss: 0.979326 Train acc: 0.631836\n",
      "Epoch: 20/200 Iteration: 1755 Train loss: 1.061862 Train acc: 0.494629\n",
      "Epoch: 20/200 Iteration: 1760 Train loss: 1.114861 Train acc: 0.257812\n",
      "Epoch: 20/200 Iteration: 1765 Train loss: 1.092987 Train acc: 0.426758\n",
      "Epoch: 20/200 Iteration: 1770 Train loss: 1.090168 Train acc: 0.440918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/200 Iteration: 1775 Train loss: 1.026903 Train acc: 0.416504\n",
      "Epoch: 20/200 Iteration: 1780 Train loss: 1.175847 Train acc: 0.327637\n",
      "Epoch: 20/200 Iteration: 1785 Train loss: 1.194475 Train acc: 0.242676\n",
      "Epoch: 21/200 Iteration: 1790 Train loss: 1.055983 Train acc: 0.503418\n",
      "Epoch: 21/200 Iteration: 1795 Train loss: 1.092070 Train acc: 0.399902\n",
      "Epoch: 21/200 Iteration: 1800 Train loss: 1.141630 Train acc: 0.348633\n",
      "Epoch: 21/200 Iteration: 1805 Train loss: 1.092843 Train acc: 0.410156\n",
      "Epoch: 21/200 Iteration: 1810 Train loss: 1.142909 Train acc: 0.312988\n",
      "Epoch: 21/200 Iteration: 1815 Train loss: 1.059895 Train acc: 0.467773\n",
      "Epoch: 21/200 Iteration: 1820 Train loss: 1.000083 Train acc: 0.579102\n",
      "Epoch: 21/200 Iteration: 1825 Train loss: 1.001472 Train acc: 0.584961\n",
      "Epoch: 21/200 Iteration: 1830 Train loss: 1.119508 Train acc: 0.278320\n",
      "Epoch: 21/200 Iteration: 1835 Train loss: 0.981665 Train acc: 0.631836\n",
      "Epoch: 21/200 Iteration: 1840 Train loss: 1.061840 Train acc: 0.494629\n",
      "Epoch: 21/200 Iteration: 1845 Train loss: 1.116887 Train acc: 0.263184\n",
      "Epoch: 21/200 Iteration: 1850 Train loss: 1.095003 Train acc: 0.362793\n",
      "Epoch: 21/200 Iteration: 1855 Train loss: 1.098474 Train acc: 0.359375\n",
      "Epoch: 21/200 Iteration: 1860 Train loss: 1.020458 Train acc: 0.434570\n",
      "Epoch: 21/200 Iteration: 1865 Train loss: 1.179573 Train acc: 0.336914\n",
      "Epoch: 21/200 Iteration: 1870 Train loss: 1.187364 Train acc: 0.240723\n",
      "Epoch: 22/200 Iteration: 1875 Train loss: 1.054512 Train acc: 0.503418\n",
      "Epoch: 22/200 Iteration: 1880 Train loss: 1.093310 Train acc: 0.399902\n",
      "Epoch: 22/200 Iteration: 1885 Train loss: 1.139068 Train acc: 0.348633\n",
      "Epoch: 22/200 Iteration: 1890 Train loss: 1.093184 Train acc: 0.410156\n",
      "Epoch: 22/200 Iteration: 1895 Train loss: 1.143368 Train acc: 0.312988\n",
      "Epoch: 22/200 Iteration: 1900 Train loss: 1.058972 Train acc: 0.467773\n",
      "Epoch: 22/200 Iteration: 1905 Train loss: 1.000993 Train acc: 0.579102\n",
      "Epoch: 22/200 Iteration: 1910 Train loss: 1.000731 Train acc: 0.584961\n",
      "Epoch: 22/200 Iteration: 1915 Train loss: 1.123663 Train acc: 0.278320\n",
      "Epoch: 22/200 Iteration: 1920 Train loss: 0.980613 Train acc: 0.631836\n",
      "Epoch: 22/200 Iteration: 1925 Train loss: 1.059541 Train acc: 0.494629\n",
      "Epoch: 22/200 Iteration: 1930 Train loss: 1.115476 Train acc: 0.257324\n",
      "Epoch: 22/200 Iteration: 1935 Train loss: 1.090649 Train acc: 0.423828\n",
      "Epoch: 22/200 Iteration: 1940 Train loss: 1.095408 Train acc: 0.434570\n",
      "Epoch: 22/200 Iteration: 1945 Train loss: 1.020797 Train acc: 0.438965\n",
      "Epoch: 22/200 Iteration: 1950 Train loss: 1.180242 Train acc: 0.318359\n",
      "Epoch: 22/200 Iteration: 1955 Train loss: 1.195435 Train acc: 0.238281\n",
      "Epoch: 23/200 Iteration: 1960 Train loss: 1.055063 Train acc: 0.503418\n",
      "Epoch: 23/200 Iteration: 1965 Train loss: 1.092821 Train acc: 0.399902\n",
      "Epoch: 23/200 Iteration: 1970 Train loss: 1.138855 Train acc: 0.348633\n",
      "Epoch: 23/200 Iteration: 1975 Train loss: 1.092655 Train acc: 0.410156\n",
      "Epoch: 23/200 Iteration: 1980 Train loss: 1.143390 Train acc: 0.312988\n",
      "Epoch: 23/200 Iteration: 1985 Train loss: 1.059591 Train acc: 0.467773\n",
      "Epoch: 23/200 Iteration: 1990 Train loss: 0.998341 Train acc: 0.579102\n",
      "Epoch: 23/200 Iteration: 1995 Train loss: 0.997110 Train acc: 0.584961\n",
      "Epoch: 23/200 Iteration: 2000 Train loss: 1.123699 Train acc: 0.278320\n",
      "Epoch: 23/200 Iteration: 2005 Train loss: 0.975602 Train acc: 0.631836\n",
      "Epoch: 23/200 Iteration: 2010 Train loss: 1.061425 Train acc: 0.494629\n",
      "Epoch: 23/200 Iteration: 2015 Train loss: 1.116631 Train acc: 0.258789\n",
      "Epoch: 23/200 Iteration: 2020 Train loss: 1.091686 Train acc: 0.422363\n",
      "Epoch: 23/200 Iteration: 2025 Train loss: 1.093505 Train acc: 0.414062\n",
      "Epoch: 23/200 Iteration: 2030 Train loss: 1.021756 Train acc: 0.433594\n",
      "Epoch: 23/200 Iteration: 2035 Train loss: 1.173793 Train acc: 0.333984\n",
      "Epoch: 23/200 Iteration: 2040 Train loss: 1.195725 Train acc: 0.233887\n",
      "Epoch: 24/200 Iteration: 2045 Train loss: 1.055285 Train acc: 0.503418\n",
      "Epoch: 24/200 Iteration: 2050 Train loss: 1.092466 Train acc: 0.399902\n",
      "Epoch: 24/200 Iteration: 2055 Train loss: 1.141796 Train acc: 0.348633\n",
      "Epoch: 24/200 Iteration: 2060 Train loss: 1.093141 Train acc: 0.410156\n",
      "Epoch: 24/200 Iteration: 2065 Train loss: 1.142966 Train acc: 0.312988\n",
      "Epoch: 24/200 Iteration: 2070 Train loss: 1.062135 Train acc: 0.467773\n",
      "Epoch: 24/200 Iteration: 2075 Train loss: 1.002612 Train acc: 0.579102\n",
      "Epoch: 24/200 Iteration: 2080 Train loss: 1.001569 Train acc: 0.584961\n",
      "Epoch: 24/200 Iteration: 2085 Train loss: 1.121618 Train acc: 0.278320\n",
      "Epoch: 24/200 Iteration: 2090 Train loss: 0.981404 Train acc: 0.631836\n",
      "Epoch: 24/200 Iteration: 2095 Train loss: 1.060840 Train acc: 0.494629\n",
      "Epoch: 24/200 Iteration: 2100 Train loss: 1.117337 Train acc: 0.272461\n",
      "Epoch: 24/200 Iteration: 2105 Train loss: 1.092636 Train acc: 0.402832\n",
      "Epoch: 24/200 Iteration: 2110 Train loss: 1.097510 Train acc: 0.389160\n",
      "Epoch: 24/200 Iteration: 2115 Train loss: 1.019220 Train acc: 0.441895\n",
      "Epoch: 24/200 Iteration: 2120 Train loss: 1.181071 Train acc: 0.333496\n",
      "Epoch: 24/200 Iteration: 2125 Train loss: 1.190036 Train acc: 0.231934\n",
      "Epoch: 25/200 Iteration: 2130 Train loss: 1.053370 Train acc: 0.503418\n",
      "Epoch: 25/200 Iteration: 2135 Train loss: 1.092993 Train acc: 0.399902\n",
      "Epoch: 25/200 Iteration: 2140 Train loss: 1.139128 Train acc: 0.348633\n",
      "Epoch: 25/200 Iteration: 2145 Train loss: 1.092670 Train acc: 0.410156\n",
      "Epoch: 25/200 Iteration: 2150 Train loss: 1.143549 Train acc: 0.312988\n",
      "Epoch: 25/200 Iteration: 2155 Train loss: 1.059628 Train acc: 0.467773\n",
      "Epoch: 25/200 Iteration: 2160 Train loss: 1.000721 Train acc: 0.579102\n",
      "Epoch: 25/200 Iteration: 2165 Train loss: 0.999391 Train acc: 0.584961\n",
      "Epoch: 25/200 Iteration: 2170 Train loss: 1.125406 Train acc: 0.278320\n",
      "Epoch: 25/200 Iteration: 2175 Train loss: 0.982250 Train acc: 0.631836\n",
      "Epoch: 25/200 Iteration: 2180 Train loss: 1.059154 Train acc: 0.494629\n",
      "Epoch: 25/200 Iteration: 2185 Train loss: 1.112886 Train acc: 0.257324\n",
      "Epoch: 25/200 Iteration: 2190 Train loss: 1.093056 Train acc: 0.431641\n",
      "Epoch: 25/200 Iteration: 2195 Train loss: 1.092964 Train acc: 0.438477\n",
      "Epoch: 25/200 Iteration: 2200 Train loss: 1.021849 Train acc: 0.441406\n",
      "Epoch: 25/200 Iteration: 2205 Train loss: 1.180717 Train acc: 0.328613\n",
      "Epoch: 25/200 Iteration: 2210 Train loss: 1.197895 Train acc: 0.242676\n",
      "Epoch: 26/200 Iteration: 2215 Train loss: 1.057827 Train acc: 0.503418\n",
      "Epoch: 26/200 Iteration: 2220 Train loss: 1.092655 Train acc: 0.399902\n",
      "Epoch: 26/200 Iteration: 2225 Train loss: 1.139340 Train acc: 0.348633\n",
      "Epoch: 26/200 Iteration: 2230 Train loss: 1.093803 Train acc: 0.410156\n",
      "Epoch: 26/200 Iteration: 2235 Train loss: 1.144704 Train acc: 0.312988\n",
      "Epoch: 26/200 Iteration: 2240 Train loss: 1.059098 Train acc: 0.467773\n",
      "Epoch: 26/200 Iteration: 2245 Train loss: 1.001897 Train acc: 0.579102\n",
      "Epoch: 26/200 Iteration: 2250 Train loss: 0.998791 Train acc: 0.584961\n",
      "Epoch: 26/200 Iteration: 2255 Train loss: 1.122469 Train acc: 0.278320\n",
      "Epoch: 26/200 Iteration: 2260 Train loss: 0.980255 Train acc: 0.631836\n",
      "Epoch: 26/200 Iteration: 2265 Train loss: 1.061051 Train acc: 0.494629\n",
      "Epoch: 26/200 Iteration: 2270 Train loss: 1.119665 Train acc: 0.248047\n",
      "Epoch: 26/200 Iteration: 2275 Train loss: 1.087683 Train acc: 0.434082\n",
      "Epoch: 26/200 Iteration: 2280 Train loss: 1.094701 Train acc: 0.415039\n",
      "Epoch: 26/200 Iteration: 2285 Train loss: 1.023361 Train acc: 0.436523\n",
      "Epoch: 26/200 Iteration: 2290 Train loss: 1.175624 Train acc: 0.336914\n",
      "Epoch: 26/200 Iteration: 2295 Train loss: 1.187162 Train acc: 0.233887\n",
      "Epoch: 27/200 Iteration: 2300 Train loss: 1.052072 Train acc: 0.503418\n",
      "Epoch: 27/200 Iteration: 2305 Train loss: 1.093012 Train acc: 0.399902\n",
      "Epoch: 27/200 Iteration: 2310 Train loss: 1.136723 Train acc: 0.348633\n",
      "Epoch: 27/200 Iteration: 2315 Train loss: 1.090564 Train acc: 0.410156\n",
      "Epoch: 27/200 Iteration: 2320 Train loss: 1.143926 Train acc: 0.312988\n",
      "Epoch: 27/200 Iteration: 2325 Train loss: 1.060652 Train acc: 0.467773\n",
      "Epoch: 27/200 Iteration: 2330 Train loss: 1.000973 Train acc: 0.579102\n",
      "Epoch: 27/200 Iteration: 2335 Train loss: 0.998455 Train acc: 0.584961\n",
      "Epoch: 27/200 Iteration: 2340 Train loss: 1.129022 Train acc: 0.278320\n",
      "Epoch: 27/200 Iteration: 2345 Train loss: 0.978170 Train acc: 0.631836\n",
      "Epoch: 27/200 Iteration: 2350 Train loss: 1.057474 Train acc: 0.494629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/200 Iteration: 2355 Train loss: 1.107515 Train acc: 0.269043\n",
      "Epoch: 27/200 Iteration: 2360 Train loss: 1.096212 Train acc: 0.393555\n",
      "Epoch: 27/200 Iteration: 2365 Train loss: 1.096061 Train acc: 0.403320\n",
      "Epoch: 27/200 Iteration: 2370 Train loss: 1.013983 Train acc: 0.449707\n",
      "Epoch: 27/200 Iteration: 2375 Train loss: 1.188657 Train acc: 0.328125\n",
      "Epoch: 27/200 Iteration: 2380 Train loss: 1.197102 Train acc: 0.231445\n",
      "Epoch: 28/200 Iteration: 2385 Train loss: 1.057811 Train acc: 0.503418\n",
      "Epoch: 28/200 Iteration: 2390 Train loss: 1.093196 Train acc: 0.399902\n",
      "Epoch: 28/200 Iteration: 2395 Train loss: 1.140304 Train acc: 0.348633\n",
      "Epoch: 28/200 Iteration: 2400 Train loss: 1.093290 Train acc: 0.410156\n",
      "Epoch: 28/200 Iteration: 2405 Train loss: 1.142109 Train acc: 0.312988\n",
      "Epoch: 28/200 Iteration: 2410 Train loss: 1.060399 Train acc: 0.467773\n",
      "Epoch: 28/200 Iteration: 2415 Train loss: 1.001072 Train acc: 0.579102\n",
      "Epoch: 28/200 Iteration: 2420 Train loss: 0.998836 Train acc: 0.584961\n",
      "Epoch: 28/200 Iteration: 2425 Train loss: 1.126055 Train acc: 0.278320\n",
      "Epoch: 28/200 Iteration: 2430 Train loss: 0.979733 Train acc: 0.631836\n",
      "Epoch: 28/200 Iteration: 2435 Train loss: 1.059324 Train acc: 0.494629\n",
      "Epoch: 28/200 Iteration: 2440 Train loss: 1.119119 Train acc: 0.248047\n",
      "Epoch: 28/200 Iteration: 2445 Train loss: 1.087742 Train acc: 0.436035\n",
      "Epoch: 28/200 Iteration: 2450 Train loss: 1.087939 Train acc: 0.465820\n",
      "Epoch: 28/200 Iteration: 2455 Train loss: 1.023334 Train acc: 0.429199\n",
      "Epoch: 28/200 Iteration: 2460 Train loss: 1.178127 Train acc: 0.328613\n",
      "Epoch: 28/200 Iteration: 2465 Train loss: 1.196112 Train acc: 0.234863\n",
      "Epoch: 29/200 Iteration: 2470 Train loss: 1.055583 Train acc: 0.503418\n",
      "Epoch: 29/200 Iteration: 2475 Train loss: 1.093641 Train acc: 0.399902\n",
      "Epoch: 29/200 Iteration: 2480 Train loss: 1.139001 Train acc: 0.348633\n",
      "Epoch: 29/200 Iteration: 2485 Train loss: 1.093168 Train acc: 0.410156\n",
      "Epoch: 29/200 Iteration: 2490 Train loss: 1.144784 Train acc: 0.312988\n",
      "Epoch: 29/200 Iteration: 2495 Train loss: 1.059535 Train acc: 0.467773\n",
      "Epoch: 29/200 Iteration: 2500 Train loss: 1.000700 Train acc: 0.579102\n",
      "Epoch: 29/200 Iteration: 2505 Train loss: 0.999725 Train acc: 0.584961\n",
      "Epoch: 29/200 Iteration: 2510 Train loss: 1.125137 Train acc: 0.278320\n",
      "Epoch: 29/200 Iteration: 2515 Train loss: 0.979208 Train acc: 0.631836\n",
      "Epoch: 29/200 Iteration: 2520 Train loss: 1.060209 Train acc: 0.494629\n",
      "Epoch: 29/200 Iteration: 2525 Train loss: 1.114802 Train acc: 0.266113\n",
      "Epoch: 29/200 Iteration: 2530 Train loss: 1.101406 Train acc: 0.337402\n",
      "Epoch: 29/200 Iteration: 2535 Train loss: 1.093963 Train acc: 0.396484\n",
      "Epoch: 29/200 Iteration: 2540 Train loss: 1.020592 Train acc: 0.437988\n",
      "Epoch: 29/200 Iteration: 2545 Train loss: 1.178654 Train acc: 0.318359\n",
      "Epoch: 29/200 Iteration: 2550 Train loss: 1.191259 Train acc: 0.233887\n",
      "Epoch: 30/200 Iteration: 2555 Train loss: 1.057037 Train acc: 0.503418\n",
      "Epoch: 30/200 Iteration: 2560 Train loss: 1.092969 Train acc: 0.400391\n",
      "Epoch: 30/200 Iteration: 2565 Train loss: 1.135282 Train acc: 0.348633\n",
      "Epoch: 30/200 Iteration: 2570 Train loss: 1.092781 Train acc: 0.410156\n",
      "Epoch: 30/200 Iteration: 2575 Train loss: 1.141496 Train acc: 0.312988\n",
      "Epoch: 30/200 Iteration: 2580 Train loss: 1.059764 Train acc: 0.467773\n",
      "Epoch: 30/200 Iteration: 2585 Train loss: 1.001431 Train acc: 0.579102\n",
      "Epoch: 30/200 Iteration: 2590 Train loss: 0.998411 Train acc: 0.584961\n",
      "Epoch: 30/200 Iteration: 2595 Train loss: 1.124125 Train acc: 0.278320\n",
      "Epoch: 30/200 Iteration: 2600 Train loss: 0.976995 Train acc: 0.631836\n",
      "Epoch: 30/200 Iteration: 2605 Train loss: 1.058831 Train acc: 0.494629\n",
      "Epoch: 30/200 Iteration: 2610 Train loss: 1.121475 Train acc: 0.257324\n",
      "Epoch: 30/200 Iteration: 2615 Train loss: 1.088233 Train acc: 0.444824\n",
      "Epoch: 30/200 Iteration: 2620 Train loss: 1.094858 Train acc: 0.428223\n",
      "Epoch: 30/200 Iteration: 2625 Train loss: 1.019614 Train acc: 0.440430\n",
      "Epoch: 30/200 Iteration: 2630 Train loss: 1.180984 Train acc: 0.344238\n",
      "Epoch: 30/200 Iteration: 2635 Train loss: 1.193317 Train acc: 0.236328\n",
      "Epoch: 31/200 Iteration: 2640 Train loss: 1.052336 Train acc: 0.503418\n",
      "Epoch: 31/200 Iteration: 2645 Train loss: 1.093139 Train acc: 0.399902\n",
      "Epoch: 31/200 Iteration: 2650 Train loss: 1.138716 Train acc: 0.348633\n",
      "Epoch: 31/200 Iteration: 2655 Train loss: 1.091213 Train acc: 0.410156\n",
      "Epoch: 31/200 Iteration: 2660 Train loss: 1.142022 Train acc: 0.312988\n",
      "Epoch: 31/200 Iteration: 2665 Train loss: 1.060964 Train acc: 0.467773\n",
      "Epoch: 31/200 Iteration: 2670 Train loss: 1.002023 Train acc: 0.579102\n",
      "Epoch: 31/200 Iteration: 2675 Train loss: 0.998734 Train acc: 0.584961\n",
      "Epoch: 31/200 Iteration: 2680 Train loss: 1.126020 Train acc: 0.278320\n",
      "Epoch: 31/200 Iteration: 2685 Train loss: 0.977563 Train acc: 0.631836\n",
      "Epoch: 31/200 Iteration: 2690 Train loss: 1.058877 Train acc: 0.494629\n",
      "Epoch: 31/200 Iteration: 2695 Train loss: 1.118094 Train acc: 0.241211\n",
      "Epoch: 31/200 Iteration: 2700 Train loss: 1.090649 Train acc: 0.438477\n",
      "Epoch: 31/200 Iteration: 2705 Train loss: 1.089881 Train acc: 0.453613\n",
      "Epoch: 31/200 Iteration: 2710 Train loss: 1.020532 Train acc: 0.435547\n",
      "Epoch: 31/200 Iteration: 2715 Train loss: 1.176312 Train acc: 0.347656\n",
      "Epoch: 31/200 Iteration: 2720 Train loss: 1.193305 Train acc: 0.235840\n",
      "Epoch: 32/200 Iteration: 2725 Train loss: 1.051871 Train acc: 0.503418\n",
      "Epoch: 32/200 Iteration: 2730 Train loss: 1.093549 Train acc: 0.399902\n",
      "Epoch: 32/200 Iteration: 2735 Train loss: 1.138469 Train acc: 0.348633\n",
      "Epoch: 32/200 Iteration: 2740 Train loss: 1.091783 Train acc: 0.410156\n",
      "Epoch: 32/200 Iteration: 2745 Train loss: 1.144190 Train acc: 0.312988\n",
      "Epoch: 32/200 Iteration: 2750 Train loss: 1.060005 Train acc: 0.467773\n",
      "Epoch: 32/200 Iteration: 2755 Train loss: 1.002653 Train acc: 0.579102\n",
      "Epoch: 32/200 Iteration: 2760 Train loss: 0.999262 Train acc: 0.584961\n",
      "Epoch: 32/200 Iteration: 2765 Train loss: 1.127476 Train acc: 0.278320\n",
      "Epoch: 32/200 Iteration: 2770 Train loss: 0.979409 Train acc: 0.631836\n",
      "Epoch: 32/200 Iteration: 2775 Train loss: 1.057816 Train acc: 0.494629\n",
      "Epoch: 32/200 Iteration: 2780 Train loss: 1.114032 Train acc: 0.251953\n",
      "Epoch: 32/200 Iteration: 2785 Train loss: 1.091447 Train acc: 0.426270\n",
      "Epoch: 32/200 Iteration: 2790 Train loss: 1.094671 Train acc: 0.409668\n",
      "Epoch: 32/200 Iteration: 2795 Train loss: 1.017504 Train acc: 0.444824\n",
      "Epoch: 32/200 Iteration: 2800 Train loss: 1.184579 Train acc: 0.339844\n",
      "Epoch: 32/200 Iteration: 2805 Train loss: 1.196227 Train acc: 0.232910\n",
      "Epoch: 33/200 Iteration: 2810 Train loss: 1.053895 Train acc: 0.503418\n",
      "Epoch: 33/200 Iteration: 2815 Train loss: 1.093711 Train acc: 0.399902\n",
      "Epoch: 33/200 Iteration: 2820 Train loss: 1.138526 Train acc: 0.348633\n",
      "Epoch: 33/200 Iteration: 2825 Train loss: 1.092378 Train acc: 0.410156\n",
      "Epoch: 33/200 Iteration: 2830 Train loss: 1.144389 Train acc: 0.312988\n",
      "Epoch: 33/200 Iteration: 2835 Train loss: 1.058863 Train acc: 0.467773\n",
      "Epoch: 33/200 Iteration: 2840 Train loss: 1.000045 Train acc: 0.579102\n",
      "Epoch: 33/200 Iteration: 2845 Train loss: 0.999650 Train acc: 0.584961\n",
      "Epoch: 33/200 Iteration: 2850 Train loss: 1.129042 Train acc: 0.278320\n",
      "Epoch: 33/200 Iteration: 2855 Train loss: 0.979645 Train acc: 0.631836\n",
      "Epoch: 33/200 Iteration: 2860 Train loss: 1.057459 Train acc: 0.494629\n",
      "Epoch: 33/200 Iteration: 2865 Train loss: 1.112912 Train acc: 0.249512\n",
      "Epoch: 33/200 Iteration: 2870 Train loss: 1.090567 Train acc: 0.436523\n",
      "Epoch: 33/200 Iteration: 2875 Train loss: 1.089581 Train acc: 0.456543\n",
      "Epoch: 33/200 Iteration: 2880 Train loss: 1.019958 Train acc: 0.430176\n",
      "Epoch: 33/200 Iteration: 2885 Train loss: 1.182948 Train acc: 0.326172\n",
      "Epoch: 33/200 Iteration: 2890 Train loss: 1.198301 Train acc: 0.235352\n",
      "Epoch: 34/200 Iteration: 2895 Train loss: 1.055348 Train acc: 0.503418\n",
      "Epoch: 34/200 Iteration: 2900 Train loss: 1.093881 Train acc: 0.399902\n",
      "Epoch: 34/200 Iteration: 2905 Train loss: 1.139528 Train acc: 0.348633\n",
      "Epoch: 34/200 Iteration: 2910 Train loss: 1.091787 Train acc: 0.410156\n",
      "Epoch: 34/200 Iteration: 2915 Train loss: 1.145072 Train acc: 0.312988\n",
      "Epoch: 34/200 Iteration: 2920 Train loss: 1.058687 Train acc: 0.467773\n",
      "Epoch: 34/200 Iteration: 2925 Train loss: 0.999273 Train acc: 0.579102\n",
      "Epoch: 34/200 Iteration: 2930 Train loss: 0.999741 Train acc: 0.584961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34/200 Iteration: 2935 Train loss: 1.126922 Train acc: 0.278320\n",
      "Epoch: 34/200 Iteration: 2940 Train loss: 0.981504 Train acc: 0.631836\n",
      "Epoch: 34/200 Iteration: 2945 Train loss: 1.059098 Train acc: 0.494629\n",
      "Epoch: 34/200 Iteration: 2950 Train loss: 1.120178 Train acc: 0.243164\n",
      "Epoch: 34/200 Iteration: 2955 Train loss: 1.089803 Train acc: 0.436523\n",
      "Epoch: 34/200 Iteration: 2960 Train loss: 1.091985 Train acc: 0.449707\n",
      "Epoch: 34/200 Iteration: 2965 Train loss: 1.019684 Train acc: 0.437500\n",
      "Epoch: 34/200 Iteration: 2970 Train loss: 1.179105 Train acc: 0.338379\n",
      "Epoch: 34/200 Iteration: 2975 Train loss: 1.193067 Train acc: 0.232422\n",
      "Epoch: 35/200 Iteration: 2980 Train loss: 1.053346 Train acc: 0.503418\n",
      "Epoch: 35/200 Iteration: 2985 Train loss: 1.093601 Train acc: 0.399902\n",
      "Epoch: 35/200 Iteration: 2990 Train loss: 1.134374 Train acc: 0.348633\n",
      "Epoch: 35/200 Iteration: 2995 Train loss: 1.090590 Train acc: 0.410156\n",
      "Epoch: 35/200 Iteration: 3000 Train loss: 1.144647 Train acc: 0.312988\n",
      "Epoch: 35/200 Iteration: 3005 Train loss: 1.059458 Train acc: 0.467773\n",
      "Epoch: 35/200 Iteration: 3010 Train loss: 1.000528 Train acc: 0.579102\n",
      "Epoch: 35/200 Iteration: 3015 Train loss: 0.998742 Train acc: 0.584961\n",
      "Epoch: 35/200 Iteration: 3020 Train loss: 1.131325 Train acc: 0.278320\n",
      "Epoch: 35/200 Iteration: 3025 Train loss: 0.978713 Train acc: 0.631836\n",
      "Epoch: 35/200 Iteration: 3030 Train loss: 1.057489 Train acc: 0.494629\n",
      "Epoch: 35/200 Iteration: 3035 Train loss: 1.110822 Train acc: 0.260254\n",
      "Epoch: 35/200 Iteration: 3040 Train loss: 1.090686 Train acc: 0.423828\n",
      "Epoch: 35/200 Iteration: 3045 Train loss: 1.093930 Train acc: 0.438477\n",
      "Epoch: 35/200 Iteration: 3050 Train loss: 1.015609 Train acc: 0.430176\n",
      "Epoch: 35/200 Iteration: 3055 Train loss: 1.185218 Train acc: 0.340332\n",
      "Epoch: 35/200 Iteration: 3060 Train loss: 1.199154 Train acc: 0.231934\n",
      "Epoch: 36/200 Iteration: 3065 Train loss: 1.056333 Train acc: 0.503418\n",
      "Epoch: 36/200 Iteration: 3070 Train loss: 1.093699 Train acc: 0.399902\n",
      "Epoch: 36/200 Iteration: 3075 Train loss: 1.139192 Train acc: 0.348633\n",
      "Epoch: 36/200 Iteration: 3080 Train loss: 1.092758 Train acc: 0.410156\n",
      "Epoch: 36/200 Iteration: 3085 Train loss: 1.145435 Train acc: 0.312988\n",
      "Epoch: 36/200 Iteration: 3090 Train loss: 1.059963 Train acc: 0.467773\n",
      "Epoch: 36/200 Iteration: 3095 Train loss: 0.999221 Train acc: 0.579102\n",
      "Epoch: 36/200 Iteration: 3100 Train loss: 0.999600 Train acc: 0.584961\n",
      "Epoch: 36/200 Iteration: 3105 Train loss: 1.127829 Train acc: 0.278320\n",
      "Epoch: 36/200 Iteration: 3110 Train loss: 0.979168 Train acc: 0.631836\n",
      "Epoch: 36/200 Iteration: 3115 Train loss: 1.058777 Train acc: 0.494629\n",
      "Epoch: 36/200 Iteration: 3120 Train loss: 1.120399 Train acc: 0.242676\n",
      "Epoch: 36/200 Iteration: 3125 Train loss: 1.087791 Train acc: 0.440430\n",
      "Epoch: 36/200 Iteration: 3130 Train loss: 1.092379 Train acc: 0.456055\n",
      "Epoch: 36/200 Iteration: 3135 Train loss: 1.020237 Train acc: 0.430664\n",
      "Epoch: 36/200 Iteration: 3140 Train loss: 1.179824 Train acc: 0.352539\n",
      "Epoch: 36/200 Iteration: 3145 Train loss: 1.194627 Train acc: 0.232422\n",
      "Epoch: 37/200 Iteration: 3150 Train loss: 1.053322 Train acc: 0.503418\n",
      "Epoch: 37/200 Iteration: 3155 Train loss: 1.094099 Train acc: 0.399902\n",
      "Epoch: 37/200 Iteration: 3160 Train loss: 1.135484 Train acc: 0.348633\n",
      "Epoch: 37/200 Iteration: 3165 Train loss: 1.090754 Train acc: 0.410156\n",
      "Epoch: 37/200 Iteration: 3170 Train loss: 1.143749 Train acc: 0.312988\n",
      "Epoch: 37/200 Iteration: 3175 Train loss: 1.060569 Train acc: 0.467773\n",
      "Epoch: 37/200 Iteration: 3180 Train loss: 0.999716 Train acc: 0.579102\n",
      "Epoch: 37/200 Iteration: 3185 Train loss: 0.997086 Train acc: 0.584961\n",
      "Epoch: 37/200 Iteration: 3190 Train loss: 1.131248 Train acc: 0.278320\n",
      "Epoch: 37/200 Iteration: 3195 Train loss: 0.978632 Train acc: 0.631836\n",
      "Epoch: 37/200 Iteration: 3200 Train loss: 1.057321 Train acc: 0.494629\n",
      "Epoch: 37/200 Iteration: 3205 Train loss: 1.111952 Train acc: 0.245605\n",
      "Epoch: 37/200 Iteration: 3210 Train loss: 1.091920 Train acc: 0.424805\n",
      "Epoch: 37/200 Iteration: 3215 Train loss: 1.094630 Train acc: 0.434570\n",
      "Epoch: 37/200 Iteration: 3220 Train loss: 1.015300 Train acc: 0.440918\n",
      "Epoch: 37/200 Iteration: 3225 Train loss: 1.184520 Train acc: 0.345703\n",
      "Epoch: 37/200 Iteration: 3230 Train loss: 1.198590 Train acc: 0.232910\n",
      "Epoch: 38/200 Iteration: 3235 Train loss: 1.055265 Train acc: 0.503418\n",
      "Epoch: 38/200 Iteration: 3240 Train loss: 1.093994 Train acc: 0.399902\n",
      "Epoch: 38/200 Iteration: 3245 Train loss: 1.137791 Train acc: 0.348633\n",
      "Epoch: 38/200 Iteration: 3250 Train loss: 1.092414 Train acc: 0.410156\n",
      "Epoch: 38/200 Iteration: 3255 Train loss: 1.142672 Train acc: 0.312988\n",
      "Epoch: 38/200 Iteration: 3260 Train loss: 1.062198 Train acc: 0.467773\n",
      "Epoch: 38/200 Iteration: 3265 Train loss: 1.000982 Train acc: 0.579102\n",
      "Epoch: 38/200 Iteration: 3270 Train loss: 0.998358 Train acc: 0.584961\n",
      "Epoch: 38/200 Iteration: 3275 Train loss: 1.128039 Train acc: 0.278320\n",
      "Epoch: 38/200 Iteration: 3280 Train loss: 0.979770 Train acc: 0.631836\n",
      "Epoch: 38/200 Iteration: 3285 Train loss: 1.058106 Train acc: 0.494629\n",
      "Epoch: 38/200 Iteration: 3290 Train loss: 1.118186 Train acc: 0.247070\n",
      "Epoch: 38/200 Iteration: 3295 Train loss: 1.092481 Train acc: 0.429199\n",
      "Epoch: 38/200 Iteration: 3300 Train loss: 1.089407 Train acc: 0.457031\n",
      "Epoch: 38/200 Iteration: 3305 Train loss: 1.018872 Train acc: 0.433105\n",
      "Epoch: 38/200 Iteration: 3310 Train loss: 1.178705 Train acc: 0.334473\n",
      "Epoch: 38/200 Iteration: 3315 Train loss: 1.197264 Train acc: 0.231934\n",
      "Epoch: 39/200 Iteration: 3320 Train loss: 1.055925 Train acc: 0.503418\n",
      "Epoch: 39/200 Iteration: 3325 Train loss: 1.093163 Train acc: 0.399902\n",
      "Epoch: 39/200 Iteration: 3330 Train loss: 1.135365 Train acc: 0.348633\n",
      "Epoch: 39/200 Iteration: 3335 Train loss: 1.092733 Train acc: 0.410156\n",
      "Epoch: 39/200 Iteration: 3340 Train loss: 1.144254 Train acc: 0.312988\n",
      "Epoch: 39/200 Iteration: 3345 Train loss: 1.060853 Train acc: 0.467773\n",
      "Epoch: 39/200 Iteration: 3350 Train loss: 0.999255 Train acc: 0.579102\n",
      "Epoch: 39/200 Iteration: 3355 Train loss: 0.998283 Train acc: 0.584961\n",
      "Epoch: 39/200 Iteration: 3360 Train loss: 1.127881 Train acc: 0.278320\n",
      "Epoch: 39/200 Iteration: 3365 Train loss: 0.978132 Train acc: 0.631836\n",
      "Epoch: 39/200 Iteration: 3370 Train loss: 1.058359 Train acc: 0.494629\n",
      "Epoch: 39/200 Iteration: 3375 Train loss: 1.121127 Train acc: 0.250000\n",
      "Epoch: 39/200 Iteration: 3380 Train loss: 1.085440 Train acc: 0.449219\n",
      "Epoch: 39/200 Iteration: 3385 Train loss: 1.090910 Train acc: 0.463867\n",
      "Epoch: 39/200 Iteration: 3390 Train loss: 1.018757 Train acc: 0.435547\n",
      "Epoch: 39/200 Iteration: 3395 Train loss: 1.180834 Train acc: 0.348145\n",
      "Epoch: 39/200 Iteration: 3400 Train loss: 1.199641 Train acc: 0.232422\n",
      "Epoch: 40/200 Iteration: 3405 Train loss: 1.054339 Train acc: 0.503418\n",
      "Epoch: 40/200 Iteration: 3410 Train loss: 1.093678 Train acc: 0.399902\n",
      "Epoch: 40/200 Iteration: 3415 Train loss: 1.135635 Train acc: 0.348633\n",
      "Epoch: 40/200 Iteration: 3420 Train loss: 1.090623 Train acc: 0.410156\n",
      "Epoch: 40/200 Iteration: 3425 Train loss: 1.145499 Train acc: 0.312988\n",
      "Epoch: 40/200 Iteration: 3430 Train loss: 1.061074 Train acc: 0.467773\n",
      "Epoch: 40/200 Iteration: 3435 Train loss: 1.000479 Train acc: 0.579102\n",
      "Epoch: 40/200 Iteration: 3440 Train loss: 0.999253 Train acc: 0.584961\n",
      "Epoch: 40/200 Iteration: 3445 Train loss: 1.129727 Train acc: 0.278320\n",
      "Epoch: 40/200 Iteration: 3450 Train loss: 0.977861 Train acc: 0.631836\n",
      "Epoch: 40/200 Iteration: 3455 Train loss: 1.057557 Train acc: 0.494629\n",
      "Epoch: 40/200 Iteration: 3460 Train loss: 1.114928 Train acc: 0.246582\n",
      "Epoch: 40/200 Iteration: 3465 Train loss: 1.090945 Train acc: 0.431152\n",
      "Epoch: 40/200 Iteration: 3470 Train loss: 1.092689 Train acc: 0.459473\n",
      "Epoch: 40/200 Iteration: 3475 Train loss: 1.017464 Train acc: 0.425781\n",
      "Epoch: 40/200 Iteration: 3480 Train loss: 1.182071 Train acc: 0.331055\n",
      "Epoch: 40/200 Iteration: 3485 Train loss: 1.194052 Train acc: 0.231445\n",
      "Epoch: 41/200 Iteration: 3490 Train loss: 1.053243 Train acc: 0.503418\n",
      "Epoch: 41/200 Iteration: 3495 Train loss: 1.093725 Train acc: 0.399902\n",
      "Epoch: 41/200 Iteration: 3500 Train loss: 1.135153 Train acc: 0.348633\n",
      "Epoch: 41/200 Iteration: 3505 Train loss: 1.091619 Train acc: 0.410156\n",
      "Epoch: 41/200 Iteration: 3510 Train loss: 1.144279 Train acc: 0.312988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41/200 Iteration: 3515 Train loss: 1.059749 Train acc: 0.467773\n",
      "Epoch: 41/200 Iteration: 3520 Train loss: 1.000605 Train acc: 0.579102\n",
      "Epoch: 41/200 Iteration: 3525 Train loss: 0.999318 Train acc: 0.584961\n",
      "Epoch: 41/200 Iteration: 3530 Train loss: 1.130707 Train acc: 0.278320\n",
      "Epoch: 41/200 Iteration: 3535 Train loss: 0.977143 Train acc: 0.631836\n",
      "Epoch: 41/200 Iteration: 3540 Train loss: 1.056601 Train acc: 0.494629\n",
      "Epoch: 41/200 Iteration: 3545 Train loss: 1.112960 Train acc: 0.251953\n",
      "Epoch: 41/200 Iteration: 3550 Train loss: 1.088916 Train acc: 0.430664\n",
      "Epoch: 41/200 Iteration: 3555 Train loss: 1.094062 Train acc: 0.445312\n",
      "Epoch: 41/200 Iteration: 3560 Train loss: 1.015064 Train acc: 0.442871\n",
      "Epoch: 41/200 Iteration: 3565 Train loss: 1.187382 Train acc: 0.333496\n",
      "Epoch: 41/200 Iteration: 3570 Train loss: 1.197835 Train acc: 0.232910\n",
      "Epoch: 42/200 Iteration: 3575 Train loss: 1.054910 Train acc: 0.503418\n",
      "Epoch: 42/200 Iteration: 3580 Train loss: 1.093367 Train acc: 0.399902\n",
      "Epoch: 42/200 Iteration: 3585 Train loss: 1.135768 Train acc: 0.348633\n",
      "Epoch: 42/200 Iteration: 3590 Train loss: 1.091131 Train acc: 0.410156\n",
      "Epoch: 42/200 Iteration: 3595 Train loss: 1.143182 Train acc: 0.312988\n",
      "Epoch: 42/200 Iteration: 3600 Train loss: 1.060065 Train acc: 0.467773\n",
      "Epoch: 42/200 Iteration: 3605 Train loss: 0.999452 Train acc: 0.579102\n",
      "Epoch: 42/200 Iteration: 3610 Train loss: 0.996821 Train acc: 0.584961\n",
      "Epoch: 42/200 Iteration: 3615 Train loss: 1.129927 Train acc: 0.278320\n",
      "Epoch: 42/200 Iteration: 3620 Train loss: 0.976142 Train acc: 0.631836\n",
      "Epoch: 42/200 Iteration: 3625 Train loss: 1.058713 Train acc: 0.494629\n",
      "Epoch: 42/200 Iteration: 3630 Train loss: 1.116892 Train acc: 0.244141\n",
      "Epoch: 42/200 Iteration: 3635 Train loss: 1.091565 Train acc: 0.434082\n",
      "Epoch: 42/200 Iteration: 3640 Train loss: 1.090031 Train acc: 0.458984\n",
      "Epoch: 42/200 Iteration: 3645 Train loss: 1.021024 Train acc: 0.423828\n",
      "Epoch: 42/200 Iteration: 3650 Train loss: 1.178464 Train acc: 0.336914\n",
      "Epoch: 42/200 Iteration: 3655 Train loss: 1.197775 Train acc: 0.231445\n",
      "Epoch: 43/200 Iteration: 3660 Train loss: 1.054473 Train acc: 0.503418\n",
      "Epoch: 43/200 Iteration: 3665 Train loss: 1.093989 Train acc: 0.399902\n",
      "Epoch: 43/200 Iteration: 3670 Train loss: 1.137648 Train acc: 0.348633\n",
      "Epoch: 43/200 Iteration: 3675 Train loss: 1.092152 Train acc: 0.410156\n",
      "Epoch: 43/200 Iteration: 3680 Train loss: 1.144458 Train acc: 0.312988\n",
      "Epoch: 43/200 Iteration: 3685 Train loss: 1.059850 Train acc: 0.467773\n",
      "Epoch: 43/200 Iteration: 3690 Train loss: 0.999526 Train acc: 0.579102\n",
      "Epoch: 43/200 Iteration: 3695 Train loss: 0.997666 Train acc: 0.584961\n",
      "Epoch: 43/200 Iteration: 3700 Train loss: 1.128348 Train acc: 0.278320\n",
      "Epoch: 43/200 Iteration: 3705 Train loss: 0.978885 Train acc: 0.631836\n",
      "Epoch: 43/200 Iteration: 3710 Train loss: 1.058265 Train acc: 0.494629\n",
      "Epoch: 43/200 Iteration: 3715 Train loss: 1.117093 Train acc: 0.254395\n",
      "Epoch: 43/200 Iteration: 3720 Train loss: 1.089086 Train acc: 0.425293\n",
      "Epoch: 43/200 Iteration: 3725 Train loss: 1.095047 Train acc: 0.436523\n",
      "Epoch: 43/200 Iteration: 3730 Train loss: 1.015202 Train acc: 0.436035\n",
      "Epoch: 43/200 Iteration: 3735 Train loss: 1.180000 Train acc: 0.336914\n",
      "Epoch: 43/200 Iteration: 3740 Train loss: 1.194395 Train acc: 0.230957\n",
      "Epoch: 44/200 Iteration: 3745 Train loss: 1.053558 Train acc: 0.503418\n",
      "Epoch: 44/200 Iteration: 3750 Train loss: 1.094585 Train acc: 0.399902\n",
      "Epoch: 44/200 Iteration: 3755 Train loss: 1.134729 Train acc: 0.348633\n",
      "Epoch: 44/200 Iteration: 3760 Train loss: 1.091125 Train acc: 0.410156\n",
      "Epoch: 44/200 Iteration: 3765 Train loss: 1.142232 Train acc: 0.312988\n",
      "Epoch: 44/200 Iteration: 3770 Train loss: 1.060229 Train acc: 0.467773\n",
      "Epoch: 44/200 Iteration: 3775 Train loss: 1.001155 Train acc: 0.579102\n",
      "Epoch: 44/200 Iteration: 3780 Train loss: 0.998206 Train acc: 0.584961\n",
      "Epoch: 44/200 Iteration: 3785 Train loss: 1.129101 Train acc: 0.278320\n",
      "Epoch: 44/200 Iteration: 3790 Train loss: 0.977924 Train acc: 0.631836\n",
      "Epoch: 44/200 Iteration: 3795 Train loss: 1.057173 Train acc: 0.494629\n",
      "Epoch: 44/200 Iteration: 3800 Train loss: 1.116510 Train acc: 0.241211\n",
      "Epoch: 44/200 Iteration: 3805 Train loss: 1.090869 Train acc: 0.432617\n",
      "Epoch: 44/200 Iteration: 3810 Train loss: 1.092487 Train acc: 0.461426\n",
      "Epoch: 44/200 Iteration: 3815 Train loss: 1.016317 Train acc: 0.437012\n",
      "Epoch: 44/200 Iteration: 3820 Train loss: 1.182323 Train acc: 0.341309\n",
      "Epoch: 44/200 Iteration: 3825 Train loss: 1.200182 Train acc: 0.231445\n",
      "Epoch: 45/200 Iteration: 3830 Train loss: 1.055321 Train acc: 0.503418\n",
      "Epoch: 45/200 Iteration: 3835 Train loss: 1.092890 Train acc: 0.399902\n",
      "Epoch: 45/200 Iteration: 3840 Train loss: 1.136919 Train acc: 0.348633\n",
      "Epoch: 45/200 Iteration: 3845 Train loss: 1.092169 Train acc: 0.410156\n",
      "Epoch: 45/200 Iteration: 3850 Train loss: 1.143869 Train acc: 0.312988\n",
      "Epoch: 45/200 Iteration: 3855 Train loss: 1.058013 Train acc: 0.467773\n",
      "Epoch: 45/200 Iteration: 3860 Train loss: 0.999792 Train acc: 0.579102\n",
      "Epoch: 45/200 Iteration: 3865 Train loss: 0.995999 Train acc: 0.584961\n",
      "Epoch: 45/200 Iteration: 3870 Train loss: 1.129235 Train acc: 0.278320\n",
      "Epoch: 45/200 Iteration: 3875 Train loss: 0.974656 Train acc: 0.631836\n",
      "Epoch: 45/200 Iteration: 3880 Train loss: 1.057910 Train acc: 0.494629\n",
      "Epoch: 45/200 Iteration: 3885 Train loss: 1.121030 Train acc: 0.253418\n",
      "Epoch: 45/200 Iteration: 3890 Train loss: 1.086863 Train acc: 0.444336\n",
      "Epoch: 45/200 Iteration: 3895 Train loss: 1.088376 Train acc: 0.465332\n",
      "Epoch: 45/200 Iteration: 3900 Train loss: 1.019823 Train acc: 0.421875\n",
      "Epoch: 45/200 Iteration: 3905 Train loss: 1.175694 Train acc: 0.348145\n",
      "Epoch: 45/200 Iteration: 3910 Train loss: 1.188167 Train acc: 0.231445\n",
      "Epoch: 46/200 Iteration: 3915 Train loss: 1.051453 Train acc: 0.503418\n",
      "Epoch: 46/200 Iteration: 3920 Train loss: 1.094782 Train acc: 0.399902\n",
      "Epoch: 46/200 Iteration: 3925 Train loss: 1.136410 Train acc: 0.348633\n",
      "Epoch: 46/200 Iteration: 3930 Train loss: 1.090599 Train acc: 0.410156\n",
      "Epoch: 46/200 Iteration: 3935 Train loss: 1.143262 Train acc: 0.312988\n",
      "Epoch: 46/200 Iteration: 3940 Train loss: 1.060229 Train acc: 0.467773\n",
      "Epoch: 46/200 Iteration: 3945 Train loss: 1.000942 Train acc: 0.579102\n",
      "Epoch: 46/200 Iteration: 3950 Train loss: 0.999002 Train acc: 0.584961\n",
      "Epoch: 46/200 Iteration: 3955 Train loss: 1.132558 Train acc: 0.278320\n",
      "Epoch: 46/200 Iteration: 3960 Train loss: 0.977620 Train acc: 0.631836\n",
      "Epoch: 46/200 Iteration: 3965 Train loss: 1.056870 Train acc: 0.494629\n",
      "Epoch: 46/200 Iteration: 3970 Train loss: 1.106772 Train acc: 0.282227\n",
      "Epoch: 46/200 Iteration: 3975 Train loss: 1.101256 Train acc: 0.345215\n",
      "Epoch: 46/200 Iteration: 3980 Train loss: 1.099253 Train acc: 0.397949\n",
      "Epoch: 46/200 Iteration: 3985 Train loss: 1.010752 Train acc: 0.468262\n",
      "Epoch: 46/200 Iteration: 3990 Train loss: 1.193052 Train acc: 0.339355\n",
      "Epoch: 46/200 Iteration: 3995 Train loss: 1.203374 Train acc: 0.231445\n",
      "Epoch: 47/200 Iteration: 4000 Train loss: 1.056417 Train acc: 0.503418\n",
      "Epoch: 47/200 Iteration: 4005 Train loss: 1.093570 Train acc: 0.399902\n",
      "Epoch: 47/200 Iteration: 4010 Train loss: 1.137302 Train acc: 0.348633\n",
      "Epoch: 47/200 Iteration: 4015 Train loss: 1.093536 Train acc: 0.410156\n",
      "Epoch: 47/200 Iteration: 4020 Train loss: 1.141979 Train acc: 0.312988\n",
      "Epoch: 47/200 Iteration: 4025 Train loss: 1.058735 Train acc: 0.467773\n",
      "Epoch: 47/200 Iteration: 4030 Train loss: 1.001320 Train acc: 0.579102\n",
      "Epoch: 47/200 Iteration: 4035 Train loss: 0.999161 Train acc: 0.584961\n",
      "Epoch: 47/200 Iteration: 4040 Train loss: 1.127070 Train acc: 0.278320\n",
      "Epoch: 47/200 Iteration: 4045 Train loss: 0.976310 Train acc: 0.631836\n",
      "Epoch: 47/200 Iteration: 4050 Train loss: 1.058519 Train acc: 0.494629\n",
      "Epoch: 47/200 Iteration: 4055 Train loss: 1.127251 Train acc: 0.255371\n",
      "Epoch: 47/200 Iteration: 4060 Train loss: 1.082315 Train acc: 0.451172\n",
      "Epoch: 47/200 Iteration: 4065 Train loss: 1.088928 Train acc: 0.473633\n",
      "Epoch: 47/200 Iteration: 4070 Train loss: 1.023179 Train acc: 0.416992\n",
      "Epoch: 47/200 Iteration: 4075 Train loss: 1.176765 Train acc: 0.322266\n",
      "Epoch: 47/200 Iteration: 4080 Train loss: 1.192323 Train acc: 0.232422\n",
      "Epoch: 48/200 Iteration: 4085 Train loss: 1.052702 Train acc: 0.503418\n",
      "Epoch: 48/200 Iteration: 4090 Train loss: 1.094133 Train acc: 0.399902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48/200 Iteration: 4095 Train loss: 1.134172 Train acc: 0.348633\n",
      "Epoch: 48/200 Iteration: 4100 Train loss: 1.091221 Train acc: 0.410156\n",
      "Epoch: 48/200 Iteration: 4105 Train loss: 1.144904 Train acc: 0.312988\n",
      "Epoch: 48/200 Iteration: 4110 Train loss: 1.059339 Train acc: 0.467773\n",
      "Epoch: 48/200 Iteration: 4115 Train loss: 1.000687 Train acc: 0.579102\n",
      "Epoch: 48/200 Iteration: 4120 Train loss: 0.997049 Train acc: 0.584961\n",
      "Epoch: 48/200 Iteration: 4125 Train loss: 1.130394 Train acc: 0.278320\n",
      "Epoch: 48/200 Iteration: 4130 Train loss: 0.976959 Train acc: 0.631836\n",
      "Epoch: 48/200 Iteration: 4135 Train loss: 1.057068 Train acc: 0.494629\n",
      "Epoch: 48/200 Iteration: 4140 Train loss: 1.109236 Train acc: 0.265137\n",
      "Epoch: 48/200 Iteration: 4145 Train loss: 1.092369 Train acc: 0.410156\n",
      "Epoch: 48/200 Iteration: 4150 Train loss: 1.094557 Train acc: 0.432617\n",
      "Epoch: 48/200 Iteration: 4155 Train loss: 1.015387 Train acc: 0.446777\n",
      "Epoch: 48/200 Iteration: 4160 Train loss: 1.183311 Train acc: 0.327148\n",
      "Epoch: 48/200 Iteration: 4165 Train loss: 1.194793 Train acc: 0.231934\n",
      "Epoch: 49/200 Iteration: 4170 Train loss: 1.053083 Train acc: 0.503418\n",
      "Epoch: 49/200 Iteration: 4175 Train loss: 1.094045 Train acc: 0.399902\n",
      "Epoch: 49/200 Iteration: 4180 Train loss: 1.136903 Train acc: 0.348633\n",
      "Epoch: 49/200 Iteration: 4185 Train loss: 1.091528 Train acc: 0.410156\n",
      "Epoch: 49/200 Iteration: 4190 Train loss: 1.144143 Train acc: 0.312988\n",
      "Epoch: 49/200 Iteration: 4195 Train loss: 1.059638 Train acc: 0.467773\n",
      "Epoch: 49/200 Iteration: 4200 Train loss: 1.000486 Train acc: 0.579102\n",
      "Epoch: 49/200 Iteration: 4205 Train loss: 0.998501 Train acc: 0.584961\n",
      "Epoch: 49/200 Iteration: 4210 Train loss: 1.131647 Train acc: 0.278320\n",
      "Epoch: 49/200 Iteration: 4215 Train loss: 0.978046 Train acc: 0.631836\n",
      "Epoch: 49/200 Iteration: 4220 Train loss: 1.056143 Train acc: 0.494629\n",
      "Epoch: 49/200 Iteration: 4225 Train loss: 1.114316 Train acc: 0.257324\n",
      "Epoch: 49/200 Iteration: 4230 Train loss: 1.088023 Train acc: 0.439453\n",
      "Epoch: 49/200 Iteration: 4235 Train loss: 1.091176 Train acc: 0.463867\n",
      "Epoch: 49/200 Iteration: 4240 Train loss: 1.015767 Train acc: 0.426758\n",
      "Epoch: 49/200 Iteration: 4245 Train loss: 1.183283 Train acc: 0.335449\n",
      "Epoch: 49/200 Iteration: 4250 Train loss: 1.198007 Train acc: 0.231934\n",
      "Epoch: 50/200 Iteration: 4255 Train loss: 1.053035 Train acc: 0.503418\n",
      "Epoch: 50/200 Iteration: 4260 Train loss: 1.093731 Train acc: 0.399902\n",
      "Epoch: 50/200 Iteration: 4265 Train loss: 1.138800 Train acc: 0.348633\n",
      "Epoch: 50/200 Iteration: 4270 Train loss: 1.092403 Train acc: 0.410156\n",
      "Epoch: 50/200 Iteration: 4275 Train loss: 1.145301 Train acc: 0.312988\n",
      "Epoch: 50/200 Iteration: 4280 Train loss: 1.059342 Train acc: 0.467773\n",
      "Epoch: 50/200 Iteration: 4285 Train loss: 0.999654 Train acc: 0.579102\n",
      "Epoch: 50/200 Iteration: 4290 Train loss: 0.998058 Train acc: 0.584961\n",
      "Epoch: 50/200 Iteration: 4295 Train loss: 1.131351 Train acc: 0.278320\n",
      "Epoch: 50/200 Iteration: 4300 Train loss: 0.978959 Train acc: 0.631836\n",
      "Epoch: 50/200 Iteration: 4305 Train loss: 1.056624 Train acc: 0.494629\n",
      "Epoch: 50/200 Iteration: 4310 Train loss: 1.110398 Train acc: 0.252441\n",
      "Epoch: 50/200 Iteration: 4315 Train loss: 1.091110 Train acc: 0.431641\n",
      "Epoch: 50/200 Iteration: 4320 Train loss: 1.094147 Train acc: 0.449219\n",
      "Epoch: 50/200 Iteration: 4325 Train loss: 1.015228 Train acc: 0.434082\n",
      "Epoch: 50/200 Iteration: 4330 Train loss: 1.183719 Train acc: 0.324707\n",
      "Epoch: 50/200 Iteration: 4335 Train loss: 1.203680 Train acc: 0.231934\n",
      "Epoch: 51/200 Iteration: 4340 Train loss: 1.057220 Train acc: 0.503418\n",
      "Epoch: 51/200 Iteration: 4345 Train loss: 1.093535 Train acc: 0.399902\n",
      "Epoch: 51/200 Iteration: 4350 Train loss: 1.135900 Train acc: 0.348633\n",
      "Epoch: 51/200 Iteration: 4355 Train loss: 1.092273 Train acc: 0.410156\n",
      "Epoch: 51/200 Iteration: 4360 Train loss: 1.143557 Train acc: 0.312988\n",
      "Epoch: 51/200 Iteration: 4365 Train loss: 1.059841 Train acc: 0.467773\n",
      "Epoch: 51/200 Iteration: 4370 Train loss: 1.001358 Train acc: 0.579102\n",
      "Epoch: 51/200 Iteration: 4375 Train loss: 0.998326 Train acc: 0.584961\n",
      "Epoch: 51/200 Iteration: 4380 Train loss: 1.129184 Train acc: 0.278320\n",
      "Epoch: 51/200 Iteration: 4385 Train loss: 0.978456 Train acc: 0.631836\n",
      "Epoch: 51/200 Iteration: 4390 Train loss: 1.057828 Train acc: 0.494629\n",
      "Epoch: 51/200 Iteration: 4395 Train loss: 1.123322 Train acc: 0.240234\n",
      "Epoch: 51/200 Iteration: 4400 Train loss: 1.083847 Train acc: 0.447754\n",
      "Epoch: 51/200 Iteration: 4405 Train loss: 1.089875 Train acc: 0.467285\n",
      "Epoch: 51/200 Iteration: 4410 Train loss: 1.019599 Train acc: 0.420898\n",
      "Epoch: 51/200 Iteration: 4415 Train loss: 1.178328 Train acc: 0.334961\n",
      "Epoch: 51/200 Iteration: 4420 Train loss: 1.191920 Train acc: 0.231445\n",
      "Epoch: 52/200 Iteration: 4425 Train loss: 1.051634 Train acc: 0.503418\n",
      "Epoch: 52/200 Iteration: 4430 Train loss: 1.093874 Train acc: 0.399902\n",
      "Epoch: 52/200 Iteration: 4435 Train loss: 1.135510 Train acc: 0.348633\n",
      "Epoch: 52/200 Iteration: 4440 Train loss: 1.090478 Train acc: 0.410156\n",
      "Epoch: 52/200 Iteration: 4445 Train loss: 1.142537 Train acc: 0.312988\n",
      "Epoch: 52/200 Iteration: 4450 Train loss: 1.059535 Train acc: 0.467773\n",
      "Epoch: 52/200 Iteration: 4455 Train loss: 1.000558 Train acc: 0.579102\n",
      "Epoch: 52/200 Iteration: 4460 Train loss: 0.998013 Train acc: 0.584961\n",
      "Epoch: 52/200 Iteration: 4465 Train loss: 1.133598 Train acc: 0.278320\n",
      "Epoch: 52/200 Iteration: 4470 Train loss: 0.977263 Train acc: 0.631836\n",
      "Epoch: 52/200 Iteration: 4475 Train loss: 1.054934 Train acc: 0.494629\n",
      "Epoch: 52/200 Iteration: 4480 Train loss: 1.097588 Train acc: 0.377930\n",
      "Epoch: 52/200 Iteration: 4485 Train loss: 1.104806 Train acc: 0.325195\n",
      "Epoch: 52/200 Iteration: 4490 Train loss: 1.098659 Train acc: 0.411621\n",
      "Epoch: 52/200 Iteration: 4495 Train loss: 1.009847 Train acc: 0.463379\n",
      "Epoch: 52/200 Iteration: 4500 Train loss: 1.192187 Train acc: 0.341309\n",
      "Epoch: 52/200 Iteration: 4505 Train loss: 1.204473 Train acc: 0.231445\n",
      "Epoch: 53/200 Iteration: 4510 Train loss: 1.062101 Train acc: 0.503418\n",
      "Epoch: 53/200 Iteration: 4515 Train loss: 1.093721 Train acc: 0.399902\n",
      "Epoch: 53/200 Iteration: 4520 Train loss: 1.135133 Train acc: 0.348633\n",
      "Epoch: 53/200 Iteration: 4525 Train loss: 1.093136 Train acc: 0.410156\n",
      "Epoch: 53/200 Iteration: 4530 Train loss: 1.141078 Train acc: 0.312988\n",
      "Epoch: 53/200 Iteration: 4535 Train loss: 1.060450 Train acc: 0.467773\n",
      "Epoch: 53/200 Iteration: 4540 Train loss: 1.000743 Train acc: 0.579102\n",
      "Epoch: 53/200 Iteration: 4545 Train loss: 0.996696 Train acc: 0.584961\n",
      "Epoch: 53/200 Iteration: 4550 Train loss: 1.129099 Train acc: 0.278320\n",
      "Epoch: 53/200 Iteration: 4555 Train loss: 0.974482 Train acc: 0.631836\n",
      "Epoch: 53/200 Iteration: 4560 Train loss: 1.058223 Train acc: 0.494629\n",
      "Epoch: 53/200 Iteration: 4565 Train loss: 1.127432 Train acc: 0.254395\n",
      "Epoch: 53/200 Iteration: 4570 Train loss: 1.083347 Train acc: 0.444336\n",
      "Epoch: 53/200 Iteration: 4575 Train loss: 1.089383 Train acc: 0.473145\n",
      "Epoch: 53/200 Iteration: 4580 Train loss: 1.024758 Train acc: 0.416504\n",
      "Epoch: 53/200 Iteration: 4585 Train loss: 1.170808 Train acc: 0.339844\n",
      "Epoch: 53/200 Iteration: 4590 Train loss: 1.195476 Train acc: 0.231445\n",
      "Epoch: 54/200 Iteration: 4595 Train loss: 1.051959 Train acc: 0.503418\n",
      "Epoch: 54/200 Iteration: 4600 Train loss: 1.094054 Train acc: 0.399902\n",
      "Epoch: 54/200 Iteration: 4605 Train loss: 1.139903 Train acc: 0.348633\n",
      "Epoch: 54/200 Iteration: 4610 Train loss: 1.091777 Train acc: 0.410156\n",
      "Epoch: 54/200 Iteration: 4615 Train loss: 1.147048 Train acc: 0.312988\n",
      "Epoch: 54/200 Iteration: 4620 Train loss: 1.058681 Train acc: 0.467773\n",
      "Epoch: 54/200 Iteration: 4625 Train loss: 0.999512 Train acc: 0.579102\n",
      "Epoch: 54/200 Iteration: 4630 Train loss: 0.996325 Train acc: 0.584961\n",
      "Epoch: 54/200 Iteration: 4635 Train loss: 1.129136 Train acc: 0.278320\n",
      "Epoch: 54/200 Iteration: 4640 Train loss: 0.975165 Train acc: 0.631836\n",
      "Epoch: 54/200 Iteration: 4645 Train loss: 1.057705 Train acc: 0.494629\n",
      "Epoch: 54/200 Iteration: 4650 Train loss: 1.121588 Train acc: 0.247559\n",
      "Epoch: 54/200 Iteration: 4655 Train loss: 1.087314 Train acc: 0.433105\n",
      "Epoch: 54/200 Iteration: 4660 Train loss: 1.094506 Train acc: 0.441406\n",
      "Epoch: 54/200 Iteration: 4665 Train loss: 1.017646 Train acc: 0.434082\n",
      "Epoch: 54/200 Iteration: 4670 Train loss: 1.180735 Train acc: 0.342285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54/200 Iteration: 4675 Train loss: 1.190659 Train acc: 0.231445\n",
      "Epoch: 55/200 Iteration: 4680 Train loss: 1.049124 Train acc: 0.503418\n",
      "Epoch: 55/200 Iteration: 4685 Train loss: 1.092875 Train acc: 0.399902\n",
      "Epoch: 55/200 Iteration: 4690 Train loss: 1.136303 Train acc: 0.348633\n",
      "Epoch: 55/200 Iteration: 4695 Train loss: 1.089844 Train acc: 0.410156\n",
      "Epoch: 55/200 Iteration: 4700 Train loss: 1.141769 Train acc: 0.312988\n",
      "Epoch: 55/200 Iteration: 4705 Train loss: 1.059109 Train acc: 0.467773\n",
      "Epoch: 55/200 Iteration: 4710 Train loss: 1.000541 Train acc: 0.579102\n",
      "Epoch: 55/200 Iteration: 4715 Train loss: 0.998625 Train acc: 0.584961\n",
      "Epoch: 55/200 Iteration: 4720 Train loss: 1.136660 Train acc: 0.278320\n",
      "Epoch: 55/200 Iteration: 4725 Train loss: 0.976650 Train acc: 0.631836\n",
      "Epoch: 55/200 Iteration: 4730 Train loss: 1.054318 Train acc: 0.494629\n",
      "Epoch: 55/200 Iteration: 4735 Train loss: 1.101485 Train acc: 0.310547\n",
      "Epoch: 55/200 Iteration: 4740 Train loss: 1.104420 Train acc: 0.367676\n",
      "Epoch: 55/200 Iteration: 4745 Train loss: 1.098304 Train acc: 0.421875\n",
      "Epoch: 55/200 Iteration: 4750 Train loss: 1.011749 Train acc: 0.454102\n",
      "Epoch: 55/200 Iteration: 4755 Train loss: 1.189497 Train acc: 0.342285\n",
      "Epoch: 55/200 Iteration: 4760 Train loss: 1.207487 Train acc: 0.231934\n",
      "Epoch: 56/200 Iteration: 4765 Train loss: 1.060637 Train acc: 0.503418\n",
      "Epoch: 56/200 Iteration: 4770 Train loss: 1.094288 Train acc: 0.399902\n",
      "Epoch: 56/200 Iteration: 4775 Train loss: 1.136712 Train acc: 0.348633\n",
      "Epoch: 56/200 Iteration: 4780 Train loss: 1.093844 Train acc: 0.410156\n",
      "Epoch: 56/200 Iteration: 4785 Train loss: 1.143493 Train acc: 0.312988\n",
      "Epoch: 56/200 Iteration: 4790 Train loss: 1.059116 Train acc: 0.467773\n",
      "Epoch: 56/200 Iteration: 4795 Train loss: 1.001199 Train acc: 0.579102\n",
      "Epoch: 56/200 Iteration: 4800 Train loss: 0.996325 Train acc: 0.584961\n",
      "Epoch: 56/200 Iteration: 4805 Train loss: 1.126415 Train acc: 0.278320\n",
      "Epoch: 56/200 Iteration: 4810 Train loss: 0.975692 Train acc: 0.631836\n",
      "Epoch: 56/200 Iteration: 4815 Train loss: 1.059331 Train acc: 0.494629\n",
      "Epoch: 56/200 Iteration: 4820 Train loss: 1.132324 Train acc: 0.250977\n",
      "Epoch: 56/200 Iteration: 4825 Train loss: 1.086782 Train acc: 0.447266\n",
      "Epoch: 56/200 Iteration: 4830 Train loss: 1.086761 Train acc: 0.473633\n",
      "Epoch: 56/200 Iteration: 4835 Train loss: 1.024814 Train acc: 0.414062\n",
      "Epoch: 56/200 Iteration: 4840 Train loss: 1.170534 Train acc: 0.344238\n",
      "Epoch: 56/200 Iteration: 4845 Train loss: 1.189918 Train acc: 0.231445\n",
      "Epoch: 57/200 Iteration: 4850 Train loss: 1.050694 Train acc: 0.503418\n",
      "Epoch: 57/200 Iteration: 4855 Train loss: 1.093592 Train acc: 0.399902\n",
      "Epoch: 57/200 Iteration: 4860 Train loss: 1.136251 Train acc: 0.348633\n",
      "Epoch: 57/200 Iteration: 4865 Train loss: 1.091289 Train acc: 0.410156\n",
      "Epoch: 57/200 Iteration: 4870 Train loss: 1.145051 Train acc: 0.312988\n",
      "Epoch: 57/200 Iteration: 4875 Train loss: 1.059477 Train acc: 0.467773\n",
      "Epoch: 57/200 Iteration: 4880 Train loss: 1.000015 Train acc: 0.579102\n",
      "Epoch: 57/200 Iteration: 4885 Train loss: 0.999088 Train acc: 0.584961\n",
      "Epoch: 57/200 Iteration: 4890 Train loss: 1.130822 Train acc: 0.278320\n",
      "Epoch: 57/200 Iteration: 4895 Train loss: 0.980694 Train acc: 0.631836\n",
      "Epoch: 57/200 Iteration: 4900 Train loss: 1.056005 Train acc: 0.494629\n",
      "Epoch: 57/200 Iteration: 4905 Train loss: 1.106356 Train acc: 0.281738\n",
      "Epoch: 57/200 Iteration: 4910 Train loss: 1.092773 Train acc: 0.403809\n",
      "Epoch: 57/200 Iteration: 4915 Train loss: 1.099037 Train acc: 0.400391\n",
      "Epoch: 57/200 Iteration: 4920 Train loss: 1.011250 Train acc: 0.460938\n",
      "Epoch: 57/200 Iteration: 4925 Train loss: 1.182612 Train acc: 0.336426\n",
      "Epoch: 57/200 Iteration: 4930 Train loss: 1.200383 Train acc: 0.231445\n",
      "Epoch: 58/200 Iteration: 4935 Train loss: 1.055213 Train acc: 0.503418\n",
      "Epoch: 58/200 Iteration: 4940 Train loss: 1.092823 Train acc: 0.399902\n",
      "Epoch: 58/200 Iteration: 4945 Train loss: 1.138600 Train acc: 0.348633\n",
      "Epoch: 58/200 Iteration: 4950 Train loss: 1.092538 Train acc: 0.410156\n",
      "Epoch: 58/200 Iteration: 4955 Train loss: 1.145954 Train acc: 0.312988\n",
      "Epoch: 58/200 Iteration: 4960 Train loss: 1.059580 Train acc: 0.467773\n",
      "Epoch: 58/200 Iteration: 4965 Train loss: 0.998657 Train acc: 0.579102\n",
      "Epoch: 58/200 Iteration: 4970 Train loss: 0.998485 Train acc: 0.584961\n",
      "Epoch: 58/200 Iteration: 4975 Train loss: 1.130890 Train acc: 0.278320\n",
      "Epoch: 58/200 Iteration: 4980 Train loss: 0.980061 Train acc: 0.631836\n",
      "Epoch: 58/200 Iteration: 4985 Train loss: 1.056357 Train acc: 0.494629\n",
      "Epoch: 58/200 Iteration: 4990 Train loss: 1.115143 Train acc: 0.241211\n",
      "Epoch: 58/200 Iteration: 4995 Train loss: 1.084115 Train acc: 0.450195\n",
      "Epoch: 58/200 Iteration: 5000 Train loss: 1.089481 Train acc: 0.477051\n",
      "Epoch: 58/200 Iteration: 5005 Train loss: 1.016481 Train acc: 0.418457\n",
      "Epoch: 58/200 Iteration: 5010 Train loss: 1.179268 Train acc: 0.326660\n",
      "Epoch: 58/200 Iteration: 5015 Train loss: 1.200336 Train acc: 0.231445\n",
      "Epoch: 59/200 Iteration: 5020 Train loss: 1.053773 Train acc: 0.503418\n",
      "Epoch: 59/200 Iteration: 5025 Train loss: 1.093743 Train acc: 0.399902\n",
      "Epoch: 59/200 Iteration: 5030 Train loss: 1.136477 Train acc: 0.348633\n",
      "Epoch: 59/200 Iteration: 5035 Train loss: 1.089871 Train acc: 0.410156\n",
      "Epoch: 59/200 Iteration: 5040 Train loss: 1.145446 Train acc: 0.312988\n",
      "Epoch: 59/200 Iteration: 5045 Train loss: 1.058933 Train acc: 0.467773\n",
      "Epoch: 59/200 Iteration: 5050 Train loss: 0.999706 Train acc: 0.579102\n",
      "Epoch: 59/200 Iteration: 5055 Train loss: 0.996337 Train acc: 0.584961\n",
      "Epoch: 59/200 Iteration: 5060 Train loss: 1.134149 Train acc: 0.278320\n",
      "Epoch: 59/200 Iteration: 5065 Train loss: 0.977128 Train acc: 0.631836\n",
      "Epoch: 59/200 Iteration: 5070 Train loss: 1.055183 Train acc: 0.494629\n",
      "Epoch: 59/200 Iteration: 5075 Train loss: 1.106371 Train acc: 0.277344\n",
      "Epoch: 59/200 Iteration: 5080 Train loss: 1.093000 Train acc: 0.410645\n",
      "Epoch: 59/200 Iteration: 5085 Train loss: 1.093759 Train acc: 0.443359\n",
      "Epoch: 59/200 Iteration: 5090 Train loss: 1.012335 Train acc: 0.432617\n",
      "Epoch: 59/200 Iteration: 5095 Train loss: 1.183339 Train acc: 0.339355\n",
      "Epoch: 59/200 Iteration: 5100 Train loss: 1.202878 Train acc: 0.231445\n",
      "Epoch: 60/200 Iteration: 5105 Train loss: 1.055303 Train acc: 0.503418\n",
      "Epoch: 60/200 Iteration: 5110 Train loss: 1.093284 Train acc: 0.399902\n",
      "Epoch: 60/200 Iteration: 5115 Train loss: 1.134434 Train acc: 0.348633\n",
      "Epoch: 60/200 Iteration: 5120 Train loss: 1.092652 Train acc: 0.410156\n",
      "Epoch: 60/200 Iteration: 5125 Train loss: 1.146620 Train acc: 0.312988\n",
      "Epoch: 60/200 Iteration: 5130 Train loss: 1.061124 Train acc: 0.467773\n",
      "Epoch: 60/200 Iteration: 5135 Train loss: 0.999895 Train acc: 0.579102\n",
      "Epoch: 60/200 Iteration: 5140 Train loss: 0.998218 Train acc: 0.584961\n",
      "Epoch: 60/200 Iteration: 5145 Train loss: 1.131541 Train acc: 0.278320\n",
      "Epoch: 60/200 Iteration: 5150 Train loss: 0.978459 Train acc: 0.631836\n",
      "Epoch: 60/200 Iteration: 5155 Train loss: 1.055772 Train acc: 0.494629\n",
      "Epoch: 60/200 Iteration: 5160 Train loss: 1.115867 Train acc: 0.254883\n",
      "Epoch: 60/200 Iteration: 5165 Train loss: 1.084046 Train acc: 0.441895\n",
      "Epoch: 60/200 Iteration: 5170 Train loss: 1.089928 Train acc: 0.469238\n",
      "Epoch: 60/200 Iteration: 5175 Train loss: 1.016923 Train acc: 0.425781\n",
      "Epoch: 60/200 Iteration: 5180 Train loss: 1.183268 Train acc: 0.349121\n",
      "Epoch: 60/200 Iteration: 5185 Train loss: 1.198766 Train acc: 0.231934\n",
      "Epoch: 61/200 Iteration: 5190 Train loss: 1.053577 Train acc: 0.503418\n",
      "Epoch: 61/200 Iteration: 5195 Train loss: 1.093881 Train acc: 0.399902\n",
      "Epoch: 61/200 Iteration: 5200 Train loss: 1.135495 Train acc: 0.348633\n",
      "Epoch: 61/200 Iteration: 5205 Train loss: 1.090981 Train acc: 0.410156\n",
      "Epoch: 61/200 Iteration: 5210 Train loss: 1.147899 Train acc: 0.312988\n",
      "Epoch: 61/200 Iteration: 5215 Train loss: 1.058767 Train acc: 0.467773\n",
      "Epoch: 61/200 Iteration: 5220 Train loss: 0.998468 Train acc: 0.579102\n",
      "Epoch: 61/200 Iteration: 5225 Train loss: 0.997907 Train acc: 0.584961\n",
      "Epoch: 61/200 Iteration: 5230 Train loss: 1.135120 Train acc: 0.278320\n",
      "Epoch: 61/200 Iteration: 5235 Train loss: 0.976649 Train acc: 0.631836\n",
      "Epoch: 61/200 Iteration: 5240 Train loss: 1.055259 Train acc: 0.494629\n",
      "Epoch: 61/200 Iteration: 5245 Train loss: 1.103602 Train acc: 0.291992\n",
      "Epoch: 61/200 Iteration: 5250 Train loss: 1.090745 Train acc: 0.427734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61/200 Iteration: 5255 Train loss: 1.094931 Train acc: 0.450684\n",
      "Epoch: 61/200 Iteration: 5260 Train loss: 1.013103 Train acc: 0.439941\n",
      "Epoch: 61/200 Iteration: 5265 Train loss: 1.186412 Train acc: 0.342285\n",
      "Epoch: 61/200 Iteration: 5270 Train loss: 1.207744 Train acc: 0.231445\n",
      "Epoch: 62/200 Iteration: 5275 Train loss: 1.057160 Train acc: 0.503418\n",
      "Epoch: 62/200 Iteration: 5280 Train loss: 1.093801 Train acc: 0.399902\n",
      "Epoch: 62/200 Iteration: 5285 Train loss: 1.136417 Train acc: 0.348633\n",
      "Epoch: 62/200 Iteration: 5290 Train loss: 1.092833 Train acc: 0.410156\n",
      "Epoch: 62/200 Iteration: 5295 Train loss: 1.147172 Train acc: 0.312988\n",
      "Epoch: 62/200 Iteration: 5300 Train loss: 1.059153 Train acc: 0.467773\n",
      "Epoch: 62/200 Iteration: 5305 Train loss: 0.998522 Train acc: 0.579102\n",
      "Epoch: 62/200 Iteration: 5310 Train loss: 0.997751 Train acc: 0.584961\n",
      "Epoch: 62/200 Iteration: 5315 Train loss: 1.131663 Train acc: 0.278320\n",
      "Epoch: 62/200 Iteration: 5320 Train loss: 0.978006 Train acc: 0.631836\n",
      "Epoch: 62/200 Iteration: 5325 Train loss: 1.056324 Train acc: 0.494629\n",
      "Epoch: 62/200 Iteration: 5330 Train loss: 1.118005 Train acc: 0.249023\n",
      "Epoch: 62/200 Iteration: 5335 Train loss: 1.080721 Train acc: 0.448730\n",
      "Epoch: 62/200 Iteration: 5340 Train loss: 1.088095 Train acc: 0.469727\n",
      "Epoch: 62/200 Iteration: 5345 Train loss: 1.019840 Train acc: 0.414062\n",
      "Epoch: 62/200 Iteration: 5350 Train loss: 1.176882 Train acc: 0.334473\n",
      "Epoch: 62/200 Iteration: 5355 Train loss: 1.193216 Train acc: 0.231445\n",
      "Epoch: 63/200 Iteration: 5360 Train loss: 1.052521 Train acc: 0.503418\n",
      "Epoch: 63/200 Iteration: 5365 Train loss: 1.092992 Train acc: 0.399902\n",
      "Epoch: 63/200 Iteration: 5370 Train loss: 1.135389 Train acc: 0.348633\n",
      "Epoch: 63/200 Iteration: 5375 Train loss: 1.089396 Train acc: 0.410156\n",
      "Epoch: 63/200 Iteration: 5380 Train loss: 1.147181 Train acc: 0.312988\n",
      "Epoch: 63/200 Iteration: 5385 Train loss: 1.057631 Train acc: 0.467773\n",
      "Epoch: 63/200 Iteration: 5390 Train loss: 1.000371 Train acc: 0.579102\n",
      "Epoch: 63/200 Iteration: 5395 Train loss: 0.997843 Train acc: 0.584961\n",
      "Epoch: 63/200 Iteration: 5400 Train loss: 1.135388 Train acc: 0.278320\n",
      "Epoch: 63/200 Iteration: 5405 Train loss: 0.977841 Train acc: 0.631836\n",
      "Epoch: 63/200 Iteration: 5410 Train loss: 1.055077 Train acc: 0.494629\n",
      "Epoch: 63/200 Iteration: 5415 Train loss: 1.097949 Train acc: 0.369629\n",
      "Epoch: 63/200 Iteration: 5420 Train loss: 1.103169 Train acc: 0.343750\n",
      "Epoch: 63/200 Iteration: 5425 Train loss: 1.100738 Train acc: 0.400879\n",
      "Epoch: 63/200 Iteration: 5430 Train loss: 1.006260 Train acc: 0.475586\n",
      "Epoch: 63/200 Iteration: 5435 Train loss: 1.189735 Train acc: 0.344238\n",
      "Epoch: 63/200 Iteration: 5440 Train loss: 1.209211 Train acc: 0.231445\n",
      "Epoch: 64/200 Iteration: 5445 Train loss: 1.062508 Train acc: 0.503418\n",
      "Epoch: 64/200 Iteration: 5450 Train loss: 1.094102 Train acc: 0.399902\n",
      "Epoch: 64/200 Iteration: 5455 Train loss: 1.134052 Train acc: 0.348633\n",
      "Epoch: 64/200 Iteration: 5460 Train loss: 1.094023 Train acc: 0.410156\n",
      "Epoch: 64/200 Iteration: 5465 Train loss: 1.142473 Train acc: 0.312988\n",
      "Epoch: 64/200 Iteration: 5470 Train loss: 1.057490 Train acc: 0.468262\n",
      "Epoch: 64/200 Iteration: 5475 Train loss: 0.991613 Train acc: 0.579102\n",
      "Epoch: 64/200 Iteration: 5480 Train loss: 0.992773 Train acc: 0.584961\n",
      "Epoch: 64/200 Iteration: 5485 Train loss: 1.127269 Train acc: 0.278320\n",
      "Epoch: 64/200 Iteration: 5490 Train loss: 0.976740 Train acc: 0.631836\n",
      "Epoch: 64/200 Iteration: 5495 Train loss: 1.059385 Train acc: 0.494629\n",
      "Epoch: 64/200 Iteration: 5500 Train loss: 1.132952 Train acc: 0.248047\n",
      "Epoch: 64/200 Iteration: 5505 Train loss: 1.082101 Train acc: 0.456543\n",
      "Epoch: 64/200 Iteration: 5510 Train loss: 1.086657 Train acc: 0.475586\n",
      "Epoch: 64/200 Iteration: 5515 Train loss: 1.025072 Train acc: 0.412598\n",
      "Epoch: 64/200 Iteration: 5520 Train loss: 1.166756 Train acc: 0.334961\n",
      "Epoch: 64/200 Iteration: 5525 Train loss: 1.188801 Train acc: 0.231445\n",
      "Epoch: 65/200 Iteration: 5530 Train loss: 1.049947 Train acc: 0.503418\n",
      "Epoch: 65/200 Iteration: 5535 Train loss: 1.093620 Train acc: 0.399902\n",
      "Epoch: 65/200 Iteration: 5540 Train loss: 1.125027 Train acc: 0.348633\n",
      "Epoch: 65/200 Iteration: 5545 Train loss: 1.089580 Train acc: 0.410156\n",
      "Epoch: 65/200 Iteration: 5550 Train loss: 1.142010 Train acc: 0.312988\n",
      "Epoch: 65/200 Iteration: 5555 Train loss: 1.062139 Train acc: 0.467773\n",
      "Epoch: 65/200 Iteration: 5560 Train loss: 1.000137 Train acc: 0.579102\n",
      "Epoch: 65/200 Iteration: 5565 Train loss: 0.996738 Train acc: 0.584961\n",
      "Epoch: 65/200 Iteration: 5570 Train loss: 1.140695 Train acc: 0.278320\n",
      "Epoch: 65/200 Iteration: 5575 Train loss: 0.976536 Train acc: 0.631836\n",
      "Epoch: 65/200 Iteration: 5580 Train loss: 1.054827 Train acc: 0.494629\n",
      "Epoch: 65/200 Iteration: 5585 Train loss: 1.093905 Train acc: 0.406250\n",
      "Epoch: 65/200 Iteration: 5590 Train loss: 1.183301 Train acc: 0.212891\n",
      "Epoch: 65/200 Iteration: 5595 Train loss: 1.106577 Train acc: 0.320312\n",
      "Epoch: 65/200 Iteration: 5600 Train loss: 1.005520 Train acc: 0.472656\n",
      "Epoch: 65/200 Iteration: 5605 Train loss: 1.193086 Train acc: 0.339844\n",
      "Epoch: 65/200 Iteration: 5610 Train loss: 1.204967 Train acc: 0.234863\n",
      "Epoch: 66/200 Iteration: 5615 Train loss: 1.068469 Train acc: 0.501465\n",
      "Epoch: 66/200 Iteration: 5620 Train loss: 1.093009 Train acc: 0.399902\n",
      "Epoch: 66/200 Iteration: 5625 Train loss: 1.137403 Train acc: 0.348633\n",
      "Epoch: 66/200 Iteration: 5630 Train loss: 1.093090 Train acc: 0.410156\n",
      "Epoch: 66/200 Iteration: 5635 Train loss: 1.142234 Train acc: 0.312988\n",
      "Epoch: 66/200 Iteration: 5640 Train loss: 1.060170 Train acc: 0.467773\n",
      "Epoch: 66/200 Iteration: 5645 Train loss: 1.000210 Train acc: 0.579102\n",
      "Epoch: 66/200 Iteration: 5650 Train loss: 0.998526 Train acc: 0.584961\n",
      "Epoch: 66/200 Iteration: 5655 Train loss: 1.124541 Train acc: 0.278320\n",
      "Epoch: 66/200 Iteration: 5660 Train loss: 0.980576 Train acc: 0.631836\n",
      "Epoch: 66/200 Iteration: 5665 Train loss: 1.059026 Train acc: 0.494629\n",
      "Epoch: 66/200 Iteration: 5670 Train loss: 1.125497 Train acc: 0.254395\n",
      "Epoch: 66/200 Iteration: 5675 Train loss: 1.083895 Train acc: 0.449707\n",
      "Epoch: 66/200 Iteration: 5680 Train loss: 1.087382 Train acc: 0.474121\n",
      "Epoch: 66/200 Iteration: 5685 Train loss: 1.028552 Train acc: 0.412598\n",
      "Epoch: 66/200 Iteration: 5690 Train loss: 1.165161 Train acc: 0.305664\n",
      "Epoch: 66/200 Iteration: 5695 Train loss: 1.189285 Train acc: 0.231445\n",
      "Epoch: 67/200 Iteration: 5700 Train loss: 1.052744 Train acc: 0.503418\n",
      "Epoch: 67/200 Iteration: 5705 Train loss: 1.093568 Train acc: 0.399902\n",
      "Epoch: 67/200 Iteration: 5710 Train loss: 1.138750 Train acc: 0.348633\n",
      "Epoch: 67/200 Iteration: 5715 Train loss: 1.092427 Train acc: 0.410156\n",
      "Epoch: 67/200 Iteration: 5720 Train loss: 1.149194 Train acc: 0.312988\n",
      "Epoch: 67/200 Iteration: 5725 Train loss: 1.059765 Train acc: 0.467773\n",
      "Epoch: 67/200 Iteration: 5730 Train loss: 0.999386 Train acc: 0.579102\n",
      "Epoch: 67/200 Iteration: 5735 Train loss: 0.997514 Train acc: 0.584961\n",
      "Epoch: 67/200 Iteration: 5740 Train loss: 1.128955 Train acc: 0.278320\n",
      "Epoch: 67/200 Iteration: 5745 Train loss: 0.978794 Train acc: 0.631836\n",
      "Epoch: 67/200 Iteration: 5750 Train loss: 1.057824 Train acc: 0.494629\n",
      "Epoch: 67/200 Iteration: 5755 Train loss: 1.119323 Train acc: 0.241699\n",
      "Epoch: 67/200 Iteration: 5760 Train loss: 1.085320 Train acc: 0.426270\n",
      "Epoch: 67/200 Iteration: 5765 Train loss: 1.097781 Train acc: 0.420410\n",
      "Epoch: 67/200 Iteration: 5770 Train loss: 1.015802 Train acc: 0.444824\n",
      "Epoch: 67/200 Iteration: 5775 Train loss: 1.179000 Train acc: 0.341797\n",
      "Epoch: 67/200 Iteration: 5780 Train loss: 1.189876 Train acc: 0.233398\n",
      "Epoch: 68/200 Iteration: 5785 Train loss: 1.050780 Train acc: 0.503418\n",
      "Epoch: 68/200 Iteration: 5790 Train loss: 1.092272 Train acc: 0.399902\n",
      "Epoch: 68/200 Iteration: 5795 Train loss: 1.135643 Train acc: 0.348633\n",
      "Epoch: 68/200 Iteration: 5800 Train loss: 1.090478 Train acc: 0.410156\n",
      "Epoch: 68/200 Iteration: 5805 Train loss: 1.147094 Train acc: 0.312988\n",
      "Epoch: 68/200 Iteration: 5810 Train loss: 1.059817 Train acc: 0.467773\n",
      "Epoch: 68/200 Iteration: 5815 Train loss: 0.999421 Train acc: 0.579102\n",
      "Epoch: 68/200 Iteration: 5820 Train loss: 0.997830 Train acc: 0.584961\n",
      "Epoch: 68/200 Iteration: 5825 Train loss: 1.135710 Train acc: 0.278320\n",
      "Epoch: 68/200 Iteration: 5830 Train loss: 0.978310 Train acc: 0.631836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68/200 Iteration: 5835 Train loss: 1.053609 Train acc: 0.494629\n",
      "Epoch: 68/200 Iteration: 5840 Train loss: 1.096920 Train acc: 0.353027\n",
      "Epoch: 68/200 Iteration: 5845 Train loss: 1.095365 Train acc: 0.412598\n",
      "Epoch: 68/200 Iteration: 5850 Train loss: 1.096771 Train acc: 0.459473\n",
      "Epoch: 68/200 Iteration: 5855 Train loss: 1.008097 Train acc: 0.449707\n",
      "Epoch: 68/200 Iteration: 5860 Train loss: 1.190462 Train acc: 0.335938\n",
      "Epoch: 68/200 Iteration: 5865 Train loss: 1.208320 Train acc: 0.233398\n",
      "Epoch: 69/200 Iteration: 5870 Train loss: 1.061309 Train acc: 0.503418\n",
      "Epoch: 69/200 Iteration: 5875 Train loss: 1.092623 Train acc: 0.400391\n",
      "Epoch: 69/200 Iteration: 5880 Train loss: 1.137471 Train acc: 0.348633\n",
      "Epoch: 69/200 Iteration: 5885 Train loss: 1.094589 Train acc: 0.410156\n",
      "Epoch: 69/200 Iteration: 5890 Train loss: 1.148555 Train acc: 0.312988\n",
      "Epoch: 69/200 Iteration: 5895 Train loss: 1.060305 Train acc: 0.467773\n",
      "Epoch: 69/200 Iteration: 5900 Train loss: 0.998042 Train acc: 0.579102\n",
      "Epoch: 69/200 Iteration: 5905 Train loss: 0.994841 Train acc: 0.584961\n",
      "Epoch: 69/200 Iteration: 5910 Train loss: 1.128124 Train acc: 0.278320\n",
      "Epoch: 69/200 Iteration: 5915 Train loss: 0.978167 Train acc: 0.631836\n",
      "Epoch: 69/200 Iteration: 5920 Train loss: 1.057644 Train acc: 0.494629\n",
      "Epoch: 69/200 Iteration: 5925 Train loss: 1.129449 Train acc: 0.252930\n",
      "Epoch: 69/200 Iteration: 5930 Train loss: 1.076114 Train acc: 0.456055\n",
      "Epoch: 69/200 Iteration: 5935 Train loss: 1.092101 Train acc: 0.470215\n",
      "Epoch: 69/200 Iteration: 5940 Train loss: 1.021842 Train acc: 0.413086\n",
      "Epoch: 69/200 Iteration: 5945 Train loss: 1.168082 Train acc: 0.332031\n",
      "Epoch: 69/200 Iteration: 5950 Train loss: 1.182512 Train acc: 0.231445\n",
      "Epoch: 70/200 Iteration: 5955 Train loss: 1.049673 Train acc: 0.503418\n",
      "Epoch: 70/200 Iteration: 5960 Train loss: 1.093392 Train acc: 0.399902\n",
      "Epoch: 70/200 Iteration: 5965 Train loss: 1.134560 Train acc: 0.348633\n",
      "Epoch: 70/200 Iteration: 5970 Train loss: 1.089890 Train acc: 0.410156\n",
      "Epoch: 70/200 Iteration: 5975 Train loss: 1.148132 Train acc: 0.312988\n",
      "Epoch: 70/200 Iteration: 5980 Train loss: 1.059577 Train acc: 0.467773\n",
      "Epoch: 70/200 Iteration: 5985 Train loss: 1.000720 Train acc: 0.579102\n",
      "Epoch: 70/200 Iteration: 5990 Train loss: 0.997960 Train acc: 0.584961\n",
      "Epoch: 70/200 Iteration: 5995 Train loss: 1.137569 Train acc: 0.278320\n",
      "Epoch: 70/200 Iteration: 6000 Train loss: 0.978706 Train acc: 0.631836\n",
      "Epoch: 70/200 Iteration: 6005 Train loss: 1.052853 Train acc: 0.494629\n",
      "Epoch: 70/200 Iteration: 6010 Train loss: 1.093990 Train acc: 0.397949\n",
      "Epoch: 70/200 Iteration: 6015 Train loss: 1.105534 Train acc: 0.326660\n",
      "Epoch: 70/200 Iteration: 6020 Train loss: 1.098475 Train acc: 0.416016\n",
      "Epoch: 70/200 Iteration: 6025 Train loss: 1.003770 Train acc: 0.473145\n",
      "Epoch: 70/200 Iteration: 6030 Train loss: 1.195682 Train acc: 0.343262\n",
      "Epoch: 70/200 Iteration: 6035 Train loss: 1.212899 Train acc: 0.231445\n",
      "Epoch: 71/200 Iteration: 6040 Train loss: 1.064377 Train acc: 0.503418\n",
      "Epoch: 71/200 Iteration: 6045 Train loss: 1.093728 Train acc: 0.399902\n",
      "Epoch: 71/200 Iteration: 6050 Train loss: 1.137003 Train acc: 0.348633\n",
      "Epoch: 71/200 Iteration: 6055 Train loss: 1.094542 Train acc: 0.410156\n",
      "Epoch: 71/200 Iteration: 6060 Train loss: 1.146367 Train acc: 0.312988\n",
      "Epoch: 71/200 Iteration: 6065 Train loss: 1.060900 Train acc: 0.467773\n",
      "Epoch: 71/200 Iteration: 6070 Train loss: 0.996184 Train acc: 0.579102\n",
      "Epoch: 71/200 Iteration: 6075 Train loss: 0.996822 Train acc: 0.584961\n",
      "Epoch: 71/200 Iteration: 6080 Train loss: 1.128845 Train acc: 0.278320\n",
      "Epoch: 71/200 Iteration: 6085 Train loss: 0.976530 Train acc: 0.631836\n",
      "Epoch: 71/200 Iteration: 6090 Train loss: 1.057686 Train acc: 0.494629\n",
      "Epoch: 71/200 Iteration: 6095 Train loss: 1.132919 Train acc: 0.251465\n",
      "Epoch: 71/200 Iteration: 6100 Train loss: 1.071212 Train acc: 0.454590\n",
      "Epoch: 71/200 Iteration: 6105 Train loss: 1.086157 Train acc: 0.477051\n",
      "Epoch: 71/200 Iteration: 6110 Train loss: 1.025665 Train acc: 0.411621\n",
      "Epoch: 71/200 Iteration: 6115 Train loss: 1.165448 Train acc: 0.312988\n",
      "Epoch: 71/200 Iteration: 6120 Train loss: 1.178088 Train acc: 0.230957\n",
      "Epoch: 72/200 Iteration: 6125 Train loss: 1.049759 Train acc: 0.503418\n",
      "Epoch: 72/200 Iteration: 6130 Train loss: 1.093021 Train acc: 0.399902\n",
      "Epoch: 72/200 Iteration: 6135 Train loss: 1.134376 Train acc: 0.348633\n",
      "Epoch: 72/200 Iteration: 6140 Train loss: 1.089159 Train acc: 0.410156\n",
      "Epoch: 72/200 Iteration: 6145 Train loss: 1.147445 Train acc: 0.312988\n",
      "Epoch: 72/200 Iteration: 6150 Train loss: 1.059210 Train acc: 0.467773\n",
      "Epoch: 72/200 Iteration: 6155 Train loss: 1.001686 Train acc: 0.579102\n",
      "Epoch: 72/200 Iteration: 6160 Train loss: 0.998358 Train acc: 0.584961\n",
      "Epoch: 72/200 Iteration: 6165 Train loss: 1.138612 Train acc: 0.278320\n",
      "Epoch: 72/200 Iteration: 6170 Train loss: 0.979282 Train acc: 0.631836\n",
      "Epoch: 72/200 Iteration: 6175 Train loss: 1.052866 Train acc: 0.494629\n",
      "Epoch: 72/200 Iteration: 6180 Train loss: 1.093895 Train acc: 0.407715\n",
      "Epoch: 72/200 Iteration: 6185 Train loss: 1.106141 Train acc: 0.307617\n",
      "Epoch: 72/200 Iteration: 6190 Train loss: 1.099392 Train acc: 0.385254\n",
      "Epoch: 72/200 Iteration: 6195 Train loss: 1.005019 Train acc: 0.453613\n",
      "Epoch: 72/200 Iteration: 6200 Train loss: 1.192909 Train acc: 0.348633\n",
      "Epoch: 72/200 Iteration: 6205 Train loss: 1.215234 Train acc: 0.232422\n",
      "Epoch: 73/200 Iteration: 6210 Train loss: 1.064613 Train acc: 0.503418\n",
      "Epoch: 73/200 Iteration: 6215 Train loss: 1.093937 Train acc: 0.400391\n",
      "Epoch: 73/200 Iteration: 6220 Train loss: 1.136611 Train acc: 0.348633\n",
      "Epoch: 73/200 Iteration: 6225 Train loss: 1.094748 Train acc: 0.410156\n",
      "Epoch: 73/200 Iteration: 6230 Train loss: 1.146689 Train acc: 0.312988\n",
      "Epoch: 73/200 Iteration: 6235 Train loss: 1.058453 Train acc: 0.467773\n",
      "Epoch: 73/200 Iteration: 6240 Train loss: 0.996983 Train acc: 0.579102\n",
      "Epoch: 73/200 Iteration: 6245 Train loss: 0.996349 Train acc: 0.584961\n",
      "Epoch: 73/200 Iteration: 6250 Train loss: 1.128875 Train acc: 0.278320\n",
      "Epoch: 73/200 Iteration: 6255 Train loss: 0.976928 Train acc: 0.631836\n",
      "Epoch: 73/200 Iteration: 6260 Train loss: 1.057852 Train acc: 0.494629\n",
      "Epoch: 73/200 Iteration: 6265 Train loss: 1.131996 Train acc: 0.253418\n",
      "Epoch: 73/200 Iteration: 6270 Train loss: 1.072756 Train acc: 0.457520\n",
      "Epoch: 73/200 Iteration: 6275 Train loss: 1.088816 Train acc: 0.475098\n",
      "Epoch: 73/200 Iteration: 6280 Train loss: 1.024563 Train acc: 0.411133\n",
      "Epoch: 73/200 Iteration: 6285 Train loss: 1.167836 Train acc: 0.311035\n",
      "Epoch: 73/200 Iteration: 6290 Train loss: 1.181624 Train acc: 0.232422\n",
      "Epoch: 74/200 Iteration: 6295 Train loss: 1.048824 Train acc: 0.503418\n",
      "Epoch: 74/200 Iteration: 6300 Train loss: 1.092462 Train acc: 0.399902\n",
      "Epoch: 74/200 Iteration: 6305 Train loss: 1.135525 Train acc: 0.348633\n",
      "Epoch: 74/200 Iteration: 6310 Train loss: 1.089124 Train acc: 0.410156\n",
      "Epoch: 74/200 Iteration: 6315 Train loss: 1.148675 Train acc: 0.312988\n",
      "Epoch: 74/200 Iteration: 6320 Train loss: 1.060947 Train acc: 0.467773\n",
      "Epoch: 74/200 Iteration: 6325 Train loss: 1.001667 Train acc: 0.579102\n",
      "Epoch: 74/200 Iteration: 6330 Train loss: 0.997937 Train acc: 0.584961\n",
      "Epoch: 74/200 Iteration: 6335 Train loss: 1.138097 Train acc: 0.278320\n",
      "Epoch: 74/200 Iteration: 6340 Train loss: 0.978018 Train acc: 0.631836\n",
      "Epoch: 74/200 Iteration: 6345 Train loss: 1.052516 Train acc: 0.494629\n",
      "Epoch: 74/200 Iteration: 6350 Train loss: 1.092273 Train acc: 0.407715\n",
      "Epoch: 74/200 Iteration: 6355 Train loss: 1.104855 Train acc: 0.331055\n",
      "Epoch: 74/200 Iteration: 6360 Train loss: 1.101216 Train acc: 0.403809\n",
      "Epoch: 74/200 Iteration: 6365 Train loss: 1.005986 Train acc: 0.474121\n",
      "Epoch: 74/200 Iteration: 6370 Train loss: 1.193112 Train acc: 0.351074\n",
      "Epoch: 74/200 Iteration: 6375 Train loss: 1.212662 Train acc: 0.231934\n",
      "Epoch: 75/200 Iteration: 6380 Train loss: 1.064031 Train acc: 0.503418\n",
      "Epoch: 75/200 Iteration: 6385 Train loss: 1.094136 Train acc: 0.399902\n",
      "Epoch: 75/200 Iteration: 6390 Train loss: 1.138564 Train acc: 0.348633\n",
      "Epoch: 75/200 Iteration: 6395 Train loss: 1.094187 Train acc: 0.410156\n",
      "Epoch: 75/200 Iteration: 6400 Train loss: 1.145820 Train acc: 0.312988\n",
      "Epoch: 75/200 Iteration: 6405 Train loss: 1.058919 Train acc: 0.467773\n",
      "Epoch: 75/200 Iteration: 6410 Train loss: 0.997309 Train acc: 0.579102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75/200 Iteration: 6415 Train loss: 0.996717 Train acc: 0.584961\n",
      "Epoch: 75/200 Iteration: 6420 Train loss: 1.128505 Train acc: 0.278320\n",
      "Epoch: 75/200 Iteration: 6425 Train loss: 0.977673 Train acc: 0.631836\n",
      "Epoch: 75/200 Iteration: 6430 Train loss: 1.057540 Train acc: 0.494629\n",
      "Epoch: 75/200 Iteration: 6435 Train loss: 1.132098 Train acc: 0.250000\n",
      "Epoch: 75/200 Iteration: 6440 Train loss: 1.069657 Train acc: 0.471191\n",
      "Epoch: 75/200 Iteration: 6445 Train loss: 1.087151 Train acc: 0.475586\n",
      "Epoch: 75/200 Iteration: 6450 Train loss: 1.022968 Train acc: 0.411133\n",
      "Epoch: 75/200 Iteration: 6455 Train loss: 1.167232 Train acc: 0.321289\n",
      "Epoch: 75/200 Iteration: 6460 Train loss: 1.182002 Train acc: 0.232422\n",
      "Epoch: 76/200 Iteration: 6465 Train loss: 1.049841 Train acc: 0.503418\n",
      "Epoch: 76/200 Iteration: 6470 Train loss: 1.092521 Train acc: 0.399902\n",
      "Epoch: 76/200 Iteration: 6475 Train loss: 1.133791 Train acc: 0.348633\n",
      "Epoch: 76/200 Iteration: 6480 Train loss: 1.089398 Train acc: 0.410156\n",
      "Epoch: 76/200 Iteration: 6485 Train loss: 1.147679 Train acc: 0.312988\n",
      "Epoch: 76/200 Iteration: 6490 Train loss: 1.059004 Train acc: 0.467773\n",
      "Epoch: 76/200 Iteration: 6495 Train loss: 1.000700 Train acc: 0.579102\n",
      "Epoch: 76/200 Iteration: 6500 Train loss: 0.997378 Train acc: 0.584961\n",
      "Epoch: 76/200 Iteration: 6505 Train loss: 1.138779 Train acc: 0.278320\n",
      "Epoch: 76/200 Iteration: 6510 Train loss: 0.977320 Train acc: 0.631836\n",
      "Epoch: 76/200 Iteration: 6515 Train loss: 1.052250 Train acc: 0.494629\n",
      "Epoch: 76/200 Iteration: 6520 Train loss: 1.091659 Train acc: 0.412598\n",
      "Epoch: 76/200 Iteration: 6525 Train loss: 1.108137 Train acc: 0.299805\n",
      "Epoch: 76/200 Iteration: 6530 Train loss: 1.101771 Train acc: 0.381348\n",
      "Epoch: 76/200 Iteration: 6535 Train loss: 1.003425 Train acc: 0.481445\n",
      "Epoch: 76/200 Iteration: 6540 Train loss: 1.188537 Train acc: 0.346680\n",
      "Epoch: 76/200 Iteration: 6545 Train loss: 1.216291 Train acc: 0.236816\n",
      "Epoch: 77/200 Iteration: 6550 Train loss: 1.066633 Train acc: 0.503418\n",
      "Epoch: 77/200 Iteration: 6555 Train loss: 1.094548 Train acc: 0.400391\n",
      "Epoch: 77/200 Iteration: 6560 Train loss: 1.139360 Train acc: 0.348633\n",
      "Epoch: 77/200 Iteration: 6565 Train loss: 1.094493 Train acc: 0.410156\n",
      "Epoch: 77/200 Iteration: 6570 Train loss: 1.144454 Train acc: 0.312988\n",
      "Epoch: 77/200 Iteration: 6575 Train loss: 1.058406 Train acc: 0.467773\n",
      "Epoch: 77/200 Iteration: 6580 Train loss: 0.997649 Train acc: 0.579102\n",
      "Epoch: 77/200 Iteration: 6585 Train loss: 0.996296 Train acc: 0.584961\n",
      "Epoch: 77/200 Iteration: 6590 Train loss: 1.127371 Train acc: 0.278320\n",
      "Epoch: 77/200 Iteration: 6595 Train loss: 0.976989 Train acc: 0.631836\n",
      "Epoch: 77/200 Iteration: 6600 Train loss: 1.058862 Train acc: 0.494629\n",
      "Epoch: 77/200 Iteration: 6605 Train loss: 1.139621 Train acc: 0.250000\n",
      "Epoch: 77/200 Iteration: 6610 Train loss: 1.071361 Train acc: 0.469727\n",
      "Epoch: 77/200 Iteration: 6615 Train loss: 1.090047 Train acc: 0.472656\n",
      "Epoch: 77/200 Iteration: 6620 Train loss: 1.025339 Train acc: 0.411133\n",
      "Epoch: 77/200 Iteration: 6625 Train loss: 1.162917 Train acc: 0.308105\n",
      "Epoch: 77/200 Iteration: 6630 Train loss: 1.174394 Train acc: 0.232910\n",
      "Epoch: 78/200 Iteration: 6635 Train loss: 1.047733 Train acc: 0.503418\n",
      "Epoch: 78/200 Iteration: 6640 Train loss: 1.092041 Train acc: 0.399902\n",
      "Epoch: 78/200 Iteration: 6645 Train loss: 1.133114 Train acc: 0.348633\n",
      "Epoch: 78/200 Iteration: 6650 Train loss: 1.088857 Train acc: 0.410156\n",
      "Epoch: 78/200 Iteration: 6655 Train loss: 1.146455 Train acc: 0.312988\n",
      "Epoch: 78/200 Iteration: 6660 Train loss: 1.060910 Train acc: 0.467773\n",
      "Epoch: 78/200 Iteration: 6665 Train loss: 1.001911 Train acc: 0.579102\n",
      "Epoch: 78/200 Iteration: 6670 Train loss: 0.998159 Train acc: 0.584961\n",
      "Epoch: 78/200 Iteration: 6675 Train loss: 1.138652 Train acc: 0.278320\n",
      "Epoch: 78/200 Iteration: 6680 Train loss: 0.978083 Train acc: 0.631836\n",
      "Epoch: 78/200 Iteration: 6685 Train loss: 1.052099 Train acc: 0.494629\n",
      "Epoch: 78/200 Iteration: 6690 Train loss: 1.090476 Train acc: 0.403320\n",
      "Epoch: 78/200 Iteration: 6695 Train loss: 1.117642 Train acc: 0.255371\n",
      "Epoch: 78/200 Iteration: 6700 Train loss: 1.100149 Train acc: 0.430664\n",
      "Epoch: 78/200 Iteration: 6705 Train loss: 1.006469 Train acc: 0.466797\n",
      "Epoch: 78/200 Iteration: 6710 Train loss: 1.193026 Train acc: 0.349609\n",
      "Epoch: 78/200 Iteration: 6715 Train loss: 1.211922 Train acc: 0.240723\n",
      "Epoch: 79/200 Iteration: 6720 Train loss: 1.071365 Train acc: 0.502441\n",
      "Epoch: 79/200 Iteration: 6725 Train loss: 1.094886 Train acc: 0.402344\n",
      "Epoch: 79/200 Iteration: 6730 Train loss: 1.137707 Train acc: 0.348633\n",
      "Epoch: 79/200 Iteration: 6735 Train loss: 1.095821 Train acc: 0.410156\n",
      "Epoch: 79/200 Iteration: 6740 Train loss: 1.145678 Train acc: 0.312988\n",
      "Epoch: 79/200 Iteration: 6745 Train loss: 1.058531 Train acc: 0.467773\n",
      "Epoch: 79/200 Iteration: 6750 Train loss: 0.999500 Train acc: 0.579102\n",
      "Epoch: 79/200 Iteration: 6755 Train loss: 0.994092 Train acc: 0.584961\n",
      "Epoch: 79/200 Iteration: 6760 Train loss: 1.127289 Train acc: 0.278320\n",
      "Epoch: 79/200 Iteration: 6765 Train loss: 0.975513 Train acc: 0.631836\n",
      "Epoch: 79/200 Iteration: 6770 Train loss: 1.059244 Train acc: 0.494629\n",
      "Epoch: 79/200 Iteration: 6775 Train loss: 1.137945 Train acc: 0.250488\n",
      "Epoch: 79/200 Iteration: 6780 Train loss: 1.069843 Train acc: 0.473633\n",
      "Epoch: 79/200 Iteration: 6785 Train loss: 1.087434 Train acc: 0.473633\n",
      "Epoch: 79/200 Iteration: 6790 Train loss: 1.024982 Train acc: 0.412598\n",
      "Epoch: 79/200 Iteration: 6795 Train loss: 1.161846 Train acc: 0.320801\n",
      "Epoch: 79/200 Iteration: 6800 Train loss: 1.168712 Train acc: 0.233887\n",
      "Epoch: 80/200 Iteration: 6805 Train loss: 1.046350 Train acc: 0.503418\n",
      "Epoch: 80/200 Iteration: 6810 Train loss: 1.093377 Train acc: 0.399902\n",
      "Epoch: 80/200 Iteration: 6815 Train loss: 1.136227 Train acc: 0.348633\n",
      "Epoch: 80/200 Iteration: 6820 Train loss: 1.089545 Train acc: 0.410156\n",
      "Epoch: 80/200 Iteration: 6825 Train loss: 1.146720 Train acc: 0.312988\n",
      "Epoch: 80/200 Iteration: 6830 Train loss: 1.060116 Train acc: 0.467773\n",
      "Epoch: 80/200 Iteration: 6835 Train loss: 1.000876 Train acc: 0.579102\n",
      "Epoch: 80/200 Iteration: 6840 Train loss: 0.998728 Train acc: 0.584961\n",
      "Epoch: 80/200 Iteration: 6845 Train loss: 1.139132 Train acc: 0.278320\n",
      "Epoch: 80/200 Iteration: 6850 Train loss: 0.977897 Train acc: 0.631836\n",
      "Epoch: 80/200 Iteration: 6855 Train loss: 1.051977 Train acc: 0.494629\n",
      "Epoch: 80/200 Iteration: 6860 Train loss: 1.090945 Train acc: 0.405762\n",
      "Epoch: 80/200 Iteration: 6865 Train loss: 1.115331 Train acc: 0.276855\n",
      "Epoch: 80/200 Iteration: 6870 Train loss: 1.100938 Train acc: 0.423340\n",
      "Epoch: 80/200 Iteration: 6875 Train loss: 1.006009 Train acc: 0.465332\n",
      "Epoch: 80/200 Iteration: 6880 Train loss: 1.187842 Train acc: 0.340332\n",
      "Epoch: 80/200 Iteration: 6885 Train loss: 1.218198 Train acc: 0.238770\n",
      "Epoch: 81/200 Iteration: 6890 Train loss: 1.070239 Train acc: 0.500488\n",
      "Epoch: 81/200 Iteration: 6895 Train loss: 1.093801 Train acc: 0.402832\n",
      "Epoch: 81/200 Iteration: 6900 Train loss: 1.141365 Train acc: 0.348633\n",
      "Epoch: 81/200 Iteration: 6905 Train loss: 1.094532 Train acc: 0.410156\n",
      "Epoch: 81/200 Iteration: 6910 Train loss: 1.144921 Train acc: 0.312988\n",
      "Epoch: 81/200 Iteration: 6915 Train loss: 1.059306 Train acc: 0.467773\n",
      "Epoch: 81/200 Iteration: 6920 Train loss: 0.999819 Train acc: 0.579102\n",
      "Epoch: 81/200 Iteration: 6925 Train loss: 0.996101 Train acc: 0.584961\n",
      "Epoch: 81/200 Iteration: 6930 Train loss: 1.125985 Train acc: 0.278320\n",
      "Epoch: 81/200 Iteration: 6935 Train loss: 0.975949 Train acc: 0.631836\n",
      "Epoch: 81/200 Iteration: 6940 Train loss: 1.059157 Train acc: 0.494629\n",
      "Epoch: 81/200 Iteration: 6945 Train loss: 1.140437 Train acc: 0.248047\n",
      "Epoch: 81/200 Iteration: 6950 Train loss: 1.072857 Train acc: 0.462891\n",
      "Epoch: 81/200 Iteration: 6955 Train loss: 1.089225 Train acc: 0.465820\n",
      "Epoch: 81/200 Iteration: 6960 Train loss: 1.025417 Train acc: 0.412109\n",
      "Epoch: 81/200 Iteration: 6965 Train loss: 1.162957 Train acc: 0.314941\n",
      "Epoch: 81/200 Iteration: 6970 Train loss: 1.168656 Train acc: 0.232422\n",
      "Epoch: 82/200 Iteration: 6975 Train loss: 1.047434 Train acc: 0.503418\n",
      "Epoch: 82/200 Iteration: 6980 Train loss: 1.092980 Train acc: 0.399902\n",
      "Epoch: 82/200 Iteration: 6985 Train loss: 1.134231 Train acc: 0.348633\n",
      "Epoch: 82/200 Iteration: 6990 Train loss: 1.088776 Train acc: 0.410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82/200 Iteration: 6995 Train loss: 1.145822 Train acc: 0.312988\n",
      "Epoch: 82/200 Iteration: 7000 Train loss: 1.060097 Train acc: 0.467773\n",
      "Epoch: 82/200 Iteration: 7005 Train loss: 1.000972 Train acc: 0.579102\n",
      "Epoch: 82/200 Iteration: 7010 Train loss: 0.999332 Train acc: 0.584961\n",
      "Epoch: 82/200 Iteration: 7015 Train loss: 1.137675 Train acc: 0.278320\n",
      "Epoch: 82/200 Iteration: 7020 Train loss: 0.978275 Train acc: 0.631836\n",
      "Epoch: 82/200 Iteration: 7025 Train loss: 1.052386 Train acc: 0.494629\n",
      "Epoch: 82/200 Iteration: 7030 Train loss: 1.091069 Train acc: 0.407227\n",
      "Epoch: 82/200 Iteration: 7035 Train loss: 1.115640 Train acc: 0.263672\n",
      "Epoch: 82/200 Iteration: 7040 Train loss: 1.098822 Train acc: 0.432129\n",
      "Epoch: 82/200 Iteration: 7045 Train loss: 1.004742 Train acc: 0.479492\n",
      "Epoch: 82/200 Iteration: 7050 Train loss: 1.189731 Train acc: 0.351074\n",
      "Epoch: 82/200 Iteration: 7055 Train loss: 1.214688 Train acc: 0.240723\n",
      "Epoch: 83/200 Iteration: 7060 Train loss: 1.069615 Train acc: 0.502441\n",
      "Epoch: 83/200 Iteration: 7065 Train loss: 1.094547 Train acc: 0.402832\n",
      "Epoch: 83/200 Iteration: 7070 Train loss: 1.138820 Train acc: 0.348633\n",
      "Epoch: 83/200 Iteration: 7075 Train loss: 1.094809 Train acc: 0.410156\n",
      "Epoch: 83/200 Iteration: 7080 Train loss: 1.144650 Train acc: 0.312988\n",
      "Epoch: 83/200 Iteration: 7085 Train loss: 1.059258 Train acc: 0.467773\n",
      "Epoch: 83/200 Iteration: 7090 Train loss: 0.998556 Train acc: 0.579102\n",
      "Epoch: 83/200 Iteration: 7095 Train loss: 0.997731 Train acc: 0.584961\n",
      "Epoch: 83/200 Iteration: 7100 Train loss: 1.126288 Train acc: 0.278320\n",
      "Epoch: 83/200 Iteration: 7105 Train loss: 0.977124 Train acc: 0.631836\n",
      "Epoch: 83/200 Iteration: 7110 Train loss: 1.058692 Train acc: 0.494629\n",
      "Epoch: 83/200 Iteration: 7115 Train loss: 1.135996 Train acc: 0.253906\n",
      "Epoch: 83/200 Iteration: 7120 Train loss: 1.070620 Train acc: 0.462402\n",
      "Epoch: 83/200 Iteration: 7125 Train loss: 1.089961 Train acc: 0.465820\n",
      "Epoch: 83/200 Iteration: 7130 Train loss: 1.023342 Train acc: 0.410645\n",
      "Epoch: 83/200 Iteration: 7135 Train loss: 1.165666 Train acc: 0.308594\n",
      "Epoch: 83/200 Iteration: 7140 Train loss: 1.175008 Train acc: 0.231934\n",
      "Epoch: 84/200 Iteration: 7145 Train loss: 1.048471 Train acc: 0.503418\n",
      "Epoch: 84/200 Iteration: 7150 Train loss: 1.092508 Train acc: 0.399902\n",
      "Epoch: 84/200 Iteration: 7155 Train loss: 1.133732 Train acc: 0.348633\n",
      "Epoch: 84/200 Iteration: 7160 Train loss: 1.089362 Train acc: 0.410156\n",
      "Epoch: 84/200 Iteration: 7165 Train loss: 1.146124 Train acc: 0.312988\n",
      "Epoch: 84/200 Iteration: 7170 Train loss: 1.060057 Train acc: 0.467773\n",
      "Epoch: 84/200 Iteration: 7175 Train loss: 0.999950 Train acc: 0.579102\n",
      "Epoch: 84/200 Iteration: 7180 Train loss: 0.998102 Train acc: 0.584961\n",
      "Epoch: 84/200 Iteration: 7185 Train loss: 1.137144 Train acc: 0.278320\n",
      "Epoch: 84/200 Iteration: 7190 Train loss: 0.978387 Train acc: 0.631836\n",
      "Epoch: 84/200 Iteration: 7195 Train loss: 1.053689 Train acc: 0.494629\n",
      "Epoch: 84/200 Iteration: 7200 Train loss: 1.091570 Train acc: 0.407227\n",
      "Epoch: 84/200 Iteration: 7205 Train loss: 1.111676 Train acc: 0.278809\n",
      "Epoch: 84/200 Iteration: 7210 Train loss: 1.099088 Train acc: 0.416504\n",
      "Epoch: 84/200 Iteration: 7215 Train loss: 1.004959 Train acc: 0.476562\n",
      "Epoch: 84/200 Iteration: 7220 Train loss: 1.190240 Train acc: 0.333984\n",
      "Epoch: 84/200 Iteration: 7225 Train loss: 1.215310 Train acc: 0.243652\n",
      "Epoch: 85/200 Iteration: 7230 Train loss: 1.066821 Train acc: 0.503418\n",
      "Epoch: 85/200 Iteration: 7235 Train loss: 1.094697 Train acc: 0.400879\n",
      "Epoch: 85/200 Iteration: 7240 Train loss: 1.136260 Train acc: 0.348633\n",
      "Epoch: 85/200 Iteration: 7245 Train loss: 1.094649 Train acc: 0.410156\n",
      "Epoch: 85/200 Iteration: 7250 Train loss: 1.144591 Train acc: 0.312988\n",
      "Epoch: 85/200 Iteration: 7255 Train loss: 1.058616 Train acc: 0.467773\n",
      "Epoch: 85/200 Iteration: 7260 Train loss: 1.000136 Train acc: 0.579102\n",
      "Epoch: 85/200 Iteration: 7265 Train loss: 0.996069 Train acc: 0.584961\n",
      "Epoch: 85/200 Iteration: 7270 Train loss: 1.128908 Train acc: 0.278320\n",
      "Epoch: 85/200 Iteration: 7275 Train loss: 0.975313 Train acc: 0.631836\n",
      "Epoch: 85/200 Iteration: 7280 Train loss: 1.058885 Train acc: 0.494629\n",
      "Epoch: 85/200 Iteration: 7285 Train loss: 1.134494 Train acc: 0.250977\n",
      "Epoch: 85/200 Iteration: 7290 Train loss: 1.068894 Train acc: 0.462891\n",
      "Epoch: 85/200 Iteration: 7295 Train loss: 1.091399 Train acc: 0.472168\n",
      "Epoch: 85/200 Iteration: 7300 Train loss: 1.020622 Train acc: 0.412109\n",
      "Epoch: 85/200 Iteration: 7305 Train loss: 1.163455 Train acc: 0.311523\n",
      "Epoch: 85/200 Iteration: 7310 Train loss: 1.182689 Train acc: 0.231445\n",
      "Epoch: 86/200 Iteration: 7315 Train loss: 1.048121 Train acc: 0.503418\n",
      "Epoch: 86/200 Iteration: 7320 Train loss: 1.093589 Train acc: 0.399902\n",
      "Epoch: 86/200 Iteration: 7325 Train loss: 1.135599 Train acc: 0.348633\n",
      "Epoch: 86/200 Iteration: 7330 Train loss: 1.089576 Train acc: 0.410156\n",
      "Epoch: 86/200 Iteration: 7335 Train loss: 1.147565 Train acc: 0.312988\n",
      "Epoch: 86/200 Iteration: 7340 Train loss: 1.059273 Train acc: 0.467773\n",
      "Epoch: 86/200 Iteration: 7345 Train loss: 1.001410 Train acc: 0.579102\n",
      "Epoch: 86/200 Iteration: 7350 Train loss: 0.998036 Train acc: 0.584961\n",
      "Epoch: 86/200 Iteration: 7355 Train loss: 1.137878 Train acc: 0.278320\n",
      "Epoch: 86/200 Iteration: 7360 Train loss: 0.977651 Train acc: 0.631836\n",
      "Epoch: 86/200 Iteration: 7365 Train loss: 1.052619 Train acc: 0.494629\n",
      "Epoch: 86/200 Iteration: 7370 Train loss: 1.094030 Train acc: 0.396973\n",
      "Epoch: 86/200 Iteration: 7375 Train loss: 1.105932 Train acc: 0.325195\n",
      "Epoch: 86/200 Iteration: 7380 Train loss: 1.100041 Train acc: 0.421387\n",
      "Epoch: 86/200 Iteration: 7385 Train loss: 1.007653 Train acc: 0.464355\n",
      "Epoch: 86/200 Iteration: 7390 Train loss: 1.184197 Train acc: 0.339355\n",
      "Epoch: 86/200 Iteration: 7395 Train loss: 1.213688 Train acc: 0.233398\n",
      "Epoch: 87/200 Iteration: 7400 Train loss: 1.064456 Train acc: 0.503418\n",
      "Epoch: 87/200 Iteration: 7405 Train loss: 1.094298 Train acc: 0.399902\n",
      "Epoch: 87/200 Iteration: 7410 Train loss: 1.137889 Train acc: 0.348633\n",
      "Epoch: 87/200 Iteration: 7415 Train loss: 1.094603 Train acc: 0.410156\n",
      "Epoch: 87/200 Iteration: 7420 Train loss: 1.147822 Train acc: 0.312988\n",
      "Epoch: 87/200 Iteration: 7425 Train loss: 1.058575 Train acc: 0.467773\n",
      "Epoch: 87/200 Iteration: 7430 Train loss: 0.998792 Train acc: 0.579102\n",
      "Epoch: 87/200 Iteration: 7435 Train loss: 0.995721 Train acc: 0.584961\n",
      "Epoch: 87/200 Iteration: 7440 Train loss: 1.129446 Train acc: 0.278320\n",
      "Epoch: 87/200 Iteration: 7445 Train loss: 0.975185 Train acc: 0.631836\n",
      "Epoch: 87/200 Iteration: 7450 Train loss: 1.057712 Train acc: 0.494629\n",
      "Epoch: 87/200 Iteration: 7455 Train loss: 1.134791 Train acc: 0.250000\n",
      "Epoch: 87/200 Iteration: 7460 Train loss: 1.070245 Train acc: 0.458496\n",
      "Epoch: 87/200 Iteration: 7465 Train loss: 1.090484 Train acc: 0.467773\n",
      "Epoch: 87/200 Iteration: 7470 Train loss: 1.021535 Train acc: 0.412109\n",
      "Epoch: 87/200 Iteration: 7475 Train loss: 1.169557 Train acc: 0.312988\n",
      "Epoch: 87/200 Iteration: 7480 Train loss: 1.175364 Train acc: 0.232910\n",
      "Epoch: 88/200 Iteration: 7485 Train loss: 1.048572 Train acc: 0.503418\n",
      "Epoch: 88/200 Iteration: 7490 Train loss: 1.092906 Train acc: 0.399902\n",
      "Epoch: 88/200 Iteration: 7495 Train loss: 1.134401 Train acc: 0.348633\n",
      "Epoch: 88/200 Iteration: 7500 Train loss: 1.089491 Train acc: 0.410156\n",
      "Epoch: 88/200 Iteration: 7505 Train loss: 1.146600 Train acc: 0.312988\n",
      "Epoch: 88/200 Iteration: 7510 Train loss: 1.058785 Train acc: 0.467773\n",
      "Epoch: 88/200 Iteration: 7515 Train loss: 1.001277 Train acc: 0.579102\n",
      "Epoch: 88/200 Iteration: 7520 Train loss: 0.999819 Train acc: 0.584961\n",
      "Epoch: 88/200 Iteration: 7525 Train loss: 1.139393 Train acc: 0.278320\n",
      "Epoch: 88/200 Iteration: 7530 Train loss: 0.978779 Train acc: 0.631836\n",
      "Epoch: 88/200 Iteration: 7535 Train loss: 1.051992 Train acc: 0.494629\n",
      "Epoch: 88/200 Iteration: 7540 Train loss: 1.090452 Train acc: 0.401367\n",
      "Epoch: 88/200 Iteration: 7545 Train loss: 1.113934 Train acc: 0.298340\n",
      "Epoch: 88/200 Iteration: 7550 Train loss: 1.100473 Train acc: 0.418457\n",
      "Epoch: 88/200 Iteration: 7555 Train loss: 1.005198 Train acc: 0.469238\n",
      "Epoch: 88/200 Iteration: 7560 Train loss: 1.195120 Train acc: 0.346191\n",
      "Epoch: 88/200 Iteration: 7565 Train loss: 1.212268 Train acc: 0.235840\n",
      "Epoch: 89/200 Iteration: 7570 Train loss: 1.067092 Train acc: 0.502930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/200 Iteration: 7575 Train loss: 1.093423 Train acc: 0.400391\n",
      "Epoch: 89/200 Iteration: 7580 Train loss: 1.138151 Train acc: 0.348633\n",
      "Epoch: 89/200 Iteration: 7585 Train loss: 1.094204 Train acc: 0.410156\n",
      "Epoch: 89/200 Iteration: 7590 Train loss: 1.145625 Train acc: 0.312988\n",
      "Epoch: 89/200 Iteration: 7595 Train loss: 1.058390 Train acc: 0.467773\n",
      "Epoch: 89/200 Iteration: 7600 Train loss: 0.999509 Train acc: 0.579102\n",
      "Epoch: 89/200 Iteration: 7605 Train loss: 0.996934 Train acc: 0.584961\n",
      "Epoch: 89/200 Iteration: 7610 Train loss: 1.128950 Train acc: 0.278320\n",
      "Epoch: 89/200 Iteration: 7615 Train loss: 0.976489 Train acc: 0.631836\n",
      "Epoch: 89/200 Iteration: 7620 Train loss: 1.057931 Train acc: 0.494629\n",
      "Epoch: 89/200 Iteration: 7625 Train loss: 1.136552 Train acc: 0.249512\n",
      "Epoch: 89/200 Iteration: 7630 Train loss: 1.070194 Train acc: 0.465332\n",
      "Epoch: 89/200 Iteration: 7635 Train loss: 1.088458 Train acc: 0.476074\n",
      "Epoch: 89/200 Iteration: 7640 Train loss: 1.021191 Train acc: 0.411133\n",
      "Epoch: 89/200 Iteration: 7645 Train loss: 1.162346 Train acc: 0.302246\n",
      "Epoch: 89/200 Iteration: 7650 Train loss: 1.182952 Train acc: 0.231934\n",
      "Epoch: 90/200 Iteration: 7655 Train loss: 1.048745 Train acc: 0.503418\n",
      "Epoch: 90/200 Iteration: 7660 Train loss: 1.093605 Train acc: 0.399902\n",
      "Epoch: 90/200 Iteration: 7665 Train loss: 1.135410 Train acc: 0.348633\n",
      "Epoch: 90/200 Iteration: 7670 Train loss: 1.089934 Train acc: 0.410156\n",
      "Epoch: 90/200 Iteration: 7675 Train loss: 1.147542 Train acc: 0.312988\n",
      "Epoch: 90/200 Iteration: 7680 Train loss: 1.058776 Train acc: 0.467773\n",
      "Epoch: 90/200 Iteration: 7685 Train loss: 1.001589 Train acc: 0.579102\n",
      "Epoch: 90/200 Iteration: 7690 Train loss: 0.997998 Train acc: 0.584961\n",
      "Epoch: 90/200 Iteration: 7695 Train loss: 1.138852 Train acc: 0.278320\n",
      "Epoch: 90/200 Iteration: 7700 Train loss: 0.978371 Train acc: 0.631836\n",
      "Epoch: 90/200 Iteration: 7705 Train loss: 1.052635 Train acc: 0.494629\n",
      "Epoch: 90/200 Iteration: 7710 Train loss: 1.092707 Train acc: 0.412598\n",
      "Epoch: 90/200 Iteration: 7715 Train loss: 1.106847 Train acc: 0.314941\n",
      "Epoch: 90/200 Iteration: 7720 Train loss: 1.099230 Train acc: 0.436035\n",
      "Epoch: 90/200 Iteration: 7725 Train loss: 1.009301 Train acc: 0.438965\n",
      "Epoch: 90/200 Iteration: 7730 Train loss: 1.190653 Train acc: 0.335449\n",
      "Epoch: 90/200 Iteration: 7735 Train loss: 1.217593 Train acc: 0.232422\n",
      "Epoch: 91/200 Iteration: 7740 Train loss: 1.062459 Train acc: 0.503418\n",
      "Epoch: 91/200 Iteration: 7745 Train loss: 1.095069 Train acc: 0.399902\n",
      "Epoch: 91/200 Iteration: 7750 Train loss: 1.136994 Train acc: 0.348633\n",
      "Epoch: 91/200 Iteration: 7755 Train loss: 1.093926 Train acc: 0.410156\n",
      "Epoch: 91/200 Iteration: 7760 Train loss: 1.147749 Train acc: 0.312988\n",
      "Epoch: 91/200 Iteration: 7765 Train loss: 1.058454 Train acc: 0.467773\n",
      "Epoch: 91/200 Iteration: 7770 Train loss: 0.998999 Train acc: 0.579102\n",
      "Epoch: 91/200 Iteration: 7775 Train loss: 0.995612 Train acc: 0.584961\n",
      "Epoch: 91/200 Iteration: 7780 Train loss: 1.129680 Train acc: 0.278320\n",
      "Epoch: 91/200 Iteration: 7785 Train loss: 0.975405 Train acc: 0.631836\n",
      "Epoch: 91/200 Iteration: 7790 Train loss: 1.057764 Train acc: 0.494629\n",
      "Epoch: 91/200 Iteration: 7795 Train loss: 1.132734 Train acc: 0.251465\n",
      "Epoch: 91/200 Iteration: 7800 Train loss: 1.069232 Train acc: 0.460938\n",
      "Epoch: 91/200 Iteration: 7805 Train loss: 1.090465 Train acc: 0.464844\n",
      "Epoch: 91/200 Iteration: 7810 Train loss: 1.019840 Train acc: 0.412598\n",
      "Epoch: 91/200 Iteration: 7815 Train loss: 1.166850 Train acc: 0.320801\n",
      "Epoch: 91/200 Iteration: 7820 Train loss: 1.172887 Train acc: 0.230957\n",
      "Epoch: 92/200 Iteration: 7825 Train loss: 1.047940 Train acc: 0.503418\n",
      "Epoch: 92/200 Iteration: 7830 Train loss: 1.093666 Train acc: 0.399902\n",
      "Epoch: 92/200 Iteration: 7835 Train loss: 1.134863 Train acc: 0.348633\n",
      "Epoch: 92/200 Iteration: 7840 Train loss: 1.089924 Train acc: 0.410156\n",
      "Epoch: 92/200 Iteration: 7845 Train loss: 1.146519 Train acc: 0.312988\n",
      "Epoch: 92/200 Iteration: 7850 Train loss: 1.060200 Train acc: 0.467773\n",
      "Epoch: 92/200 Iteration: 7855 Train loss: 1.000752 Train acc: 0.579102\n",
      "Epoch: 92/200 Iteration: 7860 Train loss: 0.998616 Train acc: 0.584961\n",
      "Epoch: 92/200 Iteration: 7865 Train loss: 1.138426 Train acc: 0.278320\n",
      "Epoch: 92/200 Iteration: 7870 Train loss: 0.979684 Train acc: 0.631836\n",
      "Epoch: 92/200 Iteration: 7875 Train loss: 1.051453 Train acc: 0.494629\n",
      "Epoch: 92/200 Iteration: 7880 Train loss: 1.090220 Train acc: 0.405762\n",
      "Epoch: 92/200 Iteration: 7885 Train loss: 1.115778 Train acc: 0.270020\n",
      "Epoch: 92/200 Iteration: 7890 Train loss: 1.097550 Train acc: 0.449219\n",
      "Epoch: 92/200 Iteration: 7895 Train loss: 1.006652 Train acc: 0.456055\n",
      "Epoch: 92/200 Iteration: 7900 Train loss: 1.193221 Train acc: 0.347656\n",
      "Epoch: 92/200 Iteration: 7905 Train loss: 1.215325 Train acc: 0.231445\n",
      "Epoch: 93/200 Iteration: 7910 Train loss: 1.065774 Train acc: 0.502441\n",
      "Epoch: 93/200 Iteration: 7915 Train loss: 1.094519 Train acc: 0.400391\n",
      "Epoch: 93/200 Iteration: 7920 Train loss: 1.137833 Train acc: 0.348633\n",
      "Epoch: 93/200 Iteration: 7925 Train loss: 1.093476 Train acc: 0.410156\n",
      "Epoch: 93/200 Iteration: 7930 Train loss: 1.145473 Train acc: 0.312988\n",
      "Epoch: 93/200 Iteration: 7935 Train loss: 1.059498 Train acc: 0.467773\n",
      "Epoch: 93/200 Iteration: 7940 Train loss: 0.999493 Train acc: 0.579102\n",
      "Epoch: 93/200 Iteration: 7945 Train loss: 0.995654 Train acc: 0.584961\n",
      "Epoch: 93/200 Iteration: 7950 Train loss: 1.129192 Train acc: 0.278320\n",
      "Epoch: 93/200 Iteration: 7955 Train loss: 0.975804 Train acc: 0.631836\n",
      "Epoch: 93/200 Iteration: 7960 Train loss: 1.058003 Train acc: 0.494629\n",
      "Epoch: 93/200 Iteration: 7965 Train loss: 1.132482 Train acc: 0.250977\n",
      "Epoch: 93/200 Iteration: 7970 Train loss: 1.070633 Train acc: 0.452148\n",
      "Epoch: 93/200 Iteration: 7975 Train loss: 1.088868 Train acc: 0.471680\n",
      "Epoch: 93/200 Iteration: 7980 Train loss: 1.022492 Train acc: 0.411621\n",
      "Epoch: 93/200 Iteration: 7985 Train loss: 1.164311 Train acc: 0.327148\n",
      "Epoch: 93/200 Iteration: 7990 Train loss: 1.180541 Train acc: 0.232910\n",
      "Epoch: 94/200 Iteration: 7995 Train loss: 1.048881 Train acc: 0.503418\n",
      "Epoch: 94/200 Iteration: 8000 Train loss: 1.094441 Train acc: 0.399902\n",
      "Epoch: 94/200 Iteration: 8005 Train loss: 1.137139 Train acc: 0.348633\n",
      "Epoch: 94/200 Iteration: 8010 Train loss: 1.090355 Train acc: 0.410156\n",
      "Epoch: 94/200 Iteration: 8015 Train loss: 1.145287 Train acc: 0.312988\n",
      "Epoch: 94/200 Iteration: 8020 Train loss: 1.059044 Train acc: 0.467773\n",
      "Epoch: 94/200 Iteration: 8025 Train loss: 1.000761 Train acc: 0.579102\n",
      "Epoch: 94/200 Iteration: 8030 Train loss: 0.998304 Train acc: 0.584961\n",
      "Epoch: 94/200 Iteration: 8035 Train loss: 1.137515 Train acc: 0.278320\n",
      "Epoch: 94/200 Iteration: 8040 Train loss: 0.979679 Train acc: 0.631836\n",
      "Epoch: 94/200 Iteration: 8045 Train loss: 1.052927 Train acc: 0.494629\n",
      "Epoch: 94/200 Iteration: 8050 Train loss: 1.092789 Train acc: 0.408203\n",
      "Epoch: 94/200 Iteration: 8055 Train loss: 1.108934 Train acc: 0.312500\n",
      "Epoch: 94/200 Iteration: 8060 Train loss: 1.100428 Train acc: 0.426758\n",
      "Epoch: 94/200 Iteration: 8065 Train loss: 1.008743 Train acc: 0.440918\n",
      "Epoch: 94/200 Iteration: 8070 Train loss: 1.189538 Train acc: 0.337402\n",
      "Epoch: 94/200 Iteration: 8075 Train loss: 1.216150 Train acc: 0.233398\n",
      "Epoch: 95/200 Iteration: 8080 Train loss: 1.063849 Train acc: 0.503418\n",
      "Epoch: 95/200 Iteration: 8085 Train loss: 1.094128 Train acc: 0.400391\n",
      "Epoch: 95/200 Iteration: 8090 Train loss: 1.139295 Train acc: 0.348633\n",
      "Epoch: 95/200 Iteration: 8095 Train loss: 1.094116 Train acc: 0.410156\n",
      "Epoch: 95/200 Iteration: 8100 Train loss: 1.147155 Train acc: 0.312988\n",
      "Epoch: 95/200 Iteration: 8105 Train loss: 1.060200 Train acc: 0.467773\n",
      "Epoch: 95/200 Iteration: 8110 Train loss: 0.998369 Train acc: 0.579102\n",
      "Epoch: 95/200 Iteration: 8115 Train loss: 0.996277 Train acc: 0.584961\n",
      "Epoch: 95/200 Iteration: 8120 Train loss: 1.130301 Train acc: 0.278320\n",
      "Epoch: 95/200 Iteration: 8125 Train loss: 0.976526 Train acc: 0.631836\n",
      "Epoch: 95/200 Iteration: 8130 Train loss: 1.057499 Train acc: 0.494629\n",
      "Epoch: 95/200 Iteration: 8135 Train loss: 1.132983 Train acc: 0.247070\n",
      "Epoch: 95/200 Iteration: 8140 Train loss: 1.070182 Train acc: 0.455078\n",
      "Epoch: 95/200 Iteration: 8145 Train loss: 1.092551 Train acc: 0.466797\n",
      "Epoch: 95/200 Iteration: 8150 Train loss: 1.020838 Train acc: 0.412109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95/200 Iteration: 8155 Train loss: 1.168108 Train acc: 0.319824\n",
      "Epoch: 95/200 Iteration: 8160 Train loss: 1.180489 Train acc: 0.232910\n",
      "Epoch: 96/200 Iteration: 8165 Train loss: 1.048934 Train acc: 0.503418\n",
      "Epoch: 96/200 Iteration: 8170 Train loss: 1.092334 Train acc: 0.399902\n",
      "Epoch: 96/200 Iteration: 8175 Train loss: 1.133132 Train acc: 0.348633\n",
      "Epoch: 96/200 Iteration: 8180 Train loss: 1.089090 Train acc: 0.410156\n",
      "Epoch: 96/200 Iteration: 8185 Train loss: 1.146079 Train acc: 0.312988\n",
      "Epoch: 96/200 Iteration: 8190 Train loss: 1.059616 Train acc: 0.467773\n",
      "Epoch: 96/200 Iteration: 8195 Train loss: 1.000423 Train acc: 0.579102\n",
      "Epoch: 96/200 Iteration: 8200 Train loss: 0.998471 Train acc: 0.584961\n",
      "Epoch: 96/200 Iteration: 8205 Train loss: 1.138104 Train acc: 0.278320\n",
      "Epoch: 96/200 Iteration: 8210 Train loss: 0.977647 Train acc: 0.631836\n",
      "Epoch: 96/200 Iteration: 8215 Train loss: 1.052620 Train acc: 0.494629\n",
      "Epoch: 96/200 Iteration: 8220 Train loss: 1.093828 Train acc: 0.403320\n",
      "Epoch: 96/200 Iteration: 8225 Train loss: 1.110086 Train acc: 0.304688\n",
      "Epoch: 96/200 Iteration: 8230 Train loss: 1.099886 Train acc: 0.427734\n",
      "Epoch: 96/200 Iteration: 8235 Train loss: 1.006224 Train acc: 0.485352\n",
      "Epoch: 96/200 Iteration: 8240 Train loss: 1.195514 Train acc: 0.347168\n",
      "Epoch: 96/200 Iteration: 8245 Train loss: 1.213702 Train acc: 0.233398\n",
      "Epoch: 97/200 Iteration: 8250 Train loss: 1.064932 Train acc: 0.503418\n",
      "Epoch: 97/200 Iteration: 8255 Train loss: 1.095172 Train acc: 0.400879\n",
      "Epoch: 97/200 Iteration: 8260 Train loss: 1.138331 Train acc: 0.348633\n",
      "Epoch: 97/200 Iteration: 8265 Train loss: 1.094234 Train acc: 0.410156\n",
      "Epoch: 97/200 Iteration: 8270 Train loss: 1.146220 Train acc: 0.312988\n",
      "Epoch: 97/200 Iteration: 8275 Train loss: 1.058718 Train acc: 0.467773\n",
      "Epoch: 97/200 Iteration: 8280 Train loss: 0.997144 Train acc: 0.579102\n",
      "Epoch: 97/200 Iteration: 8285 Train loss: 0.994723 Train acc: 0.584961\n",
      "Epoch: 97/200 Iteration: 8290 Train loss: 1.129945 Train acc: 0.278320\n",
      "Epoch: 97/200 Iteration: 8295 Train loss: 0.973899 Train acc: 0.631836\n",
      "Epoch: 97/200 Iteration: 8300 Train loss: 1.057974 Train acc: 0.494629\n",
      "Epoch: 97/200 Iteration: 8305 Train loss: 1.133473 Train acc: 0.254395\n",
      "Epoch: 97/200 Iteration: 8310 Train loss: 1.066591 Train acc: 0.463867\n",
      "Epoch: 97/200 Iteration: 8315 Train loss: 1.087841 Train acc: 0.473145\n",
      "Epoch: 97/200 Iteration: 8320 Train loss: 1.021947 Train acc: 0.411621\n",
      "Epoch: 97/200 Iteration: 8325 Train loss: 1.169183 Train acc: 0.314941\n",
      "Epoch: 97/200 Iteration: 8330 Train loss: 1.178548 Train acc: 0.232910\n",
      "Epoch: 98/200 Iteration: 8335 Train loss: 1.048442 Train acc: 0.503418\n",
      "Epoch: 98/200 Iteration: 8340 Train loss: 1.093705 Train acc: 0.399902\n",
      "Epoch: 98/200 Iteration: 8345 Train loss: 1.135611 Train acc: 0.348633\n",
      "Epoch: 98/200 Iteration: 8350 Train loss: 1.089527 Train acc: 0.410156\n",
      "Epoch: 98/200 Iteration: 8355 Train loss: 1.144643 Train acc: 0.312988\n",
      "Epoch: 98/200 Iteration: 8360 Train loss: 1.060551 Train acc: 0.467773\n",
      "Epoch: 98/200 Iteration: 8365 Train loss: 1.001090 Train acc: 0.579102\n",
      "Epoch: 98/200 Iteration: 8370 Train loss: 1.000387 Train acc: 0.584961\n",
      "Epoch: 98/200 Iteration: 8375 Train loss: 1.137471 Train acc: 0.278320\n",
      "Epoch: 98/200 Iteration: 8380 Train loss: 0.979787 Train acc: 0.631836\n",
      "Epoch: 98/200 Iteration: 8385 Train loss: 1.053060 Train acc: 0.494629\n",
      "Epoch: 98/200 Iteration: 8390 Train loss: 1.092799 Train acc: 0.403320\n",
      "Epoch: 98/200 Iteration: 8395 Train loss: 1.114461 Train acc: 0.278320\n",
      "Epoch: 98/200 Iteration: 8400 Train loss: 1.101574 Train acc: 0.423340\n",
      "Epoch: 98/200 Iteration: 8405 Train loss: 1.006349 Train acc: 0.467285\n",
      "Epoch: 98/200 Iteration: 8410 Train loss: 1.193011 Train acc: 0.345215\n",
      "Epoch: 98/200 Iteration: 8415 Train loss: 1.211100 Train acc: 0.233887\n",
      "Epoch: 99/200 Iteration: 8420 Train loss: 1.065370 Train acc: 0.503418\n",
      "Epoch: 99/200 Iteration: 8425 Train loss: 1.094028 Train acc: 0.400391\n",
      "Epoch: 99/200 Iteration: 8430 Train loss: 1.137144 Train acc: 0.348633\n",
      "Epoch: 99/200 Iteration: 8435 Train loss: 1.094205 Train acc: 0.410156\n",
      "Epoch: 99/200 Iteration: 8440 Train loss: 1.146069 Train acc: 0.312988\n",
      "Epoch: 99/200 Iteration: 8445 Train loss: 1.059238 Train acc: 0.467773\n",
      "Epoch: 99/200 Iteration: 8450 Train loss: 0.997733 Train acc: 0.579102\n",
      "Epoch: 99/200 Iteration: 8455 Train loss: 0.997349 Train acc: 0.584961\n",
      "Epoch: 99/200 Iteration: 8460 Train loss: 1.129298 Train acc: 0.278320\n",
      "Epoch: 99/200 Iteration: 8465 Train loss: 0.978025 Train acc: 0.631836\n",
      "Epoch: 99/200 Iteration: 8470 Train loss: 1.056979 Train acc: 0.494629\n",
      "Epoch: 99/200 Iteration: 8475 Train loss: 1.134031 Train acc: 0.248535\n",
      "Epoch: 99/200 Iteration: 8480 Train loss: 1.068458 Train acc: 0.465332\n",
      "Epoch: 99/200 Iteration: 8485 Train loss: 1.088325 Train acc: 0.478027\n",
      "Epoch: 99/200 Iteration: 8490 Train loss: 1.022342 Train acc: 0.411621\n",
      "Epoch: 99/200 Iteration: 8495 Train loss: 1.165305 Train acc: 0.314453\n",
      "Epoch: 99/200 Iteration: 8500 Train loss: 1.180354 Train acc: 0.231445\n",
      "Epoch: 100/200 Iteration: 8505 Train loss: 1.048801 Train acc: 0.503418\n",
      "Epoch: 100/200 Iteration: 8510 Train loss: 1.092961 Train acc: 0.399902\n",
      "Epoch: 100/200 Iteration: 8515 Train loss: 1.133926 Train acc: 0.348633\n",
      "Epoch: 100/200 Iteration: 8520 Train loss: 1.088977 Train acc: 0.410156\n",
      "Epoch: 100/200 Iteration: 8525 Train loss: 1.147342 Train acc: 0.312988\n",
      "Epoch: 100/200 Iteration: 8530 Train loss: 1.058730 Train acc: 0.467773\n",
      "Epoch: 100/200 Iteration: 8535 Train loss: 1.001009 Train acc: 0.579102\n",
      "Epoch: 100/200 Iteration: 8540 Train loss: 0.997861 Train acc: 0.584961\n",
      "Epoch: 100/200 Iteration: 8545 Train loss: 1.137851 Train acc: 0.278320\n",
      "Epoch: 100/200 Iteration: 8550 Train loss: 0.977499 Train acc: 0.631836\n",
      "Epoch: 100/200 Iteration: 8555 Train loss: 1.052350 Train acc: 0.494629\n",
      "Epoch: 100/200 Iteration: 8560 Train loss: 1.091522 Train acc: 0.400879\n",
      "Epoch: 100/200 Iteration: 8565 Train loss: 1.110483 Train acc: 0.299316\n",
      "Epoch: 100/200 Iteration: 8570 Train loss: 1.099541 Train acc: 0.427246\n",
      "Epoch: 100/200 Iteration: 8575 Train loss: 1.005058 Train acc: 0.480469\n",
      "Epoch: 100/200 Iteration: 8580 Train loss: 1.194391 Train acc: 0.342285\n",
      "Epoch: 100/200 Iteration: 8585 Train loss: 1.214175 Train acc: 0.235352\n",
      "Epoch: 101/200 Iteration: 8590 Train loss: 1.063963 Train acc: 0.503418\n",
      "Epoch: 101/200 Iteration: 8595 Train loss: 1.093454 Train acc: 0.400391\n",
      "Epoch: 101/200 Iteration: 8600 Train loss: 1.136057 Train acc: 0.348633\n",
      "Epoch: 101/200 Iteration: 8605 Train loss: 1.093782 Train acc: 0.410156\n",
      "Epoch: 101/200 Iteration: 8610 Train loss: 1.146102 Train acc: 0.312988\n",
      "Epoch: 101/200 Iteration: 8615 Train loss: 1.059808 Train acc: 0.467773\n",
      "Epoch: 101/200 Iteration: 8620 Train loss: 0.999897 Train acc: 0.579102\n",
      "Epoch: 101/200 Iteration: 8625 Train loss: 0.996540 Train acc: 0.584961\n",
      "Epoch: 101/200 Iteration: 8630 Train loss: 1.129276 Train acc: 0.278320\n",
      "Epoch: 101/200 Iteration: 8635 Train loss: 0.975993 Train acc: 0.631836\n",
      "Epoch: 101/200 Iteration: 8640 Train loss: 1.057621 Train acc: 0.494629\n",
      "Epoch: 101/200 Iteration: 8645 Train loss: 1.135369 Train acc: 0.254395\n",
      "Epoch: 101/200 Iteration: 8650 Train loss: 1.067496 Train acc: 0.461914\n",
      "Epoch: 101/200 Iteration: 8655 Train loss: 1.088916 Train acc: 0.470215\n",
      "Epoch: 101/200 Iteration: 8660 Train loss: 1.020652 Train acc: 0.411621\n",
      "Epoch: 101/200 Iteration: 8665 Train loss: 1.165831 Train acc: 0.302734\n",
      "Epoch: 101/200 Iteration: 8670 Train loss: 1.182818 Train acc: 0.231445\n",
      "Epoch: 102/200 Iteration: 8675 Train loss: 1.048289 Train acc: 0.503418\n",
      "Epoch: 102/200 Iteration: 8680 Train loss: 1.093479 Train acc: 0.399902\n",
      "Epoch: 102/200 Iteration: 8685 Train loss: 1.135839 Train acc: 0.348633\n",
      "Epoch: 102/200 Iteration: 8690 Train loss: 1.089552 Train acc: 0.410156\n",
      "Epoch: 102/200 Iteration: 8695 Train loss: 1.148070 Train acc: 0.312988\n",
      "Epoch: 102/200 Iteration: 8700 Train loss: 1.058723 Train acc: 0.467773\n",
      "Epoch: 102/200 Iteration: 8705 Train loss: 1.001791 Train acc: 0.579102\n",
      "Epoch: 102/200 Iteration: 8710 Train loss: 0.997263 Train acc: 0.584961\n",
      "Epoch: 102/200 Iteration: 8715 Train loss: 1.138116 Train acc: 0.278320\n",
      "Epoch: 102/200 Iteration: 8720 Train loss: 0.978215 Train acc: 0.631836\n",
      "Epoch: 102/200 Iteration: 8725 Train loss: 1.052271 Train acc: 0.494629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102/200 Iteration: 8730 Train loss: 1.090427 Train acc: 0.402832\n",
      "Epoch: 102/200 Iteration: 8735 Train loss: 1.109934 Train acc: 0.302246\n",
      "Epoch: 102/200 Iteration: 8740 Train loss: 1.100779 Train acc: 0.431152\n",
      "Epoch: 102/200 Iteration: 8745 Train loss: 1.005678 Train acc: 0.460938\n",
      "Epoch: 102/200 Iteration: 8750 Train loss: 1.188576 Train acc: 0.341797\n",
      "Epoch: 102/200 Iteration: 8755 Train loss: 1.215942 Train acc: 0.234375\n",
      "Epoch: 103/200 Iteration: 8760 Train loss: 1.064232 Train acc: 0.503418\n",
      "Epoch: 103/200 Iteration: 8765 Train loss: 1.094014 Train acc: 0.400391\n",
      "Epoch: 103/200 Iteration: 8770 Train loss: 1.136629 Train acc: 0.348633\n",
      "Epoch: 103/200 Iteration: 8775 Train loss: 1.093166 Train acc: 0.410156\n",
      "Epoch: 103/200 Iteration: 8780 Train loss: 1.148197 Train acc: 0.312988\n",
      "Epoch: 103/200 Iteration: 8785 Train loss: 1.058780 Train acc: 0.467773\n",
      "Epoch: 103/200 Iteration: 8790 Train loss: 0.999259 Train acc: 0.579102\n",
      "Epoch: 103/200 Iteration: 8795 Train loss: 0.995882 Train acc: 0.584961\n",
      "Epoch: 103/200 Iteration: 8800 Train loss: 1.130404 Train acc: 0.278320\n",
      "Epoch: 103/200 Iteration: 8805 Train loss: 0.975747 Train acc: 0.631836\n",
      "Epoch: 103/200 Iteration: 8810 Train loss: 1.057159 Train acc: 0.494629\n",
      "Epoch: 103/200 Iteration: 8815 Train loss: 1.134962 Train acc: 0.241699\n",
      "Epoch: 103/200 Iteration: 8820 Train loss: 1.066461 Train acc: 0.462891\n",
      "Epoch: 103/200 Iteration: 8825 Train loss: 1.090162 Train acc: 0.469727\n",
      "Epoch: 103/200 Iteration: 8830 Train loss: 1.021233 Train acc: 0.412598\n",
      "Epoch: 103/200 Iteration: 8835 Train loss: 1.165389 Train acc: 0.316406\n",
      "Epoch: 103/200 Iteration: 8840 Train loss: 1.179099 Train acc: 0.232910\n",
      "Epoch: 104/200 Iteration: 8845 Train loss: 1.048151 Train acc: 0.503418\n",
      "Epoch: 104/200 Iteration: 8850 Train loss: 1.093146 Train acc: 0.399902\n",
      "Epoch: 104/200 Iteration: 8855 Train loss: 1.135098 Train acc: 0.348633\n",
      "Epoch: 104/200 Iteration: 8860 Train loss: 1.089854 Train acc: 0.410156\n",
      "Epoch: 104/200 Iteration: 8865 Train loss: 1.148429 Train acc: 0.312988\n",
      "Epoch: 104/200 Iteration: 8870 Train loss: 1.059273 Train acc: 0.467773\n",
      "Epoch: 104/200 Iteration: 8875 Train loss: 1.001388 Train acc: 0.579102\n",
      "Epoch: 104/200 Iteration: 8880 Train loss: 0.998675 Train acc: 0.584961\n",
      "Epoch: 104/200 Iteration: 8885 Train loss: 1.139872 Train acc: 0.278320\n",
      "Epoch: 104/200 Iteration: 8890 Train loss: 0.978803 Train acc: 0.631836\n",
      "Epoch: 104/200 Iteration: 8895 Train loss: 1.052320 Train acc: 0.494629\n",
      "Epoch: 104/200 Iteration: 8900 Train loss: 1.090532 Train acc: 0.403320\n",
      "Epoch: 104/200 Iteration: 8905 Train loss: 1.114138 Train acc: 0.289062\n",
      "Epoch: 104/200 Iteration: 8910 Train loss: 1.102209 Train acc: 0.424805\n",
      "Epoch: 104/200 Iteration: 8915 Train loss: 1.005419 Train acc: 0.479492\n",
      "Epoch: 104/200 Iteration: 8920 Train loss: 1.193153 Train acc: 0.347168\n",
      "Epoch: 104/200 Iteration: 8925 Train loss: 1.213420 Train acc: 0.236816\n",
      "Epoch: 105/200 Iteration: 8930 Train loss: 1.066293 Train acc: 0.503418\n",
      "Epoch: 105/200 Iteration: 8935 Train loss: 1.094133 Train acc: 0.400879\n",
      "Epoch: 105/200 Iteration: 8940 Train loss: 1.135632 Train acc: 0.348633\n",
      "Epoch: 105/200 Iteration: 8945 Train loss: 1.094270 Train acc: 0.410156\n",
      "Epoch: 105/200 Iteration: 8950 Train loss: 1.146217 Train acc: 0.312988\n",
      "Epoch: 105/200 Iteration: 8955 Train loss: 1.060072 Train acc: 0.467773\n",
      "Epoch: 105/200 Iteration: 8960 Train loss: 0.996878 Train acc: 0.579102\n",
      "Epoch: 105/200 Iteration: 8965 Train loss: 0.996621 Train acc: 0.584961\n",
      "Epoch: 105/200 Iteration: 8970 Train loss: 1.129434 Train acc: 0.278320\n",
      "Epoch: 105/200 Iteration: 8975 Train loss: 0.977926 Train acc: 0.631836\n",
      "Epoch: 105/200 Iteration: 8980 Train loss: 1.057873 Train acc: 0.494629\n",
      "Epoch: 105/200 Iteration: 8985 Train loss: 1.134282 Train acc: 0.247559\n",
      "Epoch: 105/200 Iteration: 8990 Train loss: 1.068130 Train acc: 0.457031\n",
      "Epoch: 105/200 Iteration: 8995 Train loss: 1.089127 Train acc: 0.472168\n",
      "Epoch: 105/200 Iteration: 9000 Train loss: 1.024420 Train acc: 0.412109\n",
      "Epoch: 105/200 Iteration: 9005 Train loss: 1.163712 Train acc: 0.315430\n",
      "Epoch: 105/200 Iteration: 9010 Train loss: 1.184942 Train acc: 0.231445\n",
      "Epoch: 106/200 Iteration: 9015 Train loss: 1.048522 Train acc: 0.503418\n",
      "Epoch: 106/200 Iteration: 9020 Train loss: 1.093311 Train acc: 0.399902\n",
      "Epoch: 106/200 Iteration: 9025 Train loss: 1.134443 Train acc: 0.348633\n",
      "Epoch: 106/200 Iteration: 9030 Train loss: 1.089590 Train acc: 0.410156\n",
      "Epoch: 106/200 Iteration: 9035 Train loss: 1.147976 Train acc: 0.312988\n",
      "Epoch: 106/200 Iteration: 9040 Train loss: 1.060486 Train acc: 0.467773\n",
      "Epoch: 106/200 Iteration: 9045 Train loss: 1.001060 Train acc: 0.579102\n",
      "Epoch: 106/200 Iteration: 9050 Train loss: 0.997902 Train acc: 0.584961\n",
      "Epoch: 106/200 Iteration: 9055 Train loss: 1.139240 Train acc: 0.278320\n",
      "Epoch: 106/200 Iteration: 9060 Train loss: 0.978686 Train acc: 0.631836\n",
      "Epoch: 106/200 Iteration: 9065 Train loss: 1.051927 Train acc: 0.494629\n",
      "Epoch: 106/200 Iteration: 9070 Train loss: 1.092208 Train acc: 0.403809\n",
      "Epoch: 106/200 Iteration: 9075 Train loss: 1.106488 Train acc: 0.325195\n",
      "Epoch: 106/200 Iteration: 9080 Train loss: 1.099411 Train acc: 0.437500\n",
      "Epoch: 106/200 Iteration: 9085 Train loss: 1.005054 Train acc: 0.463867\n",
      "Epoch: 106/200 Iteration: 9090 Train loss: 1.187534 Train acc: 0.345215\n",
      "Epoch: 106/200 Iteration: 9095 Train loss: 1.217285 Train acc: 0.232422\n",
      "Epoch: 107/200 Iteration: 9100 Train loss: 1.060970 Train acc: 0.503418\n",
      "Epoch: 107/200 Iteration: 9105 Train loss: 1.095114 Train acc: 0.400391\n",
      "Epoch: 107/200 Iteration: 9110 Train loss: 1.136588 Train acc: 0.348633\n",
      "Epoch: 107/200 Iteration: 9115 Train loss: 1.093871 Train acc: 0.410156\n",
      "Epoch: 107/200 Iteration: 9120 Train loss: 1.148097 Train acc: 0.312988\n",
      "Epoch: 107/200 Iteration: 9125 Train loss: 1.060758 Train acc: 0.467773\n",
      "Epoch: 107/200 Iteration: 9130 Train loss: 0.997143 Train acc: 0.579102\n",
      "Epoch: 107/200 Iteration: 9135 Train loss: 0.995343 Train acc: 0.584961\n",
      "Epoch: 107/200 Iteration: 9140 Train loss: 1.131013 Train acc: 0.278320\n",
      "Epoch: 107/200 Iteration: 9145 Train loss: 0.977141 Train acc: 0.631836\n",
      "Epoch: 107/200 Iteration: 9150 Train loss: 1.056967 Train acc: 0.494629\n",
      "Epoch: 107/200 Iteration: 9155 Train loss: 1.130027 Train acc: 0.238770\n",
      "Epoch: 107/200 Iteration: 9160 Train loss: 1.070882 Train acc: 0.457520\n",
      "Epoch: 107/200 Iteration: 9165 Train loss: 1.092276 Train acc: 0.466309\n",
      "Epoch: 107/200 Iteration: 9170 Train loss: 1.020006 Train acc: 0.413086\n",
      "Epoch: 107/200 Iteration: 9175 Train loss: 1.170885 Train acc: 0.334473\n",
      "Epoch: 107/200 Iteration: 9180 Train loss: 1.184319 Train acc: 0.231445\n",
      "Epoch: 108/200 Iteration: 9185 Train loss: 1.049608 Train acc: 0.503418\n",
      "Epoch: 108/200 Iteration: 9190 Train loss: 1.092978 Train acc: 0.399902\n",
      "Epoch: 108/200 Iteration: 9195 Train loss: 1.133982 Train acc: 0.348633\n",
      "Epoch: 108/200 Iteration: 9200 Train loss: 1.089094 Train acc: 0.410156\n",
      "Epoch: 108/200 Iteration: 9205 Train loss: 1.148562 Train acc: 0.312988\n",
      "Epoch: 108/200 Iteration: 9210 Train loss: 1.059421 Train acc: 0.467773\n",
      "Epoch: 108/200 Iteration: 9215 Train loss: 1.000598 Train acc: 0.579102\n",
      "Epoch: 108/200 Iteration: 9220 Train loss: 0.997094 Train acc: 0.584961\n",
      "Epoch: 108/200 Iteration: 9225 Train loss: 1.139578 Train acc: 0.278320\n",
      "Epoch: 108/200 Iteration: 9230 Train loss: 0.978101 Train acc: 0.631836\n",
      "Epoch: 108/200 Iteration: 9235 Train loss: 1.051652 Train acc: 0.494629\n",
      "Epoch: 108/200 Iteration: 9240 Train loss: 1.090962 Train acc: 0.401855\n",
      "Epoch: 108/200 Iteration: 9245 Train loss: 1.116199 Train acc: 0.280762\n",
      "Epoch: 108/200 Iteration: 9250 Train loss: 1.100211 Train acc: 0.426270\n",
      "Epoch: 108/200 Iteration: 9255 Train loss: 1.006674 Train acc: 0.462402\n",
      "Epoch: 108/200 Iteration: 9260 Train loss: 1.193136 Train acc: 0.342773\n",
      "Epoch: 108/200 Iteration: 9265 Train loss: 1.211345 Train acc: 0.235352\n",
      "Epoch: 109/200 Iteration: 9270 Train loss: 1.064433 Train acc: 0.503418\n",
      "Epoch: 109/200 Iteration: 9275 Train loss: 1.094093 Train acc: 0.400391\n",
      "Epoch: 109/200 Iteration: 9280 Train loss: 1.134314 Train acc: 0.348633\n",
      "Epoch: 109/200 Iteration: 9285 Train loss: 1.094207 Train acc: 0.410156\n",
      "Epoch: 109/200 Iteration: 9290 Train loss: 1.145350 Train acc: 0.312988\n",
      "Epoch: 109/200 Iteration: 9295 Train loss: 1.059437 Train acc: 0.467773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109/200 Iteration: 9300 Train loss: 0.997919 Train acc: 0.579102\n",
      "Epoch: 109/200 Iteration: 9305 Train loss: 0.996271 Train acc: 0.584961\n",
      "Epoch: 109/200 Iteration: 9310 Train loss: 1.130325 Train acc: 0.278809\n",
      "Epoch: 109/200 Iteration: 9315 Train loss: 0.976997 Train acc: 0.631836\n",
      "Epoch: 109/200 Iteration: 9320 Train loss: 1.057641 Train acc: 0.494629\n",
      "Epoch: 109/200 Iteration: 9325 Train loss: 1.130493 Train acc: 0.247559\n",
      "Epoch: 109/200 Iteration: 9330 Train loss: 1.068834 Train acc: 0.463867\n",
      "Epoch: 109/200 Iteration: 9335 Train loss: 1.088580 Train acc: 0.475586\n",
      "Epoch: 109/200 Iteration: 9340 Train loss: 1.021729 Train acc: 0.411133\n",
      "Epoch: 109/200 Iteration: 9345 Train loss: 1.163698 Train acc: 0.305176\n",
      "Epoch: 109/200 Iteration: 9350 Train loss: 1.189672 Train acc: 0.231934\n",
      "Epoch: 110/200 Iteration: 9355 Train loss: 1.049414 Train acc: 0.503418\n",
      "Epoch: 110/200 Iteration: 9360 Train loss: 1.093988 Train acc: 0.399902\n",
      "Epoch: 110/200 Iteration: 9365 Train loss: 1.136127 Train acc: 0.348633\n",
      "Epoch: 110/200 Iteration: 9370 Train loss: 1.089696 Train acc: 0.410156\n",
      "Epoch: 110/200 Iteration: 9375 Train loss: 1.147594 Train acc: 0.312988\n",
      "Epoch: 110/200 Iteration: 9380 Train loss: 1.059748 Train acc: 0.467773\n",
      "Epoch: 110/200 Iteration: 9385 Train loss: 1.001002 Train acc: 0.579102\n",
      "Epoch: 110/200 Iteration: 9390 Train loss: 0.997035 Train acc: 0.584961\n",
      "Epoch: 110/200 Iteration: 9395 Train loss: 1.138143 Train acc: 0.278320\n",
      "Epoch: 110/200 Iteration: 9400 Train loss: 0.976222 Train acc: 0.631836\n",
      "Epoch: 110/200 Iteration: 9405 Train loss: 1.052876 Train acc: 0.494629\n",
      "Epoch: 110/200 Iteration: 9410 Train loss: 1.094874 Train acc: 0.404297\n",
      "Epoch: 110/200 Iteration: 9415 Train loss: 1.101321 Train acc: 0.354004\n",
      "Epoch: 110/200 Iteration: 9420 Train loss: 1.101550 Train acc: 0.444824\n",
      "Epoch: 110/200 Iteration: 9425 Train loss: 1.008613 Train acc: 0.450684\n",
      "Epoch: 110/200 Iteration: 9430 Train loss: 1.186808 Train acc: 0.346680\n",
      "Epoch: 110/200 Iteration: 9435 Train loss: 1.213841 Train acc: 0.230957\n",
      "Epoch: 111/200 Iteration: 9440 Train loss: 1.059609 Train acc: 0.503418\n",
      "Epoch: 111/200 Iteration: 9445 Train loss: 1.093914 Train acc: 0.399902\n",
      "Epoch: 111/200 Iteration: 9450 Train loss: 1.137529 Train acc: 0.348633\n",
      "Epoch: 111/200 Iteration: 9455 Train loss: 1.093713 Train acc: 0.410156\n",
      "Epoch: 111/200 Iteration: 9460 Train loss: 1.148065 Train acc: 0.312988\n",
      "Epoch: 111/200 Iteration: 9465 Train loss: 1.059455 Train acc: 0.467773\n",
      "Epoch: 111/200 Iteration: 9470 Train loss: 0.997230 Train acc: 0.579102\n",
      "Epoch: 111/200 Iteration: 9475 Train loss: 0.994452 Train acc: 0.584961\n",
      "Epoch: 111/200 Iteration: 9480 Train loss: 1.133067 Train acc: 0.278320\n",
      "Epoch: 111/200 Iteration: 9485 Train loss: 0.976157 Train acc: 0.631836\n",
      "Epoch: 111/200 Iteration: 9490 Train loss: 1.056314 Train acc: 0.494629\n",
      "Epoch: 111/200 Iteration: 9495 Train loss: 1.127324 Train acc: 0.243164\n",
      "Epoch: 111/200 Iteration: 9500 Train loss: 1.071212 Train acc: 0.459473\n",
      "Epoch: 111/200 Iteration: 9505 Train loss: 1.091663 Train acc: 0.466309\n",
      "Epoch: 111/200 Iteration: 9510 Train loss: 1.017003 Train acc: 0.413574\n",
      "Epoch: 111/200 Iteration: 9515 Train loss: 1.174176 Train acc: 0.332031\n",
      "Epoch: 111/200 Iteration: 9520 Train loss: 1.180903 Train acc: 0.231934\n",
      "Epoch: 112/200 Iteration: 9525 Train loss: 1.049764 Train acc: 0.503418\n",
      "Epoch: 112/200 Iteration: 9530 Train loss: 1.093099 Train acc: 0.399902\n",
      "Epoch: 112/200 Iteration: 9535 Train loss: 1.134604 Train acc: 0.348633\n",
      "Epoch: 112/200 Iteration: 9540 Train loss: 1.089710 Train acc: 0.410156\n",
      "Epoch: 112/200 Iteration: 9545 Train loss: 1.148983 Train acc: 0.312988\n",
      "Epoch: 112/200 Iteration: 9550 Train loss: 1.059892 Train acc: 0.467773\n",
      "Epoch: 112/200 Iteration: 9555 Train loss: 1.000849 Train acc: 0.579102\n",
      "Epoch: 112/200 Iteration: 9560 Train loss: 0.995938 Train acc: 0.584961\n",
      "Epoch: 112/200 Iteration: 9565 Train loss: 1.138167 Train acc: 0.278320\n",
      "Epoch: 112/200 Iteration: 9570 Train loss: 0.977848 Train acc: 0.631836\n",
      "Epoch: 112/200 Iteration: 9575 Train loss: 1.052597 Train acc: 0.494629\n",
      "Epoch: 112/200 Iteration: 9580 Train loss: 1.091987 Train acc: 0.406738\n",
      "Epoch: 112/200 Iteration: 9585 Train loss: 1.110465 Train acc: 0.302734\n",
      "Epoch: 112/200 Iteration: 9590 Train loss: 1.097957 Train acc: 0.437500\n",
      "Epoch: 112/200 Iteration: 9595 Train loss: 1.006119 Train acc: 0.480957\n",
      "Epoch: 112/200 Iteration: 9600 Train loss: 1.195464 Train acc: 0.347656\n",
      "Epoch: 112/200 Iteration: 9605 Train loss: 1.213387 Train acc: 0.231934\n",
      "Epoch: 113/200 Iteration: 9610 Train loss: 1.064162 Train acc: 0.503418\n",
      "Epoch: 113/200 Iteration: 9615 Train loss: 1.094539 Train acc: 0.399902\n",
      "Epoch: 113/200 Iteration: 9620 Train loss: 1.136848 Train acc: 0.348633\n",
      "Epoch: 113/200 Iteration: 9625 Train loss: 1.093421 Train acc: 0.410156\n",
      "Epoch: 113/200 Iteration: 9630 Train loss: 1.146163 Train acc: 0.312988\n",
      "Epoch: 113/200 Iteration: 9635 Train loss: 1.059990 Train acc: 0.467773\n",
      "Epoch: 113/200 Iteration: 9640 Train loss: 0.997440 Train acc: 0.579102\n",
      "Epoch: 113/200 Iteration: 9645 Train loss: 0.996073 Train acc: 0.584961\n",
      "Epoch: 113/200 Iteration: 9650 Train loss: 1.130187 Train acc: 0.278320\n",
      "Epoch: 113/200 Iteration: 9655 Train loss: 0.976446 Train acc: 0.631836\n",
      "Epoch: 113/200 Iteration: 9660 Train loss: 1.058100 Train acc: 0.494629\n",
      "Epoch: 113/200 Iteration: 9665 Train loss: 1.133620 Train acc: 0.243652\n",
      "Epoch: 113/200 Iteration: 9670 Train loss: 1.067987 Train acc: 0.467285\n",
      "Epoch: 113/200 Iteration: 9675 Train loss: 1.088202 Train acc: 0.476562\n",
      "Epoch: 113/200 Iteration: 9680 Train loss: 1.020111 Train acc: 0.411621\n",
      "Epoch: 113/200 Iteration: 9685 Train loss: 1.164627 Train acc: 0.309570\n",
      "Epoch: 113/200 Iteration: 9690 Train loss: 1.187709 Train acc: 0.231445\n",
      "Epoch: 114/200 Iteration: 9695 Train loss: 1.050094 Train acc: 0.503418\n",
      "Epoch: 114/200 Iteration: 9700 Train loss: 1.093512 Train acc: 0.399902\n",
      "Epoch: 114/200 Iteration: 9705 Train loss: 1.136025 Train acc: 0.348633\n",
      "Epoch: 114/200 Iteration: 9710 Train loss: 1.089915 Train acc: 0.410156\n",
      "Epoch: 114/200 Iteration: 9715 Train loss: 1.148025 Train acc: 0.312988\n",
      "Epoch: 114/200 Iteration: 9720 Train loss: 1.059468 Train acc: 0.467773\n",
      "Epoch: 114/200 Iteration: 9725 Train loss: 1.001440 Train acc: 0.579102\n",
      "Epoch: 114/200 Iteration: 9730 Train loss: 0.996151 Train acc: 0.584961\n",
      "Epoch: 114/200 Iteration: 9735 Train loss: 1.138933 Train acc: 0.278320\n",
      "Epoch: 114/200 Iteration: 9740 Train loss: 0.975831 Train acc: 0.631836\n",
      "Epoch: 114/200 Iteration: 9745 Train loss: 1.052574 Train acc: 0.494629\n",
      "Epoch: 114/200 Iteration: 9750 Train loss: 1.093884 Train acc: 0.408203\n",
      "Epoch: 114/200 Iteration: 9755 Train loss: 1.103914 Train acc: 0.340820\n",
      "Epoch: 114/200 Iteration: 9760 Train loss: 1.098766 Train acc: 0.433594\n",
      "Epoch: 114/200 Iteration: 9765 Train loss: 1.006508 Train acc: 0.458496\n",
      "Epoch: 114/200 Iteration: 9770 Train loss: 1.185516 Train acc: 0.342773\n",
      "Epoch: 114/200 Iteration: 9775 Train loss: 1.217393 Train acc: 0.232422\n",
      "Epoch: 115/200 Iteration: 9780 Train loss: 1.060590 Train acc: 0.503418\n",
      "Epoch: 115/200 Iteration: 9785 Train loss: 1.094378 Train acc: 0.400391\n",
      "Epoch: 115/200 Iteration: 9790 Train loss: 1.137550 Train acc: 0.348633\n",
      "Epoch: 115/200 Iteration: 9795 Train loss: 1.093173 Train acc: 0.410156\n",
      "Epoch: 115/200 Iteration: 9800 Train loss: 1.147578 Train acc: 0.312988\n",
      "Epoch: 115/200 Iteration: 9805 Train loss: 1.059029 Train acc: 0.467773\n",
      "Epoch: 115/200 Iteration: 9810 Train loss: 0.996411 Train acc: 0.579102\n",
      "Epoch: 115/200 Iteration: 9815 Train loss: 0.995276 Train acc: 0.584961\n",
      "Epoch: 115/200 Iteration: 9820 Train loss: 1.132715 Train acc: 0.278320\n",
      "Epoch: 115/200 Iteration: 9825 Train loss: 0.976204 Train acc: 0.631836\n",
      "Epoch: 115/200 Iteration: 9830 Train loss: 1.056610 Train acc: 0.494629\n",
      "Epoch: 115/200 Iteration: 9835 Train loss: 1.124721 Train acc: 0.243164\n",
      "Epoch: 115/200 Iteration: 9840 Train loss: 1.070272 Train acc: 0.457520\n",
      "Epoch: 115/200 Iteration: 9845 Train loss: 1.089016 Train acc: 0.465820\n",
      "Epoch: 115/200 Iteration: 9850 Train loss: 1.016253 Train acc: 0.413086\n",
      "Epoch: 115/200 Iteration: 9855 Train loss: 1.170487 Train acc: 0.335938\n",
      "Epoch: 115/200 Iteration: 9860 Train loss: 1.190472 Train acc: 0.230957\n",
      "Epoch: 116/200 Iteration: 9865 Train loss: 1.049914 Train acc: 0.503418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116/200 Iteration: 9870 Train loss: 1.093950 Train acc: 0.399902\n",
      "Epoch: 116/200 Iteration: 9875 Train loss: 1.135485 Train acc: 0.348633\n",
      "Epoch: 116/200 Iteration: 9880 Train loss: 1.090046 Train acc: 0.410156\n",
      "Epoch: 116/200 Iteration: 9885 Train loss: 1.149247 Train acc: 0.312988\n",
      "Epoch: 116/200 Iteration: 9890 Train loss: 1.058927 Train acc: 0.467773\n",
      "Epoch: 116/200 Iteration: 9895 Train loss: 1.000399 Train acc: 0.579102\n",
      "Epoch: 116/200 Iteration: 9900 Train loss: 0.996221 Train acc: 0.584961\n",
      "Epoch: 116/200 Iteration: 9905 Train loss: 1.139609 Train acc: 0.278320\n",
      "Epoch: 116/200 Iteration: 9910 Train loss: 0.977086 Train acc: 0.631836\n",
      "Epoch: 116/200 Iteration: 9915 Train loss: 1.052176 Train acc: 0.494629\n",
      "Epoch: 116/200 Iteration: 9920 Train loss: 1.091880 Train acc: 0.405762\n",
      "Epoch: 116/200 Iteration: 9925 Train loss: 1.111566 Train acc: 0.301758\n",
      "Epoch: 116/200 Iteration: 9930 Train loss: 1.096885 Train acc: 0.434570\n",
      "Epoch: 116/200 Iteration: 9935 Train loss: 1.006099 Train acc: 0.488770\n",
      "Epoch: 116/200 Iteration: 9940 Train loss: 1.192007 Train acc: 0.343750\n",
      "Epoch: 116/200 Iteration: 9945 Train loss: 1.212550 Train acc: 0.233398\n",
      "Epoch: 117/200 Iteration: 9950 Train loss: 1.064796 Train acc: 0.503418\n",
      "Epoch: 117/200 Iteration: 9955 Train loss: 1.095218 Train acc: 0.399902\n",
      "Epoch: 117/200 Iteration: 9960 Train loss: 1.135880 Train acc: 0.348633\n",
      "Epoch: 117/200 Iteration: 9965 Train loss: 1.094044 Train acc: 0.410156\n",
      "Epoch: 117/200 Iteration: 9970 Train loss: 1.146909 Train acc: 0.312988\n",
      "Epoch: 117/200 Iteration: 9975 Train loss: 1.059248 Train acc: 0.467773\n",
      "Epoch: 117/200 Iteration: 9980 Train loss: 0.997478 Train acc: 0.579102\n",
      "Epoch: 117/200 Iteration: 9985 Train loss: 0.995735 Train acc: 0.584961\n",
      "Epoch: 117/200 Iteration: 9990 Train loss: 1.130381 Train acc: 0.278320\n",
      "Epoch: 117/200 Iteration: 9995 Train loss: 0.976590 Train acc: 0.631836\n",
      "Epoch: 117/200 Iteration: 10000 Train loss: 1.057994 Train acc: 0.494629\n",
      "Epoch: 117/200 Iteration: 10005 Train loss: 1.135492 Train acc: 0.240723\n",
      "Epoch: 117/200 Iteration: 10010 Train loss: 1.066316 Train acc: 0.463379\n",
      "Epoch: 117/200 Iteration: 10015 Train loss: 1.089636 Train acc: 0.471680\n",
      "Epoch: 117/200 Iteration: 10020 Train loss: 1.020343 Train acc: 0.411621\n",
      "Epoch: 117/200 Iteration: 10025 Train loss: 1.164735 Train acc: 0.315918\n",
      "Epoch: 117/200 Iteration: 10030 Train loss: 1.181204 Train acc: 0.230957\n",
      "Epoch: 118/200 Iteration: 10035 Train loss: 1.049273 Train acc: 0.503418\n",
      "Epoch: 118/200 Iteration: 10040 Train loss: 1.092335 Train acc: 0.399902\n",
      "Epoch: 118/200 Iteration: 10045 Train loss: 1.134496 Train acc: 0.348633\n",
      "Epoch: 118/200 Iteration: 10050 Train loss: 1.089011 Train acc: 0.410156\n",
      "Epoch: 118/200 Iteration: 10055 Train loss: 1.148080 Train acc: 0.312988\n",
      "Epoch: 118/200 Iteration: 10060 Train loss: 1.060314 Train acc: 0.467773\n",
      "Epoch: 118/200 Iteration: 10065 Train loss: 1.000664 Train acc: 0.579102\n",
      "Epoch: 118/200 Iteration: 10070 Train loss: 0.997714 Train acc: 0.584961\n",
      "Epoch: 118/200 Iteration: 10075 Train loss: 1.138671 Train acc: 0.278320\n",
      "Epoch: 118/200 Iteration: 10080 Train loss: 0.977634 Train acc: 0.631836\n",
      "Epoch: 118/200 Iteration: 10085 Train loss: 1.052572 Train acc: 0.494629\n",
      "Epoch: 118/200 Iteration: 10090 Train loss: 1.089578 Train acc: 0.407715\n",
      "Epoch: 118/200 Iteration: 10095 Train loss: 1.118476 Train acc: 0.265137\n",
      "Epoch: 118/200 Iteration: 10100 Train loss: 1.098220 Train acc: 0.452637\n",
      "Epoch: 118/200 Iteration: 10105 Train loss: 1.005316 Train acc: 0.497070\n",
      "Epoch: 118/200 Iteration: 10110 Train loss: 1.191600 Train acc: 0.340820\n",
      "Epoch: 118/200 Iteration: 10115 Train loss: 1.217590 Train acc: 0.232422\n",
      "Epoch: 119/200 Iteration: 10120 Train loss: 1.064110 Train acc: 0.503418\n",
      "Epoch: 119/200 Iteration: 10125 Train loss: 1.095275 Train acc: 0.400391\n",
      "Epoch: 119/200 Iteration: 10130 Train loss: 1.136917 Train acc: 0.348633\n",
      "Epoch: 119/200 Iteration: 10135 Train loss: 1.093327 Train acc: 0.410156\n",
      "Epoch: 119/200 Iteration: 10140 Train loss: 1.146215 Train acc: 0.312988\n",
      "Epoch: 119/200 Iteration: 10145 Train loss: 1.059139 Train acc: 0.467773\n",
      "Epoch: 119/200 Iteration: 10150 Train loss: 0.997696 Train acc: 0.579102\n",
      "Epoch: 119/200 Iteration: 10155 Train loss: 0.995541 Train acc: 0.584961\n",
      "Epoch: 119/200 Iteration: 10160 Train loss: 1.130595 Train acc: 0.278320\n",
      "Epoch: 119/200 Iteration: 10165 Train loss: 0.975867 Train acc: 0.631836\n",
      "Epoch: 119/200 Iteration: 10170 Train loss: 1.057721 Train acc: 0.494629\n",
      "Epoch: 119/200 Iteration: 10175 Train loss: 1.134377 Train acc: 0.246582\n",
      "Epoch: 119/200 Iteration: 10180 Train loss: 1.066338 Train acc: 0.459961\n",
      "Epoch: 119/200 Iteration: 10185 Train loss: 1.088287 Train acc: 0.474121\n",
      "Epoch: 119/200 Iteration: 10190 Train loss: 1.018889 Train acc: 0.411621\n",
      "Epoch: 119/200 Iteration: 10195 Train loss: 1.165648 Train acc: 0.325195\n",
      "Epoch: 119/200 Iteration: 10200 Train loss: 1.183033 Train acc: 0.232422\n",
      "Epoch: 120/200 Iteration: 10205 Train loss: 1.049514 Train acc: 0.503418\n",
      "Epoch: 120/200 Iteration: 10210 Train loss: 1.093085 Train acc: 0.399902\n",
      "Epoch: 120/200 Iteration: 10215 Train loss: 1.134493 Train acc: 0.348633\n",
      "Epoch: 120/200 Iteration: 10220 Train loss: 1.089222 Train acc: 0.410156\n",
      "Epoch: 120/200 Iteration: 10225 Train loss: 1.147316 Train acc: 0.312988\n",
      "Epoch: 120/200 Iteration: 10230 Train loss: 1.060068 Train acc: 0.467773\n",
      "Epoch: 120/200 Iteration: 10235 Train loss: 1.002150 Train acc: 0.579102\n",
      "Epoch: 120/200 Iteration: 10240 Train loss: 0.997446 Train acc: 0.584961\n",
      "Epoch: 120/200 Iteration: 10245 Train loss: 1.138255 Train acc: 0.278320\n",
      "Epoch: 120/200 Iteration: 10250 Train loss: 0.977714 Train acc: 0.631836\n",
      "Epoch: 120/200 Iteration: 10255 Train loss: 1.052227 Train acc: 0.494629\n",
      "Epoch: 120/200 Iteration: 10260 Train loss: 1.090663 Train acc: 0.405273\n",
      "Epoch: 120/200 Iteration: 10265 Train loss: 1.118993 Train acc: 0.260254\n",
      "Epoch: 120/200 Iteration: 10270 Train loss: 1.100092 Train acc: 0.438965\n",
      "Epoch: 120/200 Iteration: 10275 Train loss: 1.005646 Train acc: 0.469238\n",
      "Epoch: 120/200 Iteration: 10280 Train loss: 1.193921 Train acc: 0.340332\n",
      "Epoch: 120/200 Iteration: 10285 Train loss: 1.209325 Train acc: 0.233887\n",
      "Epoch: 121/200 Iteration: 10290 Train loss: 1.062593 Train acc: 0.502930\n",
      "Epoch: 121/200 Iteration: 10295 Train loss: 1.094187 Train acc: 0.399902\n",
      "Epoch: 121/200 Iteration: 10300 Train loss: 1.134222 Train acc: 0.348633\n",
      "Epoch: 121/200 Iteration: 10305 Train loss: 1.093724 Train acc: 0.410156\n",
      "Epoch: 121/200 Iteration: 10310 Train loss: 1.144341 Train acc: 0.312988\n",
      "Epoch: 121/200 Iteration: 10315 Train loss: 1.057607 Train acc: 0.467773\n",
      "Epoch: 121/200 Iteration: 10320 Train loss: 0.998025 Train acc: 0.579102\n",
      "Epoch: 121/200 Iteration: 10325 Train loss: 0.996104 Train acc: 0.584961\n",
      "Epoch: 121/200 Iteration: 10330 Train loss: 1.130821 Train acc: 0.278320\n",
      "Epoch: 121/200 Iteration: 10335 Train loss: 0.975063 Train acc: 0.631836\n",
      "Epoch: 121/200 Iteration: 10340 Train loss: 1.057218 Train acc: 0.494629\n",
      "Epoch: 121/200 Iteration: 10345 Train loss: 1.132193 Train acc: 0.250000\n",
      "Epoch: 121/200 Iteration: 10350 Train loss: 1.069800 Train acc: 0.458496\n",
      "Epoch: 121/200 Iteration: 10355 Train loss: 1.090905 Train acc: 0.465820\n",
      "Epoch: 121/200 Iteration: 10360 Train loss: 1.019885 Train acc: 0.413086\n",
      "Epoch: 121/200 Iteration: 10365 Train loss: 1.165646 Train acc: 0.319336\n",
      "Epoch: 121/200 Iteration: 10370 Train loss: 1.185412 Train acc: 0.232422\n",
      "Epoch: 122/200 Iteration: 10375 Train loss: 1.049125 Train acc: 0.503418\n",
      "Epoch: 122/200 Iteration: 10380 Train loss: 1.093762 Train acc: 0.399902\n",
      "Epoch: 122/200 Iteration: 10385 Train loss: 1.139020 Train acc: 0.348633\n",
      "Epoch: 122/200 Iteration: 10390 Train loss: 1.089956 Train acc: 0.410156\n",
      "Epoch: 122/200 Iteration: 10395 Train loss: 1.148548 Train acc: 0.312988\n",
      "Epoch: 122/200 Iteration: 10400 Train loss: 1.059579 Train acc: 0.467773\n",
      "Epoch: 122/200 Iteration: 10405 Train loss: 1.000139 Train acc: 0.579102\n",
      "Epoch: 122/200 Iteration: 10410 Train loss: 0.996725 Train acc: 0.584961\n",
      "Epoch: 122/200 Iteration: 10415 Train loss: 1.137290 Train acc: 0.278320\n",
      "Epoch: 122/200 Iteration: 10420 Train loss: 0.977121 Train acc: 0.631836\n",
      "Epoch: 122/200 Iteration: 10425 Train loss: 1.053447 Train acc: 0.494629\n",
      "Epoch: 122/200 Iteration: 10430 Train loss: 1.094349 Train acc: 0.401855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122/200 Iteration: 10435 Train loss: 1.104442 Train acc: 0.347656\n",
      "Epoch: 122/200 Iteration: 10440 Train loss: 1.097804 Train acc: 0.435547\n",
      "Epoch: 122/200 Iteration: 10445 Train loss: 1.007015 Train acc: 0.463379\n",
      "Epoch: 122/200 Iteration: 10450 Train loss: 1.186575 Train acc: 0.343750\n",
      "Epoch: 122/200 Iteration: 10455 Train loss: 1.215374 Train acc: 0.232910\n",
      "Epoch: 123/200 Iteration: 10460 Train loss: 1.059492 Train acc: 0.503906\n",
      "Epoch: 123/200 Iteration: 10465 Train loss: 1.094900 Train acc: 0.399902\n",
      "Epoch: 123/200 Iteration: 10470 Train loss: 1.136437 Train acc: 0.348633\n",
      "Epoch: 123/200 Iteration: 10475 Train loss: 1.092871 Train acc: 0.410156\n",
      "Epoch: 123/200 Iteration: 10480 Train loss: 1.148171 Train acc: 0.312988\n",
      "Epoch: 123/200 Iteration: 10485 Train loss: 1.060031 Train acc: 0.467773\n",
      "Epoch: 123/200 Iteration: 10490 Train loss: 0.998711 Train acc: 0.579102\n",
      "Epoch: 123/200 Iteration: 10495 Train loss: 0.994541 Train acc: 0.584961\n",
      "Epoch: 123/200 Iteration: 10500 Train loss: 1.133382 Train acc: 0.278320\n",
      "Epoch: 123/200 Iteration: 10505 Train loss: 0.975770 Train acc: 0.631836\n",
      "Epoch: 123/200 Iteration: 10510 Train loss: 1.056087 Train acc: 0.494629\n",
      "Epoch: 123/200 Iteration: 10515 Train loss: 1.123038 Train acc: 0.250488\n",
      "Epoch: 123/200 Iteration: 10520 Train loss: 1.068464 Train acc: 0.451172\n",
      "Epoch: 123/200 Iteration: 10525 Train loss: 1.092851 Train acc: 0.465332\n",
      "Epoch: 123/200 Iteration: 10530 Train loss: 1.015977 Train acc: 0.413086\n",
      "Epoch: 123/200 Iteration: 10535 Train loss: 1.173980 Train acc: 0.320801\n",
      "Epoch: 123/200 Iteration: 10540 Train loss: 1.190220 Train acc: 0.231445\n",
      "Epoch: 124/200 Iteration: 10545 Train loss: 1.049972 Train acc: 0.503418\n",
      "Epoch: 124/200 Iteration: 10550 Train loss: 1.094234 Train acc: 0.399902\n",
      "Epoch: 124/200 Iteration: 10555 Train loss: 1.136666 Train acc: 0.348633\n",
      "Epoch: 124/200 Iteration: 10560 Train loss: 1.090883 Train acc: 0.410156\n",
      "Epoch: 124/200 Iteration: 10565 Train loss: 1.150617 Train acc: 0.312988\n",
      "Epoch: 124/200 Iteration: 10570 Train loss: 1.058365 Train acc: 0.467773\n",
      "Epoch: 124/200 Iteration: 10575 Train loss: 0.999311 Train acc: 0.579102\n",
      "Epoch: 124/200 Iteration: 10580 Train loss: 0.995368 Train acc: 0.584961\n",
      "Epoch: 124/200 Iteration: 10585 Train loss: 1.139158 Train acc: 0.278320\n",
      "Epoch: 124/200 Iteration: 10590 Train loss: 0.976535 Train acc: 0.631836\n",
      "Epoch: 124/200 Iteration: 10595 Train loss: 1.051851 Train acc: 0.494629\n",
      "Epoch: 124/200 Iteration: 10600 Train loss: 1.091503 Train acc: 0.408203\n",
      "Epoch: 124/200 Iteration: 10605 Train loss: 1.111122 Train acc: 0.301758\n",
      "Epoch: 124/200 Iteration: 10610 Train loss: 1.098166 Train acc: 0.445312\n",
      "Epoch: 124/200 Iteration: 10615 Train loss: 1.005484 Train acc: 0.473633\n",
      "Epoch: 124/200 Iteration: 10620 Train loss: 1.191246 Train acc: 0.348633\n",
      "Epoch: 124/200 Iteration: 10625 Train loss: 1.209732 Train acc: 0.232910\n",
      "Epoch: 125/200 Iteration: 10630 Train loss: 1.062608 Train acc: 0.503418\n",
      "Epoch: 125/200 Iteration: 10635 Train loss: 1.094284 Train acc: 0.400391\n",
      "Epoch: 125/200 Iteration: 10640 Train loss: 1.134704 Train acc: 0.348633\n",
      "Epoch: 125/200 Iteration: 10645 Train loss: 1.092907 Train acc: 0.410156\n",
      "Epoch: 125/200 Iteration: 10650 Train loss: 1.145687 Train acc: 0.312988\n",
      "Epoch: 125/200 Iteration: 10655 Train loss: 1.058055 Train acc: 0.467773\n",
      "Epoch: 125/200 Iteration: 10660 Train loss: 0.997358 Train acc: 0.579102\n",
      "Epoch: 125/200 Iteration: 10665 Train loss: 0.995112 Train acc: 0.584961\n",
      "Epoch: 125/200 Iteration: 10670 Train loss: 1.132673 Train acc: 0.278320\n",
      "Epoch: 125/200 Iteration: 10675 Train loss: 0.975618 Train acc: 0.631836\n",
      "Epoch: 125/200 Iteration: 10680 Train loss: 1.056413 Train acc: 0.494629\n",
      "Epoch: 125/200 Iteration: 10685 Train loss: 1.125937 Train acc: 0.243164\n",
      "Epoch: 125/200 Iteration: 10690 Train loss: 1.070319 Train acc: 0.463867\n",
      "Epoch: 125/200 Iteration: 10695 Train loss: 1.089688 Train acc: 0.471191\n",
      "Epoch: 125/200 Iteration: 10700 Train loss: 1.017888 Train acc: 0.412109\n",
      "Epoch: 125/200 Iteration: 10705 Train loss: 1.170352 Train acc: 0.318848\n",
      "Epoch: 125/200 Iteration: 10710 Train loss: 1.198860 Train acc: 0.231445\n",
      "Epoch: 126/200 Iteration: 10715 Train loss: 1.051245 Train acc: 0.503418\n",
      "Epoch: 126/200 Iteration: 10720 Train loss: 1.094491 Train acc: 0.399902\n",
      "Epoch: 126/200 Iteration: 10725 Train loss: 1.135022 Train acc: 0.348633\n",
      "Epoch: 126/200 Iteration: 10730 Train loss: 1.092108 Train acc: 0.410156\n",
      "Epoch: 126/200 Iteration: 10735 Train loss: 1.148612 Train acc: 0.312988\n",
      "Epoch: 126/200 Iteration: 10740 Train loss: 1.060179 Train acc: 0.467773\n",
      "Epoch: 126/200 Iteration: 10745 Train loss: 0.996311 Train acc: 0.579102\n",
      "Epoch: 126/200 Iteration: 10750 Train loss: 0.997213 Train acc: 0.584961\n",
      "Epoch: 126/200 Iteration: 10755 Train loss: 1.136804 Train acc: 0.278320\n",
      "Epoch: 126/200 Iteration: 10760 Train loss: 0.978911 Train acc: 0.631836\n",
      "Epoch: 126/200 Iteration: 10765 Train loss: 1.054144 Train acc: 0.494629\n",
      "Epoch: 126/200 Iteration: 10770 Train loss: 1.100119 Train acc: 0.348145\n",
      "Epoch: 126/200 Iteration: 10775 Train loss: 1.091787 Train acc: 0.408203\n",
      "Epoch: 126/200 Iteration: 10780 Train loss: 1.097906 Train acc: 0.433105\n",
      "Epoch: 126/200 Iteration: 10785 Train loss: 1.007601 Train acc: 0.443359\n",
      "Epoch: 126/200 Iteration: 10790 Train loss: 1.181166 Train acc: 0.343262\n",
      "Epoch: 126/200 Iteration: 10795 Train loss: 1.208903 Train acc: 0.231934\n",
      "Epoch: 127/200 Iteration: 10800 Train loss: 1.055638 Train acc: 0.503418\n",
      "Epoch: 127/200 Iteration: 10805 Train loss: 1.094265 Train acc: 0.399902\n",
      "Epoch: 127/200 Iteration: 10810 Train loss: 1.135855 Train acc: 0.348633\n",
      "Epoch: 127/200 Iteration: 10815 Train loss: 1.092275 Train acc: 0.410156\n",
      "Epoch: 127/200 Iteration: 10820 Train loss: 1.147133 Train acc: 0.312988\n",
      "Epoch: 127/200 Iteration: 10825 Train loss: 1.060428 Train acc: 0.467773\n",
      "Epoch: 127/200 Iteration: 10830 Train loss: 0.999265 Train acc: 0.579102\n",
      "Epoch: 127/200 Iteration: 10835 Train loss: 0.997311 Train acc: 0.584961\n",
      "Epoch: 127/200 Iteration: 10840 Train loss: 1.134178 Train acc: 0.278320\n",
      "Epoch: 127/200 Iteration: 10845 Train loss: 0.978919 Train acc: 0.631836\n",
      "Epoch: 127/200 Iteration: 10850 Train loss: 1.055187 Train acc: 0.494629\n",
      "Epoch: 127/200 Iteration: 10855 Train loss: 1.111374 Train acc: 0.238770\n",
      "Epoch: 127/200 Iteration: 10860 Train loss: 1.077328 Train acc: 0.449219\n",
      "Epoch: 127/200 Iteration: 10865 Train loss: 1.094762 Train acc: 0.462891\n",
      "Epoch: 127/200 Iteration: 10870 Train loss: 1.011812 Train acc: 0.415039\n",
      "Epoch: 127/200 Iteration: 10875 Train loss: 1.177813 Train acc: 0.339355\n",
      "Epoch: 127/200 Iteration: 10880 Train loss: 1.198547 Train acc: 0.231445\n",
      "Epoch: 128/200 Iteration: 10885 Train loss: 1.052446 Train acc: 0.503418\n",
      "Epoch: 128/200 Iteration: 10890 Train loss: 1.093885 Train acc: 0.399902\n",
      "Epoch: 128/200 Iteration: 10895 Train loss: 1.134854 Train acc: 0.348633\n",
      "Epoch: 128/200 Iteration: 10900 Train loss: 1.090250 Train acc: 0.410156\n",
      "Epoch: 128/200 Iteration: 10905 Train loss: 1.148681 Train acc: 0.312988\n",
      "Epoch: 128/200 Iteration: 10910 Train loss: 1.060150 Train acc: 0.467773\n",
      "Epoch: 128/200 Iteration: 10915 Train loss: 0.998772 Train acc: 0.579102\n",
      "Epoch: 128/200 Iteration: 10920 Train loss: 0.995936 Train acc: 0.584961\n",
      "Epoch: 128/200 Iteration: 10925 Train loss: 1.138074 Train acc: 0.278320\n",
      "Epoch: 128/200 Iteration: 10930 Train loss: 0.977746 Train acc: 0.631836\n",
      "Epoch: 128/200 Iteration: 10935 Train loss: 1.052539 Train acc: 0.494629\n",
      "Epoch: 128/200 Iteration: 10940 Train loss: 1.092124 Train acc: 0.405273\n",
      "Epoch: 128/200 Iteration: 10945 Train loss: 1.104551 Train acc: 0.349609\n",
      "Epoch: 128/200 Iteration: 10950 Train loss: 1.096903 Train acc: 0.449707\n",
      "Epoch: 128/200 Iteration: 10955 Train loss: 1.004439 Train acc: 0.486816\n",
      "Epoch: 128/200 Iteration: 10960 Train loss: 1.194652 Train acc: 0.340332\n",
      "Epoch: 128/200 Iteration: 10965 Train loss: 1.214313 Train acc: 0.231445\n",
      "Epoch: 129/200 Iteration: 10970 Train loss: 1.062158 Train acc: 0.502930\n",
      "Epoch: 129/200 Iteration: 10975 Train loss: 1.094456 Train acc: 0.399902\n",
      "Epoch: 129/200 Iteration: 10980 Train loss: 1.134342 Train acc: 0.348633\n",
      "Epoch: 129/200 Iteration: 10985 Train loss: 1.093069 Train acc: 0.410156\n",
      "Epoch: 129/200 Iteration: 10990 Train loss: 1.147166 Train acc: 0.312988\n",
      "Epoch: 129/200 Iteration: 10995 Train loss: 1.059962 Train acc: 0.467773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129/200 Iteration: 11000 Train loss: 0.997384 Train acc: 0.579102\n",
      "Epoch: 129/200 Iteration: 11005 Train loss: 0.997088 Train acc: 0.584961\n",
      "Epoch: 129/200 Iteration: 11010 Train loss: 1.131748 Train acc: 0.278320\n",
      "Epoch: 129/200 Iteration: 11015 Train loss: 0.977885 Train acc: 0.631836\n",
      "Epoch: 129/200 Iteration: 11020 Train loss: 1.056160 Train acc: 0.494629\n",
      "Epoch: 129/200 Iteration: 11025 Train loss: 1.127110 Train acc: 0.237793\n",
      "Epoch: 129/200 Iteration: 11030 Train loss: 1.066285 Train acc: 0.457520\n",
      "Epoch: 129/200 Iteration: 11035 Train loss: 1.088713 Train acc: 0.472168\n",
      "Epoch: 129/200 Iteration: 11040 Train loss: 1.017313 Train acc: 0.412109\n",
      "Epoch: 129/200 Iteration: 11045 Train loss: 1.169947 Train acc: 0.328613\n",
      "Epoch: 129/200 Iteration: 11050 Train loss: 1.194810 Train acc: 0.231934\n",
      "Epoch: 130/200 Iteration: 11055 Train loss: 1.051102 Train acc: 0.503418\n",
      "Epoch: 130/200 Iteration: 11060 Train loss: 1.093117 Train acc: 0.399902\n",
      "Epoch: 130/200 Iteration: 11065 Train loss: 1.134263 Train acc: 0.348633\n",
      "Epoch: 130/200 Iteration: 11070 Train loss: 1.088848 Train acc: 0.410156\n",
      "Epoch: 130/200 Iteration: 11075 Train loss: 1.151319 Train acc: 0.312988\n",
      "Epoch: 130/200 Iteration: 11080 Train loss: 1.060218 Train acc: 0.467773\n",
      "Epoch: 130/200 Iteration: 11085 Train loss: 1.000752 Train acc: 0.579102\n",
      "Epoch: 130/200 Iteration: 11090 Train loss: 0.996900 Train acc: 0.584961\n",
      "Epoch: 130/200 Iteration: 11095 Train loss: 1.138756 Train acc: 0.278320\n",
      "Epoch: 130/200 Iteration: 11100 Train loss: 0.977658 Train acc: 0.631836\n",
      "Epoch: 130/200 Iteration: 11105 Train loss: 1.051921 Train acc: 0.494629\n",
      "Epoch: 130/200 Iteration: 11110 Train loss: 1.091022 Train acc: 0.404297\n",
      "Epoch: 130/200 Iteration: 11115 Train loss: 1.116965 Train acc: 0.267578\n",
      "Epoch: 130/200 Iteration: 11120 Train loss: 1.101232 Train acc: 0.415039\n",
      "Epoch: 130/200 Iteration: 11125 Train loss: 1.005111 Train acc: 0.486816\n",
      "Epoch: 130/200 Iteration: 11130 Train loss: 1.192767 Train acc: 0.350586\n",
      "Epoch: 130/200 Iteration: 11135 Train loss: 1.207781 Train acc: 0.232910\n",
      "Epoch: 131/200 Iteration: 11140 Train loss: 1.062678 Train acc: 0.502930\n",
      "Epoch: 131/200 Iteration: 11145 Train loss: 1.094373 Train acc: 0.399902\n",
      "Epoch: 131/200 Iteration: 11150 Train loss: 1.133830 Train acc: 0.348633\n",
      "Epoch: 131/200 Iteration: 11155 Train loss: 1.093407 Train acc: 0.410156\n",
      "Epoch: 131/200 Iteration: 11160 Train loss: 1.145412 Train acc: 0.312988\n",
      "Epoch: 131/200 Iteration: 11165 Train loss: 1.060683 Train acc: 0.467773\n",
      "Epoch: 131/200 Iteration: 11170 Train loss: 0.996414 Train acc: 0.579102\n",
      "Epoch: 131/200 Iteration: 11175 Train loss: 0.995872 Train acc: 0.584961\n",
      "Epoch: 131/200 Iteration: 11180 Train loss: 1.131290 Train acc: 0.278320\n",
      "Epoch: 131/200 Iteration: 11185 Train loss: 0.976273 Train acc: 0.631836\n",
      "Epoch: 131/200 Iteration: 11190 Train loss: 1.056605 Train acc: 0.494629\n",
      "Epoch: 131/200 Iteration: 11195 Train loss: 1.128606 Train acc: 0.253906\n",
      "Epoch: 131/200 Iteration: 11200 Train loss: 1.066433 Train acc: 0.464355\n",
      "Epoch: 131/200 Iteration: 11205 Train loss: 1.090609 Train acc: 0.473633\n",
      "Epoch: 131/200 Iteration: 11210 Train loss: 1.017056 Train acc: 0.414551\n",
      "Epoch: 131/200 Iteration: 11215 Train loss: 1.166434 Train acc: 0.332031\n",
      "Epoch: 131/200 Iteration: 11220 Train loss: 1.196695 Train acc: 0.231445\n",
      "Epoch: 132/200 Iteration: 11225 Train loss: 1.051481 Train acc: 0.503418\n",
      "Epoch: 132/200 Iteration: 11230 Train loss: 1.093443 Train acc: 0.399902\n",
      "Epoch: 132/200 Iteration: 11235 Train loss: 1.135941 Train acc: 0.348633\n",
      "Epoch: 132/200 Iteration: 11240 Train loss: 1.090431 Train acc: 0.410156\n",
      "Epoch: 132/200 Iteration: 11245 Train loss: 1.149752 Train acc: 0.312988\n",
      "Epoch: 132/200 Iteration: 11250 Train loss: 1.059857 Train acc: 0.467773\n",
      "Epoch: 132/200 Iteration: 11255 Train loss: 0.999662 Train acc: 0.579102\n",
      "Epoch: 132/200 Iteration: 11260 Train loss: 0.996387 Train acc: 0.584961\n",
      "Epoch: 132/200 Iteration: 11265 Train loss: 1.138272 Train acc: 0.278320\n",
      "Epoch: 132/200 Iteration: 11270 Train loss: 0.978033 Train acc: 0.631836\n",
      "Epoch: 132/200 Iteration: 11275 Train loss: 1.052863 Train acc: 0.494629\n",
      "Epoch: 132/200 Iteration: 11280 Train loss: 1.094756 Train acc: 0.399902\n",
      "Epoch: 132/200 Iteration: 11285 Train loss: 1.101150 Train acc: 0.365234\n",
      "Epoch: 132/200 Iteration: 11290 Train loss: 1.098104 Train acc: 0.426270\n",
      "Epoch: 132/200 Iteration: 11295 Train loss: 1.007149 Train acc: 0.445312\n",
      "Epoch: 132/200 Iteration: 11300 Train loss: 1.188362 Train acc: 0.340820\n",
      "Epoch: 132/200 Iteration: 11305 Train loss: 1.219569 Train acc: 0.231445\n",
      "Epoch: 133/200 Iteration: 11310 Train loss: 1.058439 Train acc: 0.503418\n",
      "Epoch: 133/200 Iteration: 11315 Train loss: 1.094268 Train acc: 0.399902\n",
      "Epoch: 133/200 Iteration: 11320 Train loss: 1.134494 Train acc: 0.348633\n",
      "Epoch: 133/200 Iteration: 11325 Train loss: 1.092494 Train acc: 0.410156\n",
      "Epoch: 133/200 Iteration: 11330 Train loss: 1.150174 Train acc: 0.312988\n",
      "Epoch: 133/200 Iteration: 11335 Train loss: 1.059561 Train acc: 0.467773\n",
      "Epoch: 133/200 Iteration: 11340 Train loss: 0.997027 Train acc: 0.579102\n",
      "Epoch: 133/200 Iteration: 11345 Train loss: 0.994706 Train acc: 0.584961\n",
      "Epoch: 133/200 Iteration: 11350 Train loss: 1.133735 Train acc: 0.278320\n",
      "Epoch: 133/200 Iteration: 11355 Train loss: 0.976407 Train acc: 0.631836\n",
      "Epoch: 133/200 Iteration: 11360 Train loss: 1.055323 Train acc: 0.494629\n",
      "Epoch: 133/200 Iteration: 11365 Train loss: 1.116347 Train acc: 0.248047\n",
      "Epoch: 133/200 Iteration: 11370 Train loss: 1.070916 Train acc: 0.453613\n",
      "Epoch: 133/200 Iteration: 11375 Train loss: 1.092039 Train acc: 0.468750\n",
      "Epoch: 133/200 Iteration: 11380 Train loss: 1.013317 Train acc: 0.412598\n",
      "Epoch: 133/200 Iteration: 11385 Train loss: 1.171628 Train acc: 0.333496\n",
      "Epoch: 133/200 Iteration: 11390 Train loss: 1.194316 Train acc: 0.231445\n",
      "Epoch: 134/200 Iteration: 11395 Train loss: 1.051527 Train acc: 0.503418\n",
      "Epoch: 134/200 Iteration: 11400 Train loss: 1.093567 Train acc: 0.399902\n",
      "Epoch: 134/200 Iteration: 11405 Train loss: 1.135998 Train acc: 0.348633\n",
      "Epoch: 134/200 Iteration: 11410 Train loss: 1.089720 Train acc: 0.410156\n",
      "Epoch: 134/200 Iteration: 11415 Train loss: 1.149778 Train acc: 0.312988\n",
      "Epoch: 134/200 Iteration: 11420 Train loss: 1.058759 Train acc: 0.467773\n",
      "Epoch: 134/200 Iteration: 11425 Train loss: 1.000147 Train acc: 0.579102\n",
      "Epoch: 134/200 Iteration: 11430 Train loss: 0.996408 Train acc: 0.584961\n",
      "Epoch: 134/200 Iteration: 11435 Train loss: 1.139730 Train acc: 0.278320\n",
      "Epoch: 134/200 Iteration: 11440 Train loss: 0.976944 Train acc: 0.631836\n",
      "Epoch: 134/200 Iteration: 11445 Train loss: 1.051560 Train acc: 0.494629\n",
      "Epoch: 134/200 Iteration: 11450 Train loss: 1.088897 Train acc: 0.408691\n",
      "Epoch: 134/200 Iteration: 11455 Train loss: 1.114258 Train acc: 0.284180\n",
      "Epoch: 134/200 Iteration: 11460 Train loss: 1.099409 Train acc: 0.428223\n",
      "Epoch: 134/200 Iteration: 11465 Train loss: 1.002873 Train acc: 0.489258\n",
      "Epoch: 134/200 Iteration: 11470 Train loss: 1.193311 Train acc: 0.342773\n",
      "Epoch: 134/200 Iteration: 11475 Train loss: 1.209759 Train acc: 0.234375\n",
      "Epoch: 135/200 Iteration: 11480 Train loss: 1.063677 Train acc: 0.503418\n",
      "Epoch: 135/200 Iteration: 11485 Train loss: 1.094400 Train acc: 0.399414\n",
      "Epoch: 135/200 Iteration: 11490 Train loss: 1.132792 Train acc: 0.348633\n",
      "Epoch: 135/200 Iteration: 11495 Train loss: 1.093625 Train acc: 0.410156\n",
      "Epoch: 135/200 Iteration: 11500 Train loss: 1.145388 Train acc: 0.312988\n",
      "Epoch: 135/200 Iteration: 11505 Train loss: 1.059882 Train acc: 0.467773\n",
      "Epoch: 135/200 Iteration: 11510 Train loss: 0.997073 Train acc: 0.579102\n",
      "Epoch: 135/200 Iteration: 11515 Train loss: 0.996617 Train acc: 0.584961\n",
      "Epoch: 135/200 Iteration: 11520 Train loss: 1.131349 Train acc: 0.278320\n",
      "Epoch: 135/200 Iteration: 11525 Train loss: 0.976746 Train acc: 0.631836\n",
      "Epoch: 135/200 Iteration: 11530 Train loss: 1.057273 Train acc: 0.494629\n",
      "Epoch: 135/200 Iteration: 11535 Train loss: 1.132755 Train acc: 0.241699\n",
      "Epoch: 135/200 Iteration: 11540 Train loss: 1.070181 Train acc: 0.464355\n",
      "Epoch: 135/200 Iteration: 11545 Train loss: 1.089418 Train acc: 0.476074\n",
      "Epoch: 135/200 Iteration: 11550 Train loss: 1.019593 Train acc: 0.412109\n",
      "Epoch: 135/200 Iteration: 11555 Train loss: 1.162613 Train acc: 0.321777\n",
      "Epoch: 135/200 Iteration: 11560 Train loss: 1.191880 Train acc: 0.231445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/200 Iteration: 11565 Train loss: 1.051080 Train acc: 0.503418\n",
      "Epoch: 136/200 Iteration: 11570 Train loss: 1.093451 Train acc: 0.399902\n",
      "Epoch: 136/200 Iteration: 11575 Train loss: 1.135813 Train acc: 0.348633\n",
      "Epoch: 136/200 Iteration: 11580 Train loss: 1.090710 Train acc: 0.410156\n",
      "Epoch: 136/200 Iteration: 11585 Train loss: 1.149575 Train acc: 0.312988\n",
      "Epoch: 136/200 Iteration: 11590 Train loss: 1.058941 Train acc: 0.467773\n",
      "Epoch: 136/200 Iteration: 11595 Train loss: 1.000859 Train acc: 0.579102\n",
      "Epoch: 136/200 Iteration: 11600 Train loss: 0.996385 Train acc: 0.584961\n",
      "Epoch: 136/200 Iteration: 11605 Train loss: 1.137397 Train acc: 0.278320\n",
      "Epoch: 136/200 Iteration: 11610 Train loss: 0.975384 Train acc: 0.631836\n",
      "Epoch: 136/200 Iteration: 11615 Train loss: 1.052602 Train acc: 0.494629\n",
      "Epoch: 136/200 Iteration: 11620 Train loss: 1.095392 Train acc: 0.405273\n",
      "Epoch: 136/200 Iteration: 11625 Train loss: 1.096510 Train acc: 0.393555\n",
      "Epoch: 136/200 Iteration: 11630 Train loss: 1.097679 Train acc: 0.442871\n",
      "Epoch: 136/200 Iteration: 11635 Train loss: 1.007890 Train acc: 0.439941\n",
      "Epoch: 136/200 Iteration: 11640 Train loss: 1.186785 Train acc: 0.341797\n",
      "Epoch: 136/200 Iteration: 11645 Train loss: 1.217108 Train acc: 0.231445\n",
      "Epoch: 137/200 Iteration: 11650 Train loss: 1.056641 Train acc: 0.503418\n",
      "Epoch: 137/200 Iteration: 11655 Train loss: 1.094784 Train acc: 0.400391\n",
      "Epoch: 137/200 Iteration: 11660 Train loss: 1.136883 Train acc: 0.348633\n",
      "Epoch: 137/200 Iteration: 11665 Train loss: 1.093472 Train acc: 0.410156\n",
      "Epoch: 137/200 Iteration: 11670 Train loss: 1.150280 Train acc: 0.312988\n",
      "Epoch: 137/200 Iteration: 11675 Train loss: 1.060266 Train acc: 0.467773\n",
      "Epoch: 137/200 Iteration: 11680 Train loss: 0.997629 Train acc: 0.579102\n",
      "Epoch: 137/200 Iteration: 11685 Train loss: 0.995948 Train acc: 0.584961\n",
      "Epoch: 137/200 Iteration: 11690 Train loss: 1.134790 Train acc: 0.278320\n",
      "Epoch: 137/200 Iteration: 11695 Train loss: 0.975395 Train acc: 0.631836\n",
      "Epoch: 137/200 Iteration: 11700 Train loss: 1.054842 Train acc: 0.494629\n",
      "Epoch: 137/200 Iteration: 11705 Train loss: 1.115001 Train acc: 0.252930\n",
      "Epoch: 137/200 Iteration: 11710 Train loss: 1.068182 Train acc: 0.470215\n",
      "Epoch: 137/200 Iteration: 11715 Train loss: 1.090187 Train acc: 0.468262\n",
      "Epoch: 137/200 Iteration: 11720 Train loss: 1.015750 Train acc: 0.413574\n",
      "Epoch: 137/200 Iteration: 11725 Train loss: 1.177499 Train acc: 0.343262\n",
      "Epoch: 137/200 Iteration: 11730 Train loss: 1.201256 Train acc: 0.231445\n",
      "Epoch: 138/200 Iteration: 11735 Train loss: 1.052503 Train acc: 0.503418\n",
      "Epoch: 138/200 Iteration: 11740 Train loss: 1.094104 Train acc: 0.399902\n",
      "Epoch: 138/200 Iteration: 11745 Train loss: 1.136135 Train acc: 0.348633\n",
      "Epoch: 138/200 Iteration: 11750 Train loss: 1.090766 Train acc: 0.410156\n",
      "Epoch: 138/200 Iteration: 11755 Train loss: 1.149994 Train acc: 0.312988\n",
      "Epoch: 138/200 Iteration: 11760 Train loss: 1.059193 Train acc: 0.467773\n",
      "Epoch: 138/200 Iteration: 11765 Train loss: 0.998549 Train acc: 0.579102\n",
      "Epoch: 138/200 Iteration: 11770 Train loss: 0.995969 Train acc: 0.584961\n",
      "Epoch: 138/200 Iteration: 11775 Train loss: 1.138050 Train acc: 0.278320\n",
      "Epoch: 138/200 Iteration: 11780 Train loss: 0.977152 Train acc: 0.631836\n",
      "Epoch: 138/200 Iteration: 11785 Train loss: 1.053126 Train acc: 0.494629\n",
      "Epoch: 138/200 Iteration: 11790 Train loss: 1.092397 Train acc: 0.403809\n",
      "Epoch: 138/200 Iteration: 11795 Train loss: 1.108755 Train acc: 0.317871\n",
      "Epoch: 138/200 Iteration: 11800 Train loss: 1.097638 Train acc: 0.443848\n",
      "Epoch: 138/200 Iteration: 11805 Train loss: 1.006673 Train acc: 0.479492\n",
      "Epoch: 138/200 Iteration: 11810 Train loss: 1.196360 Train acc: 0.346680\n",
      "Epoch: 138/200 Iteration: 11815 Train loss: 1.211501 Train acc: 0.231934\n",
      "Epoch: 139/200 Iteration: 11820 Train loss: 1.061956 Train acc: 0.503418\n",
      "Epoch: 139/200 Iteration: 11825 Train loss: 1.094657 Train acc: 0.399902\n",
      "Epoch: 139/200 Iteration: 11830 Train loss: 1.133184 Train acc: 0.348633\n",
      "Epoch: 139/200 Iteration: 11835 Train loss: 1.093278 Train acc: 0.410156\n",
      "Epoch: 139/200 Iteration: 11840 Train loss: 1.146074 Train acc: 0.312988\n",
      "Epoch: 139/200 Iteration: 11845 Train loss: 1.058878 Train acc: 0.467773\n",
      "Epoch: 139/200 Iteration: 11850 Train loss: 0.997747 Train acc: 0.579102\n",
      "Epoch: 139/200 Iteration: 11855 Train loss: 0.995342 Train acc: 0.584961\n",
      "Epoch: 139/200 Iteration: 11860 Train loss: 1.133052 Train acc: 0.278320\n",
      "Epoch: 139/200 Iteration: 11865 Train loss: 0.976304 Train acc: 0.631836\n",
      "Epoch: 139/200 Iteration: 11870 Train loss: 1.056047 Train acc: 0.494629\n",
      "Epoch: 139/200 Iteration: 11875 Train loss: 1.124226 Train acc: 0.241211\n",
      "Epoch: 139/200 Iteration: 11880 Train loss: 1.065520 Train acc: 0.459473\n",
      "Epoch: 139/200 Iteration: 11885 Train loss: 1.089130 Train acc: 0.472168\n",
      "Epoch: 139/200 Iteration: 11890 Train loss: 1.017377 Train acc: 0.411621\n",
      "Epoch: 139/200 Iteration: 11895 Train loss: 1.169033 Train acc: 0.321289\n",
      "Epoch: 139/200 Iteration: 11900 Train loss: 1.186922 Train acc: 0.232422\n",
      "Epoch: 140/200 Iteration: 11905 Train loss: 1.049534 Train acc: 0.503418\n",
      "Epoch: 140/200 Iteration: 11910 Train loss: 1.093621 Train acc: 0.399902\n",
      "Epoch: 140/200 Iteration: 11915 Train loss: 1.134996 Train acc: 0.348633\n",
      "Epoch: 140/200 Iteration: 11920 Train loss: 1.089651 Train acc: 0.410156\n",
      "Epoch: 140/200 Iteration: 11925 Train loss: 1.150580 Train acc: 0.312988\n",
      "Epoch: 140/200 Iteration: 11930 Train loss: 1.059245 Train acc: 0.467773\n",
      "Epoch: 140/200 Iteration: 11935 Train loss: 1.000441 Train acc: 0.579102\n",
      "Epoch: 140/200 Iteration: 11940 Train loss: 0.995658 Train acc: 0.584961\n",
      "Epoch: 140/200 Iteration: 11945 Train loss: 1.140912 Train acc: 0.278320\n",
      "Epoch: 140/200 Iteration: 11950 Train loss: 0.975004 Train acc: 0.631836\n",
      "Epoch: 140/200 Iteration: 11955 Train loss: 1.052234 Train acc: 0.494629\n",
      "Epoch: 140/200 Iteration: 11960 Train loss: 1.088600 Train acc: 0.411621\n",
      "Epoch: 140/200 Iteration: 11965 Train loss: 1.118333 Train acc: 0.254883\n",
      "Epoch: 140/200 Iteration: 11970 Train loss: 1.100911 Train acc: 0.428711\n",
      "Epoch: 140/200 Iteration: 11975 Train loss: 1.004706 Train acc: 0.500977\n",
      "Epoch: 140/200 Iteration: 11980 Train loss: 1.194815 Train acc: 0.336426\n",
      "Epoch: 140/200 Iteration: 11985 Train loss: 1.214713 Train acc: 0.237305\n",
      "Epoch: 141/200 Iteration: 11990 Train loss: 1.063514 Train acc: 0.503418\n",
      "Epoch: 141/200 Iteration: 11995 Train loss: 1.094763 Train acc: 0.400391\n",
      "Epoch: 141/200 Iteration: 12000 Train loss: 1.135357 Train acc: 0.348633\n",
      "Epoch: 141/200 Iteration: 12005 Train loss: 1.094255 Train acc: 0.410156\n",
      "Epoch: 141/200 Iteration: 12010 Train loss: 1.145638 Train acc: 0.312988\n",
      "Epoch: 141/200 Iteration: 12015 Train loss: 1.058768 Train acc: 0.467773\n",
      "Epoch: 141/200 Iteration: 12020 Train loss: 0.997890 Train acc: 0.579102\n",
      "Epoch: 141/200 Iteration: 12025 Train loss: 0.995682 Train acc: 0.584961\n",
      "Epoch: 141/200 Iteration: 12030 Train loss: 1.128770 Train acc: 0.278320\n",
      "Epoch: 141/200 Iteration: 12035 Train loss: 0.976933 Train acc: 0.631836\n",
      "Epoch: 141/200 Iteration: 12040 Train loss: 1.058188 Train acc: 0.494629\n",
      "Epoch: 141/200 Iteration: 12045 Train loss: 1.135260 Train acc: 0.247070\n",
      "Epoch: 141/200 Iteration: 12050 Train loss: 1.066944 Train acc: 0.460938\n",
      "Epoch: 141/200 Iteration: 12055 Train loss: 1.089329 Train acc: 0.474609\n",
      "Epoch: 141/200 Iteration: 12060 Train loss: 1.020654 Train acc: 0.411621\n",
      "Epoch: 141/200 Iteration: 12065 Train loss: 1.164341 Train acc: 0.318359\n",
      "Epoch: 141/200 Iteration: 12070 Train loss: 1.190446 Train acc: 0.232910\n",
      "Epoch: 142/200 Iteration: 12075 Train loss: 1.050703 Train acc: 0.503418\n",
      "Epoch: 142/200 Iteration: 12080 Train loss: 1.093885 Train acc: 0.399902\n",
      "Epoch: 142/200 Iteration: 12085 Train loss: 1.134933 Train acc: 0.348633\n",
      "Epoch: 142/200 Iteration: 12090 Train loss: 1.091174 Train acc: 0.410156\n",
      "Epoch: 142/200 Iteration: 12095 Train loss: 1.148346 Train acc: 0.312988\n",
      "Epoch: 142/200 Iteration: 12100 Train loss: 1.056785 Train acc: 0.467773\n",
      "Epoch: 142/200 Iteration: 12105 Train loss: 0.997837 Train acc: 0.579102\n",
      "Epoch: 142/200 Iteration: 12110 Train loss: 0.995917 Train acc: 0.584961\n",
      "Epoch: 142/200 Iteration: 12115 Train loss: 1.138944 Train acc: 0.278320\n",
      "Epoch: 142/200 Iteration: 12120 Train loss: 0.973980 Train acc: 0.631836\n",
      "Epoch: 142/200 Iteration: 12125 Train loss: 1.053580 Train acc: 0.494629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142/200 Iteration: 12130 Train loss: 1.093830 Train acc: 0.405273\n",
      "Epoch: 142/200 Iteration: 12135 Train loss: 1.103829 Train acc: 0.358398\n",
      "Epoch: 142/200 Iteration: 12140 Train loss: 1.097969 Train acc: 0.441406\n",
      "Epoch: 142/200 Iteration: 12145 Train loss: 1.007325 Train acc: 0.469238\n",
      "Epoch: 142/200 Iteration: 12150 Train loss: 1.190228 Train acc: 0.342285\n",
      "Epoch: 142/200 Iteration: 12155 Train loss: 1.218272 Train acc: 0.231934\n",
      "Epoch: 143/200 Iteration: 12160 Train loss: 1.059533 Train acc: 0.503418\n",
      "Epoch: 143/200 Iteration: 12165 Train loss: 1.095009 Train acc: 0.399902\n",
      "Epoch: 143/200 Iteration: 12170 Train loss: 1.138868 Train acc: 0.348633\n",
      "Epoch: 143/200 Iteration: 12175 Train loss: 1.092897 Train acc: 0.410156\n",
      "Epoch: 143/200 Iteration: 12180 Train loss: 1.149304 Train acc: 0.312988\n",
      "Epoch: 143/200 Iteration: 12185 Train loss: 1.060512 Train acc: 0.467773\n",
      "Epoch: 143/200 Iteration: 12190 Train loss: 0.997184 Train acc: 0.579102\n",
      "Epoch: 143/200 Iteration: 12195 Train loss: 0.995618 Train acc: 0.584961\n",
      "Epoch: 143/200 Iteration: 12200 Train loss: 1.132314 Train acc: 0.278320\n",
      "Epoch: 143/200 Iteration: 12205 Train loss: 0.976972 Train acc: 0.631836\n",
      "Epoch: 143/200 Iteration: 12210 Train loss: 1.056354 Train acc: 0.494629\n",
      "Epoch: 143/200 Iteration: 12215 Train loss: 1.119893 Train acc: 0.228027\n",
      "Epoch: 143/200 Iteration: 12220 Train loss: 1.069756 Train acc: 0.457520\n",
      "Epoch: 143/200 Iteration: 12225 Train loss: 1.090614 Train acc: 0.469238\n",
      "Epoch: 143/200 Iteration: 12230 Train loss: 1.015376 Train acc: 0.411621\n",
      "Epoch: 143/200 Iteration: 12235 Train loss: 1.171637 Train acc: 0.329102\n",
      "Epoch: 143/200 Iteration: 12240 Train loss: 1.191640 Train acc: 0.230957\n",
      "Epoch: 144/200 Iteration: 12245 Train loss: 1.050148 Train acc: 0.503418\n",
      "Epoch: 144/200 Iteration: 12250 Train loss: 1.093510 Train acc: 0.399902\n",
      "Epoch: 144/200 Iteration: 12255 Train loss: 1.133980 Train acc: 0.348633\n",
      "Epoch: 144/200 Iteration: 12260 Train loss: 1.089783 Train acc: 0.410156\n",
      "Epoch: 144/200 Iteration: 12265 Train loss: 1.149843 Train acc: 0.312988\n",
      "Epoch: 144/200 Iteration: 12270 Train loss: 1.059784 Train acc: 0.467773\n",
      "Epoch: 144/200 Iteration: 12275 Train loss: 0.999730 Train acc: 0.579102\n",
      "Epoch: 144/200 Iteration: 12280 Train loss: 0.995094 Train acc: 0.584961\n",
      "Epoch: 144/200 Iteration: 12285 Train loss: 1.139424 Train acc: 0.278320\n",
      "Epoch: 144/200 Iteration: 12290 Train loss: 0.976046 Train acc: 0.631836\n",
      "Epoch: 144/200 Iteration: 12295 Train loss: 1.052023 Train acc: 0.494629\n",
      "Epoch: 144/200 Iteration: 12300 Train loss: 1.091786 Train acc: 0.400391\n",
      "Epoch: 144/200 Iteration: 12305 Train loss: 1.112784 Train acc: 0.290527\n",
      "Epoch: 144/200 Iteration: 12310 Train loss: 1.100781 Train acc: 0.419434\n",
      "Epoch: 144/200 Iteration: 12315 Train loss: 1.005074 Train acc: 0.478516\n",
      "Epoch: 144/200 Iteration: 12320 Train loss: 1.195666 Train acc: 0.344238\n",
      "Epoch: 144/200 Iteration: 12325 Train loss: 1.213175 Train acc: 0.232910\n",
      "Epoch: 145/200 Iteration: 12330 Train loss: 1.064043 Train acc: 0.503418\n",
      "Epoch: 145/200 Iteration: 12335 Train loss: 1.094371 Train acc: 0.400879\n",
      "Epoch: 145/200 Iteration: 12340 Train loss: 1.135249 Train acc: 0.348633\n",
      "Epoch: 145/200 Iteration: 12345 Train loss: 1.093791 Train acc: 0.410156\n",
      "Epoch: 145/200 Iteration: 12350 Train loss: 1.146680 Train acc: 0.312988\n",
      "Epoch: 145/200 Iteration: 12355 Train loss: 1.058880 Train acc: 0.467773\n",
      "Epoch: 145/200 Iteration: 12360 Train loss: 0.995445 Train acc: 0.579102\n",
      "Epoch: 145/200 Iteration: 12365 Train loss: 0.995902 Train acc: 0.584961\n",
      "Epoch: 145/200 Iteration: 12370 Train loss: 1.131070 Train acc: 0.278320\n",
      "Epoch: 145/200 Iteration: 12375 Train loss: 0.976914 Train acc: 0.631836\n",
      "Epoch: 145/200 Iteration: 12380 Train loss: 1.056491 Train acc: 0.494629\n",
      "Epoch: 145/200 Iteration: 12385 Train loss: 1.127865 Train acc: 0.250000\n",
      "Epoch: 145/200 Iteration: 12390 Train loss: 1.066928 Train acc: 0.458496\n",
      "Epoch: 145/200 Iteration: 12395 Train loss: 1.090201 Train acc: 0.476562\n",
      "Epoch: 145/200 Iteration: 12400 Train loss: 1.017082 Train acc: 0.412109\n",
      "Epoch: 145/200 Iteration: 12405 Train loss: 1.168194 Train acc: 0.320312\n",
      "Epoch: 145/200 Iteration: 12410 Train loss: 1.192590 Train acc: 0.231445\n",
      "Epoch: 146/200 Iteration: 12415 Train loss: 1.050558 Train acc: 0.503418\n",
      "Epoch: 146/200 Iteration: 12420 Train loss: 1.093248 Train acc: 0.399902\n",
      "Epoch: 146/200 Iteration: 12425 Train loss: 1.132695 Train acc: 0.348633\n",
      "Epoch: 146/200 Iteration: 12430 Train loss: 1.090166 Train acc: 0.410156\n",
      "Epoch: 146/200 Iteration: 12435 Train loss: 1.150284 Train acc: 0.312988\n",
      "Epoch: 146/200 Iteration: 12440 Train loss: 1.058778 Train acc: 0.467773\n",
      "Epoch: 146/200 Iteration: 12445 Train loss: 0.998311 Train acc: 0.579102\n",
      "Epoch: 146/200 Iteration: 12450 Train loss: 0.995347 Train acc: 0.584961\n",
      "Epoch: 146/200 Iteration: 12455 Train loss: 1.139581 Train acc: 0.278320\n",
      "Epoch: 146/200 Iteration: 12460 Train loss: 0.975740 Train acc: 0.631836\n",
      "Epoch: 146/200 Iteration: 12465 Train loss: 1.052696 Train acc: 0.494629\n",
      "Epoch: 146/200 Iteration: 12470 Train loss: 1.092510 Train acc: 0.393555\n",
      "Epoch: 146/200 Iteration: 12475 Train loss: 1.108062 Train acc: 0.327148\n",
      "Epoch: 146/200 Iteration: 12480 Train loss: 1.097972 Train acc: 0.444824\n",
      "Epoch: 146/200 Iteration: 12485 Train loss: 1.007062 Train acc: 0.452637\n",
      "Epoch: 146/200 Iteration: 12490 Train loss: 1.189747 Train acc: 0.341309\n",
      "Epoch: 146/200 Iteration: 12495 Train loss: 1.216908 Train acc: 0.231934\n",
      "Epoch: 147/200 Iteration: 12500 Train loss: 1.062086 Train acc: 0.503418\n",
      "Epoch: 147/200 Iteration: 12505 Train loss: 1.095710 Train acc: 0.400391\n",
      "Epoch: 147/200 Iteration: 12510 Train loss: 1.135807 Train acc: 0.348633\n",
      "Epoch: 147/200 Iteration: 12515 Train loss: 1.093221 Train acc: 0.410156\n",
      "Epoch: 147/200 Iteration: 12520 Train loss: 1.148124 Train acc: 0.312988\n",
      "Epoch: 147/200 Iteration: 12525 Train loss: 1.058205 Train acc: 0.467773\n",
      "Epoch: 147/200 Iteration: 12530 Train loss: 0.997129 Train acc: 0.579102\n",
      "Epoch: 147/200 Iteration: 12535 Train loss: 0.993832 Train acc: 0.584961\n",
      "Epoch: 147/200 Iteration: 12540 Train loss: 1.132023 Train acc: 0.278809\n",
      "Epoch: 147/200 Iteration: 12545 Train loss: 0.974798 Train acc: 0.631836\n",
      "Epoch: 147/200 Iteration: 12550 Train loss: 1.056187 Train acc: 0.494629\n",
      "Epoch: 147/200 Iteration: 12555 Train loss: 1.119287 Train acc: 0.239258\n",
      "Epoch: 147/200 Iteration: 12560 Train loss: 1.070115 Train acc: 0.449707\n",
      "Epoch: 147/200 Iteration: 12565 Train loss: 1.091341 Train acc: 0.468750\n",
      "Epoch: 147/200 Iteration: 12570 Train loss: 1.014083 Train acc: 0.413574\n",
      "Epoch: 147/200 Iteration: 12575 Train loss: 1.172555 Train acc: 0.341309\n",
      "Epoch: 147/200 Iteration: 12580 Train loss: 1.192112 Train acc: 0.232422\n",
      "Epoch: 148/200 Iteration: 12585 Train loss: 1.050608 Train acc: 0.503418\n",
      "Epoch: 148/200 Iteration: 12590 Train loss: 1.093457 Train acc: 0.399902\n",
      "Epoch: 148/200 Iteration: 12595 Train loss: 1.134689 Train acc: 0.348633\n",
      "Epoch: 148/200 Iteration: 12600 Train loss: 1.090657 Train acc: 0.410156\n",
      "Epoch: 148/200 Iteration: 12605 Train loss: 1.148907 Train acc: 0.312988\n",
      "Epoch: 148/200 Iteration: 12610 Train loss: 1.058097 Train acc: 0.467773\n",
      "Epoch: 148/200 Iteration: 12615 Train loss: 0.999147 Train acc: 0.579102\n",
      "Epoch: 148/200 Iteration: 12620 Train loss: 0.995850 Train acc: 0.584961\n",
      "Epoch: 148/200 Iteration: 12625 Train loss: 1.138925 Train acc: 0.278320\n",
      "Epoch: 148/200 Iteration: 12630 Train loss: 0.976289 Train acc: 0.631836\n",
      "Epoch: 148/200 Iteration: 12635 Train loss: 1.052384 Train acc: 0.494629\n",
      "Epoch: 148/200 Iteration: 12640 Train loss: 1.090596 Train acc: 0.396484\n",
      "Epoch: 148/200 Iteration: 12645 Train loss: 1.117550 Train acc: 0.265137\n",
      "Epoch: 148/200 Iteration: 12650 Train loss: 1.096503 Train acc: 0.434082\n",
      "Epoch: 148/200 Iteration: 12655 Train loss: 1.004891 Train acc: 0.469727\n",
      "Epoch: 148/200 Iteration: 12660 Train loss: 1.191676 Train acc: 0.350098\n",
      "Epoch: 148/200 Iteration: 12665 Train loss: 1.213314 Train acc: 0.232910\n",
      "Epoch: 149/200 Iteration: 12670 Train loss: 1.063452 Train acc: 0.503418\n",
      "Epoch: 149/200 Iteration: 12675 Train loss: 1.094142 Train acc: 0.400391\n",
      "Epoch: 149/200 Iteration: 12680 Train loss: 1.132911 Train acc: 0.348633\n",
      "Epoch: 149/200 Iteration: 12685 Train loss: 1.093749 Train acc: 0.410156\n",
      "Epoch: 149/200 Iteration: 12690 Train loss: 1.144944 Train acc: 0.312988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149/200 Iteration: 12695 Train loss: 1.057850 Train acc: 0.467773\n",
      "Epoch: 149/200 Iteration: 12700 Train loss: 0.996046 Train acc: 0.579102\n",
      "Epoch: 149/200 Iteration: 12705 Train loss: 0.995018 Train acc: 0.584961\n",
      "Epoch: 149/200 Iteration: 12710 Train loss: 1.130052 Train acc: 0.278320\n",
      "Epoch: 149/200 Iteration: 12715 Train loss: 0.975631 Train acc: 0.631836\n",
      "Epoch: 149/200 Iteration: 12720 Train loss: 1.057303 Train acc: 0.494629\n",
      "Epoch: 149/200 Iteration: 12725 Train loss: 1.134903 Train acc: 0.242676\n",
      "Epoch: 149/200 Iteration: 12730 Train loss: 1.067582 Train acc: 0.460938\n",
      "Epoch: 149/200 Iteration: 12735 Train loss: 1.090111 Train acc: 0.473633\n",
      "Epoch: 149/200 Iteration: 12740 Train loss: 1.019680 Train acc: 0.412109\n",
      "Epoch: 149/200 Iteration: 12745 Train loss: 1.164520 Train acc: 0.332520\n",
      "Epoch: 149/200 Iteration: 12750 Train loss: 1.189763 Train acc: 0.231445\n",
      "Epoch: 150/200 Iteration: 12755 Train loss: 1.049498 Train acc: 0.503418\n",
      "Epoch: 150/200 Iteration: 12760 Train loss: 1.094144 Train acc: 0.399902\n",
      "Epoch: 150/200 Iteration: 12765 Train loss: 1.137451 Train acc: 0.348633\n",
      "Epoch: 150/200 Iteration: 12770 Train loss: 1.091521 Train acc: 0.410156\n",
      "Epoch: 150/200 Iteration: 12775 Train loss: 1.150457 Train acc: 0.312988\n",
      "Epoch: 150/200 Iteration: 12780 Train loss: 1.059811 Train acc: 0.467773\n",
      "Epoch: 150/200 Iteration: 12785 Train loss: 0.998501 Train acc: 0.579102\n",
      "Epoch: 150/200 Iteration: 12790 Train loss: 0.996809 Train acc: 0.584961\n",
      "Epoch: 150/200 Iteration: 12795 Train loss: 1.138316 Train acc: 0.278320\n",
      "Epoch: 150/200 Iteration: 12800 Train loss: 0.976225 Train acc: 0.631836\n",
      "Epoch: 150/200 Iteration: 12805 Train loss: 1.052395 Train acc: 0.494629\n",
      "Epoch: 150/200 Iteration: 12810 Train loss: 1.092707 Train acc: 0.400879\n",
      "Epoch: 150/200 Iteration: 12815 Train loss: 1.104272 Train acc: 0.347168\n",
      "Epoch: 150/200 Iteration: 12820 Train loss: 1.099209 Train acc: 0.437500\n",
      "Epoch: 150/200 Iteration: 12825 Train loss: 1.007424 Train acc: 0.458496\n",
      "Epoch: 150/200 Iteration: 12830 Train loss: 1.187886 Train acc: 0.342285\n",
      "Epoch: 150/200 Iteration: 12835 Train loss: 1.218086 Train acc: 0.231445\n",
      "Epoch: 151/200 Iteration: 12840 Train loss: 1.058023 Train acc: 0.503418\n",
      "Epoch: 151/200 Iteration: 12845 Train loss: 1.094416 Train acc: 0.399902\n",
      "Epoch: 151/200 Iteration: 12850 Train loss: 1.136750 Train acc: 0.348633\n",
      "Epoch: 151/200 Iteration: 12855 Train loss: 1.092165 Train acc: 0.410156\n",
      "Epoch: 151/200 Iteration: 12860 Train loss: 1.151175 Train acc: 0.312988\n",
      "Epoch: 151/200 Iteration: 12865 Train loss: 1.056553 Train acc: 0.467773\n",
      "Epoch: 151/200 Iteration: 12870 Train loss: 0.996122 Train acc: 0.579102\n",
      "Epoch: 151/200 Iteration: 12875 Train loss: 0.994096 Train acc: 0.584961\n",
      "Epoch: 151/200 Iteration: 12880 Train loss: 1.133729 Train acc: 0.278809\n",
      "Epoch: 151/200 Iteration: 12885 Train loss: 0.972876 Train acc: 0.631836\n",
      "Epoch: 151/200 Iteration: 12890 Train loss: 1.055537 Train acc: 0.494629\n",
      "Epoch: 151/200 Iteration: 12895 Train loss: 1.118418 Train acc: 0.238281\n",
      "Epoch: 151/200 Iteration: 12900 Train loss: 1.066592 Train acc: 0.453613\n",
      "Epoch: 151/200 Iteration: 12905 Train loss: 1.091018 Train acc: 0.465332\n",
      "Epoch: 151/200 Iteration: 12910 Train loss: 1.015876 Train acc: 0.412598\n",
      "Epoch: 151/200 Iteration: 12915 Train loss: 1.171461 Train acc: 0.335938\n",
      "Epoch: 151/200 Iteration: 12920 Train loss: 1.197666 Train acc: 0.231445\n",
      "Epoch: 152/200 Iteration: 12925 Train loss: 1.050198 Train acc: 0.503418\n",
      "Epoch: 152/200 Iteration: 12930 Train loss: 1.094240 Train acc: 0.399902\n",
      "Epoch: 152/200 Iteration: 12935 Train loss: 1.136148 Train acc: 0.348633\n",
      "Epoch: 152/200 Iteration: 12940 Train loss: 1.091348 Train acc: 0.410156\n",
      "Epoch: 152/200 Iteration: 12945 Train loss: 1.148396 Train acc: 0.312988\n",
      "Epoch: 152/200 Iteration: 12950 Train loss: 1.058993 Train acc: 0.467773\n",
      "Epoch: 152/200 Iteration: 12955 Train loss: 0.997874 Train acc: 0.579102\n",
      "Epoch: 152/200 Iteration: 12960 Train loss: 0.997089 Train acc: 0.584961\n",
      "Epoch: 152/200 Iteration: 12965 Train loss: 1.138901 Train acc: 0.278320\n",
      "Epoch: 152/200 Iteration: 12970 Train loss: 0.977389 Train acc: 0.631836\n",
      "Epoch: 152/200 Iteration: 12975 Train loss: 1.051584 Train acc: 0.494629\n",
      "Epoch: 152/200 Iteration: 12980 Train loss: 1.091070 Train acc: 0.405762\n",
      "Epoch: 152/200 Iteration: 12985 Train loss: 1.106019 Train acc: 0.351074\n",
      "Epoch: 152/200 Iteration: 12990 Train loss: 1.096335 Train acc: 0.456055\n",
      "Epoch: 152/200 Iteration: 12995 Train loss: 1.006984 Train acc: 0.485352\n",
      "Epoch: 152/200 Iteration: 13000 Train loss: 1.189335 Train acc: 0.342773\n",
      "Epoch: 152/200 Iteration: 13005 Train loss: 1.217956 Train acc: 0.230957\n",
      "Epoch: 153/200 Iteration: 13010 Train loss: 1.061316 Train acc: 0.503418\n",
      "Epoch: 153/200 Iteration: 13015 Train loss: 1.094678 Train acc: 0.400391\n",
      "Epoch: 153/200 Iteration: 13020 Train loss: 1.133450 Train acc: 0.348633\n",
      "Epoch: 153/200 Iteration: 13025 Train loss: 1.093441 Train acc: 0.410156\n",
      "Epoch: 153/200 Iteration: 13030 Train loss: 1.146419 Train acc: 0.312988\n",
      "Epoch: 153/200 Iteration: 13035 Train loss: 1.059127 Train acc: 0.467773\n",
      "Epoch: 153/200 Iteration: 13040 Train loss: 0.997874 Train acc: 0.579102\n",
      "Epoch: 153/200 Iteration: 13045 Train loss: 0.994225 Train acc: 0.584961\n",
      "Epoch: 153/200 Iteration: 13050 Train loss: 1.133224 Train acc: 0.278320\n",
      "Epoch: 153/200 Iteration: 13055 Train loss: 0.974533 Train acc: 0.631836\n",
      "Epoch: 153/200 Iteration: 13060 Train loss: 1.055354 Train acc: 0.494629\n",
      "Epoch: 153/200 Iteration: 13065 Train loss: 1.120157 Train acc: 0.236816\n",
      "Epoch: 153/200 Iteration: 13070 Train loss: 1.063361 Train acc: 0.460449\n",
      "Epoch: 153/200 Iteration: 13075 Train loss: 1.089311 Train acc: 0.473633\n",
      "Epoch: 153/200 Iteration: 13080 Train loss: 1.017045 Train acc: 0.412109\n",
      "Epoch: 153/200 Iteration: 13085 Train loss: 1.173438 Train acc: 0.330566\n",
      "Epoch: 153/200 Iteration: 13090 Train loss: 1.196470 Train acc: 0.232422\n",
      "Epoch: 154/200 Iteration: 13095 Train loss: 1.051328 Train acc: 0.503418\n",
      "Epoch: 154/200 Iteration: 13100 Train loss: 1.093415 Train acc: 0.399902\n",
      "Epoch: 154/200 Iteration: 13105 Train loss: 1.137415 Train acc: 0.348633\n",
      "Epoch: 154/200 Iteration: 13110 Train loss: 1.091426 Train acc: 0.410156\n",
      "Epoch: 154/200 Iteration: 13115 Train loss: 1.150927 Train acc: 0.312988\n",
      "Epoch: 154/200 Iteration: 13120 Train loss: 1.055935 Train acc: 0.467773\n",
      "Epoch: 154/200 Iteration: 13125 Train loss: 0.996128 Train acc: 0.579102\n",
      "Epoch: 154/200 Iteration: 13130 Train loss: 0.996194 Train acc: 0.584961\n",
      "Epoch: 154/200 Iteration: 13135 Train loss: 1.137694 Train acc: 0.278320\n",
      "Epoch: 154/200 Iteration: 13140 Train loss: 0.979651 Train acc: 0.631836\n",
      "Epoch: 154/200 Iteration: 13145 Train loss: 1.053505 Train acc: 0.494629\n",
      "Epoch: 154/200 Iteration: 13150 Train loss: 1.089888 Train acc: 0.408203\n",
      "Epoch: 154/200 Iteration: 13155 Train loss: 1.119061 Train acc: 0.270996\n",
      "Epoch: 154/200 Iteration: 13160 Train loss: 1.101522 Train acc: 0.415039\n",
      "Epoch: 154/200 Iteration: 13165 Train loss: 1.005520 Train acc: 0.478027\n",
      "Epoch: 154/200 Iteration: 13170 Train loss: 1.193154 Train acc: 0.344727\n",
      "Epoch: 154/200 Iteration: 13175 Train loss: 1.210406 Train acc: 0.231445\n",
      "Epoch: 155/200 Iteration: 13180 Train loss: 1.062532 Train acc: 0.503906\n",
      "Epoch: 155/200 Iteration: 13185 Train loss: 1.095806 Train acc: 0.400879\n",
      "Epoch: 155/200 Iteration: 13190 Train loss: 1.138483 Train acc: 0.348633\n",
      "Epoch: 155/200 Iteration: 13195 Train loss: 1.094459 Train acc: 0.410156\n",
      "Epoch: 155/200 Iteration: 13200 Train loss: 1.145166 Train acc: 0.312988\n",
      "Epoch: 155/200 Iteration: 13205 Train loss: 1.060954 Train acc: 0.467773\n",
      "Epoch: 155/200 Iteration: 13210 Train loss: 0.995806 Train acc: 0.579102\n",
      "Epoch: 155/200 Iteration: 13215 Train loss: 0.997791 Train acc: 0.584961\n",
      "Epoch: 155/200 Iteration: 13220 Train loss: 1.130779 Train acc: 0.278320\n",
      "Epoch: 155/200 Iteration: 13225 Train loss: 0.979445 Train acc: 0.631836\n",
      "Epoch: 155/200 Iteration: 13230 Train loss: 1.057638 Train acc: 0.494629\n",
      "Epoch: 155/200 Iteration: 13235 Train loss: 1.130794 Train acc: 0.237305\n",
      "Epoch: 155/200 Iteration: 13240 Train loss: 1.065266 Train acc: 0.464355\n",
      "Epoch: 155/200 Iteration: 13245 Train loss: 1.089142 Train acc: 0.475586\n",
      "Epoch: 155/200 Iteration: 13250 Train loss: 1.018183 Train acc: 0.411621\n",
      "Epoch: 155/200 Iteration: 13255 Train loss: 1.162313 Train acc: 0.310547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155/200 Iteration: 13260 Train loss: 1.184348 Train acc: 0.231934\n",
      "Epoch: 156/200 Iteration: 13265 Train loss: 1.050560 Train acc: 0.503418\n",
      "Epoch: 156/200 Iteration: 13270 Train loss: 1.092380 Train acc: 0.399902\n",
      "Epoch: 156/200 Iteration: 13275 Train loss: 1.133126 Train acc: 0.348633\n",
      "Epoch: 156/200 Iteration: 13280 Train loss: 1.089629 Train acc: 0.410156\n",
      "Epoch: 156/200 Iteration: 13285 Train loss: 1.149255 Train acc: 0.312988\n",
      "Epoch: 156/200 Iteration: 13290 Train loss: 1.059926 Train acc: 0.467773\n",
      "Epoch: 156/200 Iteration: 13295 Train loss: 0.998754 Train acc: 0.579102\n",
      "Epoch: 156/200 Iteration: 13300 Train loss: 0.997824 Train acc: 0.584961\n",
      "Epoch: 156/200 Iteration: 13305 Train loss: 1.137781 Train acc: 0.278320\n",
      "Epoch: 156/200 Iteration: 13310 Train loss: 0.980592 Train acc: 0.631836\n",
      "Epoch: 156/200 Iteration: 13315 Train loss: 1.051916 Train acc: 0.494629\n",
      "Epoch: 156/200 Iteration: 13320 Train loss: 1.088494 Train acc: 0.404297\n",
      "Epoch: 156/200 Iteration: 13325 Train loss: 1.124412 Train acc: 0.245117\n",
      "Epoch: 156/200 Iteration: 13330 Train loss: 1.099261 Train acc: 0.432129\n",
      "Epoch: 156/200 Iteration: 13335 Train loss: 1.006519 Train acc: 0.477539\n",
      "Epoch: 156/200 Iteration: 13340 Train loss: 1.193586 Train acc: 0.345215\n",
      "Epoch: 156/200 Iteration: 13345 Train loss: 1.212048 Train acc: 0.241211\n",
      "Epoch: 157/200 Iteration: 13350 Train loss: 1.068388 Train acc: 0.502930\n",
      "Epoch: 157/200 Iteration: 13355 Train loss: 1.093575 Train acc: 0.401367\n",
      "Epoch: 157/200 Iteration: 13360 Train loss: 1.133726 Train acc: 0.348633\n",
      "Epoch: 157/200 Iteration: 13365 Train loss: 1.093532 Train acc: 0.410156\n",
      "Epoch: 157/200 Iteration: 13370 Train loss: 1.147093 Train acc: 0.312988\n",
      "Epoch: 157/200 Iteration: 13375 Train loss: 1.060640 Train acc: 0.467773\n",
      "Epoch: 157/200 Iteration: 13380 Train loss: 0.995122 Train acc: 0.579102\n",
      "Epoch: 157/200 Iteration: 13385 Train loss: 0.995946 Train acc: 0.584961\n",
      "Epoch: 157/200 Iteration: 13390 Train loss: 1.129308 Train acc: 0.278320\n",
      "Epoch: 157/200 Iteration: 13395 Train loss: 0.978005 Train acc: 0.631836\n",
      "Epoch: 157/200 Iteration: 13400 Train loss: 1.056924 Train acc: 0.494629\n",
      "Epoch: 157/200 Iteration: 13405 Train loss: 1.133019 Train acc: 0.254395\n",
      "Epoch: 157/200 Iteration: 13410 Train loss: 1.061703 Train acc: 0.455078\n",
      "Epoch: 157/200 Iteration: 13415 Train loss: 1.089085 Train acc: 0.468262\n",
      "Epoch: 157/200 Iteration: 13420 Train loss: 1.016085 Train acc: 0.412109\n",
      "Epoch: 157/200 Iteration: 13425 Train loss: 1.164475 Train acc: 0.313477\n",
      "Epoch: 157/200 Iteration: 13430 Train loss: 1.173399 Train acc: 0.231934\n",
      "Epoch: 158/200 Iteration: 13435 Train loss: 1.048445 Train acc: 0.503418\n",
      "Epoch: 158/200 Iteration: 13440 Train loss: 1.092888 Train acc: 0.399902\n",
      "Epoch: 158/200 Iteration: 13445 Train loss: 1.136846 Train acc: 0.348633\n",
      "Epoch: 158/200 Iteration: 13450 Train loss: 1.089379 Train acc: 0.410156\n",
      "Epoch: 158/200 Iteration: 13455 Train loss: 1.149660 Train acc: 0.312988\n",
      "Epoch: 158/200 Iteration: 13460 Train loss: 1.058905 Train acc: 0.467773\n",
      "Epoch: 158/200 Iteration: 13465 Train loss: 1.000691 Train acc: 0.579102\n",
      "Epoch: 158/200 Iteration: 13470 Train loss: 0.997393 Train acc: 0.584961\n",
      "Epoch: 158/200 Iteration: 13475 Train loss: 1.138201 Train acc: 0.278320\n",
      "Epoch: 158/200 Iteration: 13480 Train loss: 0.978093 Train acc: 0.631836\n",
      "Epoch: 158/200 Iteration: 13485 Train loss: 1.051727 Train acc: 0.494629\n",
      "Epoch: 158/200 Iteration: 13490 Train loss: 1.087784 Train acc: 0.406250\n",
      "Epoch: 158/200 Iteration: 13495 Train loss: 1.125240 Train acc: 0.232910\n",
      "Epoch: 158/200 Iteration: 13500 Train loss: 1.098830 Train acc: 0.456055\n",
      "Epoch: 158/200 Iteration: 13505 Train loss: 1.007538 Train acc: 0.459473\n",
      "Epoch: 158/200 Iteration: 13510 Train loss: 1.185729 Train acc: 0.347168\n",
      "Epoch: 158/200 Iteration: 13515 Train loss: 1.212791 Train acc: 0.242676\n",
      "Epoch: 159/200 Iteration: 13520 Train loss: 1.064488 Train acc: 0.503418\n",
      "Epoch: 159/200 Iteration: 13525 Train loss: 1.093718 Train acc: 0.401855\n",
      "Epoch: 159/200 Iteration: 13530 Train loss: 1.133085 Train acc: 0.348633\n",
      "Epoch: 159/200 Iteration: 13535 Train loss: 1.093361 Train acc: 0.410156\n",
      "Epoch: 159/200 Iteration: 13540 Train loss: 1.143697 Train acc: 0.312988\n",
      "Epoch: 159/200 Iteration: 13545 Train loss: 1.059772 Train acc: 0.467773\n",
      "Epoch: 159/200 Iteration: 13550 Train loss: 0.995371 Train acc: 0.579102\n",
      "Epoch: 159/200 Iteration: 13555 Train loss: 0.997199 Train acc: 0.584961\n",
      "Epoch: 159/200 Iteration: 13560 Train loss: 1.127445 Train acc: 0.278809\n",
      "Epoch: 159/200 Iteration: 13565 Train loss: 0.977882 Train acc: 0.631836\n",
      "Epoch: 159/200 Iteration: 13570 Train loss: 1.058264 Train acc: 0.494629\n",
      "Epoch: 159/200 Iteration: 13575 Train loss: 1.129726 Train acc: 0.246094\n",
      "Epoch: 159/200 Iteration: 13580 Train loss: 1.077455 Train acc: 0.460449\n",
      "Epoch: 159/200 Iteration: 13585 Train loss: 1.094088 Train acc: 0.466797\n",
      "Epoch: 159/200 Iteration: 13590 Train loss: 1.016277 Train acc: 0.417969\n",
      "Epoch: 159/200 Iteration: 13595 Train loss: 1.166363 Train acc: 0.335938\n",
      "Epoch: 159/200 Iteration: 13600 Train loss: 1.200905 Train acc: 0.231445\n",
      "Epoch: 160/200 Iteration: 13605 Train loss: 1.051542 Train acc: 0.503418\n",
      "Epoch: 160/200 Iteration: 13610 Train loss: 1.093545 Train acc: 0.399902\n",
      "Epoch: 160/200 Iteration: 13615 Train loss: 1.136655 Train acc: 0.348633\n",
      "Epoch: 160/200 Iteration: 13620 Train loss: 1.091777 Train acc: 0.410156\n",
      "Epoch: 160/200 Iteration: 13625 Train loss: 1.150580 Train acc: 0.312988\n",
      "Epoch: 160/200 Iteration: 13630 Train loss: 1.058111 Train acc: 0.467773\n",
      "Epoch: 160/200 Iteration: 13635 Train loss: 0.999398 Train acc: 0.579102\n",
      "Epoch: 160/200 Iteration: 13640 Train loss: 0.995959 Train acc: 0.584961\n",
      "Epoch: 160/200 Iteration: 13645 Train loss: 1.136102 Train acc: 0.278320\n",
      "Epoch: 160/200 Iteration: 13650 Train loss: 0.976456 Train acc: 0.631836\n",
      "Epoch: 160/200 Iteration: 13655 Train loss: 1.053928 Train acc: 0.494629\n",
      "Epoch: 160/200 Iteration: 13660 Train loss: 1.102587 Train acc: 0.330566\n",
      "Epoch: 160/200 Iteration: 13665 Train loss: 1.078550 Train acc: 0.452148\n",
      "Epoch: 160/200 Iteration: 13670 Train loss: 1.096826 Train acc: 0.442871\n",
      "Epoch: 160/200 Iteration: 13675 Train loss: 1.009776 Train acc: 0.433594\n",
      "Epoch: 160/200 Iteration: 13680 Train loss: 1.179078 Train acc: 0.354004\n",
      "Epoch: 160/200 Iteration: 13685 Train loss: 1.208416 Train acc: 0.231445\n",
      "Epoch: 161/200 Iteration: 13690 Train loss: 1.052643 Train acc: 0.503418\n",
      "Epoch: 161/200 Iteration: 13695 Train loss: 1.093455 Train acc: 0.399902\n",
      "Epoch: 161/200 Iteration: 13700 Train loss: 1.135294 Train acc: 0.348633\n",
      "Epoch: 161/200 Iteration: 13705 Train loss: 1.091776 Train acc: 0.410156\n",
      "Epoch: 161/200 Iteration: 13710 Train loss: 1.151128 Train acc: 0.312988\n",
      "Epoch: 161/200 Iteration: 13715 Train loss: 1.060048 Train acc: 0.467773\n",
      "Epoch: 161/200 Iteration: 13720 Train loss: 0.998249 Train acc: 0.579102\n",
      "Epoch: 161/200 Iteration: 13725 Train loss: 0.995479 Train acc: 0.584961\n",
      "Epoch: 161/200 Iteration: 13730 Train loss: 1.137836 Train acc: 0.278320\n",
      "Epoch: 161/200 Iteration: 13735 Train loss: 0.978053 Train acc: 0.631836\n",
      "Epoch: 161/200 Iteration: 13740 Train loss: 1.052813 Train acc: 0.494629\n",
      "Epoch: 161/200 Iteration: 13745 Train loss: 1.099079 Train acc: 0.371094\n",
      "Epoch: 161/200 Iteration: 13750 Train loss: 1.086524 Train acc: 0.435547\n",
      "Epoch: 161/200 Iteration: 13755 Train loss: 1.095580 Train acc: 0.454590\n",
      "Epoch: 161/200 Iteration: 13760 Train loss: 1.009429 Train acc: 0.428711\n",
      "Epoch: 161/200 Iteration: 13765 Train loss: 1.185939 Train acc: 0.344238\n",
      "Epoch: 161/200 Iteration: 13770 Train loss: 1.215146 Train acc: 0.231445\n",
      "Epoch: 162/200 Iteration: 13775 Train loss: 1.056444 Train acc: 0.503418\n",
      "Epoch: 162/200 Iteration: 13780 Train loss: 1.093792 Train acc: 0.399902\n",
      "Epoch: 162/200 Iteration: 13785 Train loss: 1.135501 Train acc: 0.348633\n",
      "Epoch: 162/200 Iteration: 13790 Train loss: 1.091250 Train acc: 0.410156\n",
      "Epoch: 162/200 Iteration: 13795 Train loss: 1.151233 Train acc: 0.312988\n",
      "Epoch: 162/200 Iteration: 13800 Train loss: 1.059222 Train acc: 0.467773\n",
      "Epoch: 162/200 Iteration: 13805 Train loss: 0.999158 Train acc: 0.579102\n",
      "Epoch: 162/200 Iteration: 13810 Train loss: 0.995256 Train acc: 0.584961\n",
      "Epoch: 162/200 Iteration: 13815 Train loss: 1.136009 Train acc: 0.278320\n",
      "Epoch: 162/200 Iteration: 13820 Train loss: 0.976338 Train acc: 0.631836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162/200 Iteration: 13825 Train loss: 1.053360 Train acc: 0.494629\n",
      "Epoch: 162/200 Iteration: 13830 Train loss: 1.103008 Train acc: 0.312988\n",
      "Epoch: 162/200 Iteration: 13835 Train loss: 1.079200 Train acc: 0.437988\n",
      "Epoch: 162/200 Iteration: 13840 Train loss: 1.095478 Train acc: 0.455566\n",
      "Epoch: 162/200 Iteration: 13845 Train loss: 1.010243 Train acc: 0.415039\n",
      "Epoch: 162/200 Iteration: 13850 Train loss: 1.184429 Train acc: 0.346680\n",
      "Epoch: 162/200 Iteration: 13855 Train loss: 1.211178 Train acc: 0.231445\n",
      "Epoch: 163/200 Iteration: 13860 Train loss: 1.054917 Train acc: 0.503418\n",
      "Epoch: 163/200 Iteration: 13865 Train loss: 1.093490 Train acc: 0.399902\n",
      "Epoch: 163/200 Iteration: 13870 Train loss: 1.136471 Train acc: 0.348633\n",
      "Epoch: 163/200 Iteration: 13875 Train loss: 1.090979 Train acc: 0.410156\n",
      "Epoch: 163/200 Iteration: 13880 Train loss: 1.150835 Train acc: 0.312988\n",
      "Epoch: 163/200 Iteration: 13885 Train loss: 1.059319 Train acc: 0.467773\n",
      "Epoch: 163/200 Iteration: 13890 Train loss: 0.997429 Train acc: 0.579102\n",
      "Epoch: 163/200 Iteration: 13895 Train loss: 0.995747 Train acc: 0.584961\n",
      "Epoch: 163/200 Iteration: 13900 Train loss: 1.137062 Train acc: 0.278320\n",
      "Epoch: 163/200 Iteration: 13905 Train loss: 0.977759 Train acc: 0.631836\n",
      "Epoch: 163/200 Iteration: 13910 Train loss: 1.052723 Train acc: 0.494629\n",
      "Epoch: 163/200 Iteration: 13915 Train loss: 1.100010 Train acc: 0.369629\n",
      "Epoch: 163/200 Iteration: 13920 Train loss: 1.085672 Train acc: 0.426758\n",
      "Epoch: 163/200 Iteration: 13925 Train loss: 1.095330 Train acc: 0.446289\n",
      "Epoch: 163/200 Iteration: 13930 Train loss: 1.009253 Train acc: 0.432129\n",
      "Epoch: 163/200 Iteration: 13935 Train loss: 1.188577 Train acc: 0.352051\n",
      "Epoch: 163/200 Iteration: 13940 Train loss: 1.214857 Train acc: 0.231445\n",
      "Epoch: 164/200 Iteration: 13945 Train loss: 1.055844 Train acc: 0.503906\n",
      "Epoch: 164/200 Iteration: 13950 Train loss: 1.093713 Train acc: 0.399902\n",
      "Epoch: 164/200 Iteration: 13955 Train loss: 1.134937 Train acc: 0.348633\n",
      "Epoch: 164/200 Iteration: 13960 Train loss: 1.091265 Train acc: 0.410156\n",
      "Epoch: 164/200 Iteration: 13965 Train loss: 1.150874 Train acc: 0.312988\n",
      "Epoch: 164/200 Iteration: 13970 Train loss: 1.059314 Train acc: 0.467773\n",
      "Epoch: 164/200 Iteration: 13975 Train loss: 0.997611 Train acc: 0.579102\n",
      "Epoch: 164/200 Iteration: 13980 Train loss: 0.995549 Train acc: 0.584961\n",
      "Epoch: 164/200 Iteration: 13985 Train loss: 1.136226 Train acc: 0.278320\n",
      "Epoch: 164/200 Iteration: 13990 Train loss: 0.976164 Train acc: 0.631836\n",
      "Epoch: 164/200 Iteration: 13995 Train loss: 1.053212 Train acc: 0.494629\n",
      "Epoch: 164/200 Iteration: 14000 Train loss: 1.099637 Train acc: 0.347656\n",
      "Epoch: 164/200 Iteration: 14005 Train loss: 1.080164 Train acc: 0.447754\n",
      "Epoch: 164/200 Iteration: 14010 Train loss: 1.091266 Train acc: 0.463379\n",
      "Epoch: 164/200 Iteration: 14015 Train loss: 1.009643 Train acc: 0.419434\n",
      "Epoch: 164/200 Iteration: 14020 Train loss: 1.181612 Train acc: 0.346680\n",
      "Epoch: 164/200 Iteration: 14025 Train loss: 1.216119 Train acc: 0.231445\n",
      "Epoch: 165/200 Iteration: 14030 Train loss: 1.055653 Train acc: 0.503418\n",
      "Epoch: 165/200 Iteration: 14035 Train loss: 1.094563 Train acc: 0.399902\n",
      "Epoch: 165/200 Iteration: 14040 Train loss: 1.134150 Train acc: 0.348633\n",
      "Epoch: 165/200 Iteration: 14045 Train loss: 1.090844 Train acc: 0.410156\n",
      "Epoch: 165/200 Iteration: 14050 Train loss: 1.151200 Train acc: 0.312988\n",
      "Epoch: 165/200 Iteration: 14055 Train loss: 1.058719 Train acc: 0.467773\n",
      "Epoch: 165/200 Iteration: 14060 Train loss: 0.998889 Train acc: 0.579102\n",
      "Epoch: 165/200 Iteration: 14065 Train loss: 0.994296 Train acc: 0.584961\n",
      "Epoch: 165/200 Iteration: 14070 Train loss: 1.137780 Train acc: 0.278320\n",
      "Epoch: 165/200 Iteration: 14075 Train loss: 0.975924 Train acc: 0.631836\n",
      "Epoch: 165/200 Iteration: 14080 Train loss: 1.053487 Train acc: 0.494629\n",
      "Epoch: 165/200 Iteration: 14085 Train loss: 1.099358 Train acc: 0.356445\n",
      "Epoch: 165/200 Iteration: 14090 Train loss: 1.085072 Train acc: 0.436035\n",
      "Epoch: 165/200 Iteration: 14095 Train loss: 1.093677 Train acc: 0.455566\n",
      "Epoch: 165/200 Iteration: 14100 Train loss: 1.007409 Train acc: 0.435059\n",
      "Epoch: 165/200 Iteration: 14105 Train loss: 1.189423 Train acc: 0.347656\n",
      "Epoch: 165/200 Iteration: 14110 Train loss: 1.210756 Train acc: 0.231934\n",
      "Epoch: 166/200 Iteration: 14115 Train loss: 1.055626 Train acc: 0.503418\n",
      "Epoch: 166/200 Iteration: 14120 Train loss: 1.094574 Train acc: 0.399902\n",
      "Epoch: 166/200 Iteration: 14125 Train loss: 1.134747 Train acc: 0.348633\n",
      "Epoch: 166/200 Iteration: 14130 Train loss: 1.091466 Train acc: 0.410156\n",
      "Epoch: 166/200 Iteration: 14135 Train loss: 1.150541 Train acc: 0.312988\n",
      "Epoch: 166/200 Iteration: 14140 Train loss: 1.058264 Train acc: 0.467773\n",
      "Epoch: 166/200 Iteration: 14145 Train loss: 0.997384 Train acc: 0.579102\n",
      "Epoch: 166/200 Iteration: 14150 Train loss: 0.994438 Train acc: 0.584961\n",
      "Epoch: 166/200 Iteration: 14155 Train loss: 1.137286 Train acc: 0.278320\n",
      "Epoch: 166/200 Iteration: 14160 Train loss: 0.975712 Train acc: 0.631836\n",
      "Epoch: 166/200 Iteration: 14165 Train loss: 1.052152 Train acc: 0.494629\n",
      "Epoch: 166/200 Iteration: 14170 Train loss: 1.094632 Train acc: 0.384277\n",
      "Epoch: 166/200 Iteration: 14175 Train loss: 1.091128 Train acc: 0.423340\n",
      "Epoch: 166/200 Iteration: 14180 Train loss: 1.092383 Train acc: 0.463867\n",
      "Epoch: 166/200 Iteration: 14185 Train loss: 1.009155 Train acc: 0.435059\n",
      "Epoch: 166/200 Iteration: 14190 Train loss: 1.184881 Train acc: 0.357422\n",
      "Epoch: 166/200 Iteration: 14195 Train loss: 1.217692 Train acc: 0.231445\n",
      "Epoch: 167/200 Iteration: 14200 Train loss: 1.060243 Train acc: 0.503906\n",
      "Epoch: 167/200 Iteration: 14205 Train loss: 1.094985 Train acc: 0.399902\n",
      "Epoch: 167/200 Iteration: 14210 Train loss: 1.134520 Train acc: 0.348633\n",
      "Epoch: 167/200 Iteration: 14215 Train loss: 1.092118 Train acc: 0.410156\n",
      "Epoch: 167/200 Iteration: 14220 Train loss: 1.148746 Train acc: 0.312988\n",
      "Epoch: 167/200 Iteration: 14225 Train loss: 1.058292 Train acc: 0.467773\n",
      "Epoch: 167/200 Iteration: 14230 Train loss: 0.996232 Train acc: 0.579102\n",
      "Epoch: 167/200 Iteration: 14235 Train loss: 0.993712 Train acc: 0.584961\n",
      "Epoch: 167/200 Iteration: 14240 Train loss: 1.134042 Train acc: 0.278809\n",
      "Epoch: 167/200 Iteration: 14245 Train loss: 0.975046 Train acc: 0.631836\n",
      "Epoch: 167/200 Iteration: 14250 Train loss: 1.055051 Train acc: 0.494629\n",
      "Epoch: 167/200 Iteration: 14255 Train loss: 1.118385 Train acc: 0.244629\n",
      "Epoch: 167/200 Iteration: 14260 Train loss: 1.063178 Train acc: 0.461426\n",
      "Epoch: 167/200 Iteration: 14265 Train loss: 1.090400 Train acc: 0.463867\n",
      "Epoch: 167/200 Iteration: 14270 Train loss: 1.012450 Train acc: 0.413574\n",
      "Epoch: 167/200 Iteration: 14275 Train loss: 1.172333 Train acc: 0.351562\n",
      "Epoch: 167/200 Iteration: 14280 Train loss: 1.195465 Train acc: 0.233398\n",
      "Epoch: 168/200 Iteration: 14285 Train loss: 1.052229 Train acc: 0.503418\n",
      "Epoch: 168/200 Iteration: 14290 Train loss: 1.094187 Train acc: 0.400391\n",
      "Epoch: 168/200 Iteration: 14295 Train loss: 1.135787 Train acc: 0.348633\n",
      "Epoch: 168/200 Iteration: 14300 Train loss: 1.089982 Train acc: 0.410156\n",
      "Epoch: 168/200 Iteration: 14305 Train loss: 1.152464 Train acc: 0.312988\n",
      "Epoch: 168/200 Iteration: 14310 Train loss: 1.058236 Train acc: 0.467773\n",
      "Epoch: 168/200 Iteration: 14315 Train loss: 0.997861 Train acc: 0.579102\n",
      "Epoch: 168/200 Iteration: 14320 Train loss: 0.995121 Train acc: 0.584961\n",
      "Epoch: 168/200 Iteration: 14325 Train loss: 1.139030 Train acc: 0.278320\n",
      "Epoch: 168/200 Iteration: 14330 Train loss: 0.975358 Train acc: 0.631836\n",
      "Epoch: 168/200 Iteration: 14335 Train loss: 1.052850 Train acc: 0.494629\n",
      "Epoch: 168/200 Iteration: 14340 Train loss: 1.090180 Train acc: 0.414062\n",
      "Epoch: 168/200 Iteration: 14345 Train loss: 1.113850 Train acc: 0.304199\n",
      "Epoch: 168/200 Iteration: 14350 Train loss: 1.101317 Train acc: 0.406738\n",
      "Epoch: 168/200 Iteration: 14355 Train loss: 1.004564 Train acc: 0.490234\n",
      "Epoch: 168/200 Iteration: 14360 Train loss: 1.197248 Train acc: 0.343262\n",
      "Epoch: 168/200 Iteration: 14365 Train loss: 1.211860 Train acc: 0.231445\n",
      "Epoch: 169/200 Iteration: 14370 Train loss: 1.062172 Train acc: 0.503906\n",
      "Epoch: 169/200 Iteration: 14375 Train loss: 1.095380 Train acc: 0.399902\n",
      "Epoch: 169/200 Iteration: 14380 Train loss: 1.133213 Train acc: 0.348633\n",
      "Epoch: 169/200 Iteration: 14385 Train loss: 1.093905 Train acc: 0.410156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169/200 Iteration: 14390 Train loss: 1.145466 Train acc: 0.312988\n",
      "Epoch: 169/200 Iteration: 14395 Train loss: 1.059386 Train acc: 0.467773\n",
      "Epoch: 169/200 Iteration: 14400 Train loss: 0.996598 Train acc: 0.579102\n",
      "Epoch: 169/200 Iteration: 14405 Train loss: 0.995397 Train acc: 0.584961\n",
      "Epoch: 169/200 Iteration: 14410 Train loss: 1.131951 Train acc: 0.278320\n",
      "Epoch: 169/200 Iteration: 14415 Train loss: 0.976918 Train acc: 0.631836\n",
      "Epoch: 169/200 Iteration: 14420 Train loss: 1.056154 Train acc: 0.494629\n",
      "Epoch: 169/200 Iteration: 14425 Train loss: 1.124503 Train acc: 0.245117\n",
      "Epoch: 169/200 Iteration: 14430 Train loss: 1.066794 Train acc: 0.466309\n",
      "Epoch: 169/200 Iteration: 14435 Train loss: 1.090249 Train acc: 0.477051\n",
      "Epoch: 169/200 Iteration: 14440 Train loss: 1.018270 Train acc: 0.411621\n",
      "Epoch: 169/200 Iteration: 14445 Train loss: 1.164879 Train acc: 0.321777\n",
      "Epoch: 169/200 Iteration: 14450 Train loss: 1.192738 Train acc: 0.231445\n",
      "Epoch: 170/200 Iteration: 14455 Train loss: 1.050223 Train acc: 0.503418\n",
      "Epoch: 170/200 Iteration: 14460 Train loss: 1.093087 Train acc: 0.399902\n",
      "Epoch: 170/200 Iteration: 14465 Train loss: 1.135136 Train acc: 0.348633\n",
      "Epoch: 170/200 Iteration: 14470 Train loss: 1.090505 Train acc: 0.410156\n",
      "Epoch: 170/200 Iteration: 14475 Train loss: 1.149926 Train acc: 0.312988\n",
      "Epoch: 170/200 Iteration: 14480 Train loss: 1.058576 Train acc: 0.467773\n",
      "Epoch: 170/200 Iteration: 14485 Train loss: 0.999004 Train acc: 0.579102\n",
      "Epoch: 170/200 Iteration: 14490 Train loss: 0.994657 Train acc: 0.584961\n",
      "Epoch: 170/200 Iteration: 14495 Train loss: 1.139885 Train acc: 0.278320\n",
      "Epoch: 170/200 Iteration: 14500 Train loss: 0.976085 Train acc: 0.631836\n",
      "Epoch: 170/200 Iteration: 14505 Train loss: 1.052610 Train acc: 0.494629\n",
      "Epoch: 170/200 Iteration: 14510 Train loss: 1.091539 Train acc: 0.405273\n",
      "Epoch: 170/200 Iteration: 14515 Train loss: 1.107623 Train acc: 0.324707\n",
      "Epoch: 170/200 Iteration: 14520 Train loss: 1.100711 Train acc: 0.434082\n",
      "Epoch: 170/200 Iteration: 14525 Train loss: 1.004489 Train acc: 0.474609\n",
      "Epoch: 170/200 Iteration: 14530 Train loss: 1.189203 Train acc: 0.348633\n",
      "Epoch: 170/200 Iteration: 14535 Train loss: 1.219579 Train acc: 0.233398\n",
      "Epoch: 171/200 Iteration: 14540 Train loss: 1.062407 Train acc: 0.503418\n",
      "Epoch: 171/200 Iteration: 14545 Train loss: 1.095199 Train acc: 0.401367\n",
      "Epoch: 171/200 Iteration: 14550 Train loss: 1.136951 Train acc: 0.348633\n",
      "Epoch: 171/200 Iteration: 14555 Train loss: 1.092937 Train acc: 0.410156\n",
      "Epoch: 171/200 Iteration: 14560 Train loss: 1.149188 Train acc: 0.312988\n",
      "Epoch: 171/200 Iteration: 14565 Train loss: 1.060811 Train acc: 0.467773\n",
      "Epoch: 171/200 Iteration: 14570 Train loss: 0.994807 Train acc: 0.579102\n",
      "Epoch: 171/200 Iteration: 14575 Train loss: 0.994145 Train acc: 0.584961\n",
      "Epoch: 171/200 Iteration: 14580 Train loss: 1.131728 Train acc: 0.278809\n",
      "Epoch: 171/200 Iteration: 14585 Train loss: 0.976101 Train acc: 0.631836\n",
      "Epoch: 171/200 Iteration: 14590 Train loss: 1.056502 Train acc: 0.494629\n",
      "Epoch: 171/200 Iteration: 14595 Train loss: 1.125165 Train acc: 0.230957\n",
      "Epoch: 171/200 Iteration: 14600 Train loss: 1.063332 Train acc: 0.457520\n",
      "Epoch: 171/200 Iteration: 14605 Train loss: 1.089514 Train acc: 0.468262\n",
      "Epoch: 171/200 Iteration: 14610 Train loss: 1.014029 Train acc: 0.411621\n",
      "Epoch: 171/200 Iteration: 14615 Train loss: 1.170889 Train acc: 0.313477\n",
      "Epoch: 171/200 Iteration: 14620 Train loss: 1.182972 Train acc: 0.233887\n",
      "Epoch: 172/200 Iteration: 14625 Train loss: 1.049875 Train acc: 0.503418\n",
      "Epoch: 172/200 Iteration: 14630 Train loss: 1.092571 Train acc: 0.399902\n",
      "Epoch: 172/200 Iteration: 14635 Train loss: 1.133749 Train acc: 0.348633\n",
      "Epoch: 172/200 Iteration: 14640 Train loss: 1.090272 Train acc: 0.410156\n",
      "Epoch: 172/200 Iteration: 14645 Train loss: 1.151429 Train acc: 0.312988\n",
      "Epoch: 172/200 Iteration: 14650 Train loss: 1.058908 Train acc: 0.467773\n",
      "Epoch: 172/200 Iteration: 14655 Train loss: 0.999438 Train acc: 0.579102\n",
      "Epoch: 172/200 Iteration: 14660 Train loss: 0.994726 Train acc: 0.584961\n",
      "Epoch: 172/200 Iteration: 14665 Train loss: 1.141291 Train acc: 0.278320\n",
      "Epoch: 172/200 Iteration: 14670 Train loss: 0.974024 Train acc: 0.631836\n",
      "Epoch: 172/200 Iteration: 14675 Train loss: 1.051808 Train acc: 0.494629\n",
      "Epoch: 172/200 Iteration: 14680 Train loss: 1.089698 Train acc: 0.402832\n",
      "Epoch: 172/200 Iteration: 14685 Train loss: 1.122575 Train acc: 0.258789\n",
      "Epoch: 172/200 Iteration: 14690 Train loss: 1.098734 Train acc: 0.442871\n",
      "Epoch: 172/200 Iteration: 14695 Train loss: 1.004178 Train acc: 0.486816\n",
      "Epoch: 172/200 Iteration: 14700 Train loss: 1.199555 Train acc: 0.343750\n",
      "Epoch: 172/200 Iteration: 14705 Train loss: 1.212769 Train acc: 0.243652\n",
      "Epoch: 173/200 Iteration: 14710 Train loss: 1.066183 Train acc: 0.502930\n",
      "Epoch: 173/200 Iteration: 14715 Train loss: 1.094196 Train acc: 0.400879\n",
      "Epoch: 173/200 Iteration: 14720 Train loss: 1.134654 Train acc: 0.348633\n",
      "Epoch: 173/200 Iteration: 14725 Train loss: 1.094465 Train acc: 0.410156\n",
      "Epoch: 173/200 Iteration: 14730 Train loss: 1.144528 Train acc: 0.312988\n",
      "Epoch: 173/200 Iteration: 14735 Train loss: 1.060670 Train acc: 0.467773\n",
      "Epoch: 173/200 Iteration: 14740 Train loss: 0.995721 Train acc: 0.579102\n",
      "Epoch: 173/200 Iteration: 14745 Train loss: 0.995748 Train acc: 0.584961\n",
      "Epoch: 173/200 Iteration: 14750 Train loss: 1.129045 Train acc: 0.278320\n",
      "Epoch: 173/200 Iteration: 14755 Train loss: 0.977438 Train acc: 0.631836\n",
      "Epoch: 173/200 Iteration: 14760 Train loss: 1.057860 Train acc: 0.494629\n",
      "Epoch: 173/200 Iteration: 14765 Train loss: 1.134229 Train acc: 0.231445\n",
      "Epoch: 173/200 Iteration: 14770 Train loss: 1.066397 Train acc: 0.466797\n",
      "Epoch: 173/200 Iteration: 14775 Train loss: 1.090334 Train acc: 0.472656\n",
      "Epoch: 173/200 Iteration: 14780 Train loss: 1.017918 Train acc: 0.411621\n",
      "Epoch: 173/200 Iteration: 14785 Train loss: 1.161682 Train acc: 0.306641\n",
      "Epoch: 173/200 Iteration: 14790 Train loss: 1.186657 Train acc: 0.232422\n",
      "Epoch: 174/200 Iteration: 14795 Train loss: 1.049116 Train acc: 0.503418\n",
      "Epoch: 174/200 Iteration: 14800 Train loss: 1.093486 Train acc: 0.399902\n",
      "Epoch: 174/200 Iteration: 14805 Train loss: 1.134260 Train acc: 0.348633\n",
      "Epoch: 174/200 Iteration: 14810 Train loss: 1.090214 Train acc: 0.410156\n",
      "Epoch: 174/200 Iteration: 14815 Train loss: 1.147909 Train acc: 0.312988\n",
      "Epoch: 174/200 Iteration: 14820 Train loss: 1.058889 Train acc: 0.467773\n",
      "Epoch: 174/200 Iteration: 14825 Train loss: 0.999450 Train acc: 0.579102\n",
      "Epoch: 174/200 Iteration: 14830 Train loss: 0.997599 Train acc: 0.584961\n",
      "Epoch: 174/200 Iteration: 14835 Train loss: 1.138094 Train acc: 0.278320\n",
      "Epoch: 174/200 Iteration: 14840 Train loss: 0.978530 Train acc: 0.631836\n",
      "Epoch: 174/200 Iteration: 14845 Train loss: 1.051745 Train acc: 0.494629\n",
      "Epoch: 174/200 Iteration: 14850 Train loss: 1.089906 Train acc: 0.405273\n",
      "Epoch: 174/200 Iteration: 14855 Train loss: 1.113891 Train acc: 0.296875\n",
      "Epoch: 174/200 Iteration: 14860 Train loss: 1.099300 Train acc: 0.419922\n",
      "Epoch: 174/200 Iteration: 14865 Train loss: 1.006137 Train acc: 0.488281\n",
      "Epoch: 174/200 Iteration: 14870 Train loss: 1.185260 Train acc: 0.349121\n",
      "Epoch: 174/200 Iteration: 14875 Train loss: 1.218302 Train acc: 0.229980\n",
      "Epoch: 175/200 Iteration: 14880 Train loss: 1.064324 Train acc: 0.502930\n",
      "Epoch: 175/200 Iteration: 14885 Train loss: 1.095358 Train acc: 0.400391\n",
      "Epoch: 175/200 Iteration: 14890 Train loss: 1.135478 Train acc: 0.348633\n",
      "Epoch: 175/200 Iteration: 14895 Train loss: 1.093699 Train acc: 0.410156\n",
      "Epoch: 175/200 Iteration: 14900 Train loss: 1.148826 Train acc: 0.312988\n",
      "Epoch: 175/200 Iteration: 14905 Train loss: 1.058753 Train acc: 0.467773\n",
      "Epoch: 175/200 Iteration: 14910 Train loss: 0.996696 Train acc: 0.579102\n",
      "Epoch: 175/200 Iteration: 14915 Train loss: 0.993155 Train acc: 0.584961\n",
      "Epoch: 175/200 Iteration: 14920 Train loss: 1.129165 Train acc: 0.278320\n",
      "Epoch: 175/200 Iteration: 14925 Train loss: 0.974513 Train acc: 0.631836\n",
      "Epoch: 175/200 Iteration: 14930 Train loss: 1.057095 Train acc: 0.494629\n",
      "Epoch: 175/200 Iteration: 14935 Train loss: 1.128602 Train acc: 0.230469\n",
      "Epoch: 175/200 Iteration: 14940 Train loss: 1.062971 Train acc: 0.459473\n",
      "Epoch: 175/200 Iteration: 14945 Train loss: 1.093325 Train acc: 0.467285\n",
      "Epoch: 175/200 Iteration: 14950 Train loss: 1.013816 Train acc: 0.412598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175/200 Iteration: 14955 Train loss: 1.166613 Train acc: 0.335938\n",
      "Epoch: 175/200 Iteration: 14960 Train loss: 1.183912 Train acc: 0.231445\n",
      "Epoch: 176/200 Iteration: 14965 Train loss: 1.049298 Train acc: 0.503418\n",
      "Epoch: 176/200 Iteration: 14970 Train loss: 1.092544 Train acc: 0.399902\n",
      "Epoch: 176/200 Iteration: 14975 Train loss: 1.136592 Train acc: 0.348633\n",
      "Epoch: 176/200 Iteration: 14980 Train loss: 1.088860 Train acc: 0.410156\n",
      "Epoch: 176/200 Iteration: 14985 Train loss: 1.152080 Train acc: 0.312988\n",
      "Epoch: 176/200 Iteration: 14990 Train loss: 1.057238 Train acc: 0.467773\n",
      "Epoch: 176/200 Iteration: 14995 Train loss: 0.999885 Train acc: 0.579102\n",
      "Epoch: 176/200 Iteration: 15000 Train loss: 0.994902 Train acc: 0.584961\n",
      "Epoch: 176/200 Iteration: 15005 Train loss: 1.139756 Train acc: 0.278320\n",
      "Epoch: 176/200 Iteration: 15010 Train loss: 0.976098 Train acc: 0.631836\n",
      "Epoch: 176/200 Iteration: 15015 Train loss: 1.051743 Train acc: 0.494629\n",
      "Epoch: 176/200 Iteration: 15020 Train loss: 1.089354 Train acc: 0.403320\n",
      "Epoch: 176/200 Iteration: 15025 Train loss: 1.120921 Train acc: 0.268555\n",
      "Epoch: 176/200 Iteration: 15030 Train loss: 1.095520 Train acc: 0.454590\n",
      "Epoch: 176/200 Iteration: 15035 Train loss: 1.005162 Train acc: 0.477051\n",
      "Epoch: 176/200 Iteration: 15040 Train loss: 1.190131 Train acc: 0.342773\n",
      "Epoch: 176/200 Iteration: 15045 Train loss: 1.212644 Train acc: 0.233887\n",
      "Epoch: 177/200 Iteration: 15050 Train loss: 1.066089 Train acc: 0.503906\n",
      "Epoch: 177/200 Iteration: 15055 Train loss: 1.094837 Train acc: 0.399902\n",
      "Epoch: 177/200 Iteration: 15060 Train loss: 1.133332 Train acc: 0.348633\n",
      "Epoch: 177/200 Iteration: 15065 Train loss: 1.093896 Train acc: 0.410156\n",
      "Epoch: 177/200 Iteration: 15070 Train loss: 1.144752 Train acc: 0.312988\n",
      "Epoch: 177/200 Iteration: 15075 Train loss: 1.059288 Train acc: 0.467773\n",
      "Epoch: 177/200 Iteration: 15080 Train loss: 0.997094 Train acc: 0.579102\n",
      "Epoch: 177/200 Iteration: 15085 Train loss: 0.995427 Train acc: 0.584961\n",
      "Epoch: 177/200 Iteration: 15090 Train loss: 1.129691 Train acc: 0.278809\n",
      "Epoch: 177/200 Iteration: 15095 Train loss: 0.977355 Train acc: 0.631836\n",
      "Epoch: 177/200 Iteration: 15100 Train loss: 1.058174 Train acc: 0.494629\n",
      "Epoch: 177/200 Iteration: 15105 Train loss: 1.131757 Train acc: 0.236816\n",
      "Epoch: 177/200 Iteration: 15110 Train loss: 1.064295 Train acc: 0.459473\n",
      "Epoch: 177/200 Iteration: 15115 Train loss: 1.089141 Train acc: 0.471680\n",
      "Epoch: 177/200 Iteration: 15120 Train loss: 1.016488 Train acc: 0.411621\n",
      "Epoch: 177/200 Iteration: 15125 Train loss: 1.166183 Train acc: 0.318359\n",
      "Epoch: 177/200 Iteration: 15130 Train loss: 1.187020 Train acc: 0.231934\n",
      "Epoch: 178/200 Iteration: 15135 Train loss: 1.049039 Train acc: 0.503418\n",
      "Epoch: 178/200 Iteration: 15140 Train loss: 1.093557 Train acc: 0.399902\n",
      "Epoch: 178/200 Iteration: 15145 Train loss: 1.133356 Train acc: 0.348633\n",
      "Epoch: 178/200 Iteration: 15150 Train loss: 1.089592 Train acc: 0.410156\n",
      "Epoch: 178/200 Iteration: 15155 Train loss: 1.149135 Train acc: 0.312988\n",
      "Epoch: 178/200 Iteration: 15160 Train loss: 1.058175 Train acc: 0.467773\n",
      "Epoch: 178/200 Iteration: 15165 Train loss: 0.998926 Train acc: 0.579102\n",
      "Epoch: 178/200 Iteration: 15170 Train loss: 0.995283 Train acc: 0.584961\n",
      "Epoch: 178/200 Iteration: 15175 Train loss: 1.141268 Train acc: 0.278320\n",
      "Epoch: 178/200 Iteration: 15180 Train loss: 0.974874 Train acc: 0.631836\n",
      "Epoch: 178/200 Iteration: 15185 Train loss: 1.051588 Train acc: 0.494629\n",
      "Epoch: 178/200 Iteration: 15190 Train loss: 1.088723 Train acc: 0.407227\n",
      "Epoch: 178/200 Iteration: 15195 Train loss: 1.114850 Train acc: 0.293945\n",
      "Epoch: 178/200 Iteration: 15200 Train loss: 1.099997 Train acc: 0.438965\n",
      "Epoch: 178/200 Iteration: 15205 Train loss: 1.006174 Train acc: 0.480957\n",
      "Epoch: 178/200 Iteration: 15210 Train loss: 1.185568 Train acc: 0.340820\n",
      "Epoch: 178/200 Iteration: 15215 Train loss: 1.220359 Train acc: 0.233887\n",
      "Epoch: 179/200 Iteration: 15220 Train loss: 1.063382 Train acc: 0.504395\n",
      "Epoch: 179/200 Iteration: 15225 Train loss: 1.094839 Train acc: 0.401855\n",
      "Epoch: 179/200 Iteration: 15230 Train loss: 1.138345 Train acc: 0.348633\n",
      "Epoch: 179/200 Iteration: 15235 Train loss: 1.093420 Train acc: 0.410156\n",
      "Epoch: 179/200 Iteration: 15240 Train loss: 1.150463 Train acc: 0.312988\n",
      "Epoch: 179/200 Iteration: 15245 Train loss: 1.059503 Train acc: 0.467773\n",
      "Epoch: 179/200 Iteration: 15250 Train loss: 0.996136 Train acc: 0.579102\n",
      "Epoch: 179/200 Iteration: 15255 Train loss: 0.993387 Train acc: 0.584961\n",
      "Epoch: 179/200 Iteration: 15260 Train loss: 1.130220 Train acc: 0.278320\n",
      "Epoch: 179/200 Iteration: 15265 Train loss: 0.975467 Train acc: 0.631836\n",
      "Epoch: 179/200 Iteration: 15270 Train loss: 1.056822 Train acc: 0.494629\n",
      "Epoch: 179/200 Iteration: 15275 Train loss: 1.126750 Train acc: 0.226074\n",
      "Epoch: 179/200 Iteration: 15280 Train loss: 1.063493 Train acc: 0.452148\n",
      "Epoch: 179/200 Iteration: 15285 Train loss: 1.092358 Train acc: 0.465820\n",
      "Epoch: 179/200 Iteration: 15290 Train loss: 1.012987 Train acc: 0.412109\n",
      "Epoch: 179/200 Iteration: 15295 Train loss: 1.170260 Train acc: 0.327637\n",
      "Epoch: 179/200 Iteration: 15300 Train loss: 1.184901 Train acc: 0.231445\n",
      "Epoch: 180/200 Iteration: 15305 Train loss: 1.048195 Train acc: 0.503418\n",
      "Epoch: 180/200 Iteration: 15310 Train loss: 1.093296 Train acc: 0.399902\n",
      "Epoch: 180/200 Iteration: 15315 Train loss: 1.137022 Train acc: 0.348633\n",
      "Epoch: 180/200 Iteration: 15320 Train loss: 1.090582 Train acc: 0.410156\n",
      "Epoch: 180/200 Iteration: 15325 Train loss: 1.151547 Train acc: 0.312988\n",
      "Epoch: 180/200 Iteration: 15330 Train loss: 1.059338 Train acc: 0.467773\n",
      "Epoch: 180/200 Iteration: 15335 Train loss: 0.999092 Train acc: 0.579102\n",
      "Epoch: 180/200 Iteration: 15340 Train loss: 0.995440 Train acc: 0.584961\n",
      "Epoch: 180/200 Iteration: 15345 Train loss: 1.140875 Train acc: 0.278320\n",
      "Epoch: 180/200 Iteration: 15350 Train loss: 0.975475 Train acc: 0.631836\n",
      "Epoch: 180/200 Iteration: 15355 Train loss: 1.050481 Train acc: 0.494629\n",
      "Epoch: 180/200 Iteration: 15360 Train loss: 1.088656 Train acc: 0.402832\n",
      "Epoch: 180/200 Iteration: 15365 Train loss: 1.121431 Train acc: 0.254395\n",
      "Epoch: 180/200 Iteration: 15370 Train loss: 1.094803 Train acc: 0.459961\n",
      "Epoch: 180/200 Iteration: 15375 Train loss: 1.006029 Train acc: 0.458984\n",
      "Epoch: 180/200 Iteration: 15380 Train loss: 1.192931 Train acc: 0.346680\n",
      "Epoch: 180/200 Iteration: 15385 Train loss: 1.213951 Train acc: 0.233398\n",
      "Epoch: 181/200 Iteration: 15390 Train loss: 1.065474 Train acc: 0.503418\n",
      "Epoch: 181/200 Iteration: 15395 Train loss: 1.093007 Train acc: 0.399902\n",
      "Epoch: 181/200 Iteration: 15400 Train loss: 1.132709 Train acc: 0.348633\n",
      "Epoch: 181/200 Iteration: 15405 Train loss: 1.093601 Train acc: 0.410156\n",
      "Epoch: 181/200 Iteration: 15410 Train loss: 1.144850 Train acc: 0.312988\n",
      "Epoch: 181/200 Iteration: 15415 Train loss: 1.059662 Train acc: 0.467773\n",
      "Epoch: 181/200 Iteration: 15420 Train loss: 0.996194 Train acc: 0.579102\n",
      "Epoch: 181/200 Iteration: 15425 Train loss: 0.996395 Train acc: 0.584961\n",
      "Epoch: 181/200 Iteration: 15430 Train loss: 1.128214 Train acc: 0.278320\n",
      "Epoch: 181/200 Iteration: 15435 Train loss: 0.976517 Train acc: 0.631836\n",
      "Epoch: 181/200 Iteration: 15440 Train loss: 1.058000 Train acc: 0.494629\n",
      "Epoch: 181/200 Iteration: 15445 Train loss: 1.127637 Train acc: 0.236816\n",
      "Epoch: 181/200 Iteration: 15450 Train loss: 1.069004 Train acc: 0.463379\n",
      "Epoch: 181/200 Iteration: 15455 Train loss: 1.091981 Train acc: 0.468262\n",
      "Epoch: 181/200 Iteration: 15460 Train loss: 1.014137 Train acc: 0.412598\n",
      "Epoch: 181/200 Iteration: 15465 Train loss: 1.168189 Train acc: 0.321777\n",
      "Epoch: 181/200 Iteration: 15470 Train loss: 1.193936 Train acc: 0.230957\n",
      "Epoch: 182/200 Iteration: 15475 Train loss: 1.048682 Train acc: 0.503418\n",
      "Epoch: 182/200 Iteration: 15480 Train loss: 1.094190 Train acc: 0.399902\n",
      "Epoch: 182/200 Iteration: 15485 Train loss: 1.134467 Train acc: 0.348633\n",
      "Epoch: 182/200 Iteration: 15490 Train loss: 1.090826 Train acc: 0.410156\n",
      "Epoch: 182/200 Iteration: 15495 Train loss: 1.149293 Train acc: 0.312988\n",
      "Epoch: 182/200 Iteration: 15500 Train loss: 1.056754 Train acc: 0.467773\n",
      "Epoch: 182/200 Iteration: 15505 Train loss: 0.998385 Train acc: 0.579102\n",
      "Epoch: 182/200 Iteration: 15510 Train loss: 0.994270 Train acc: 0.584961\n",
      "Epoch: 182/200 Iteration: 15515 Train loss: 1.141320 Train acc: 0.278320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182/200 Iteration: 15520 Train loss: 0.973622 Train acc: 0.631836\n",
      "Epoch: 182/200 Iteration: 15525 Train loss: 1.051803 Train acc: 0.494629\n",
      "Epoch: 182/200 Iteration: 15530 Train loss: 1.092565 Train acc: 0.409180\n",
      "Epoch: 182/200 Iteration: 15535 Train loss: 1.101372 Train acc: 0.375977\n",
      "Epoch: 182/200 Iteration: 15540 Train loss: 1.098835 Train acc: 0.447754\n",
      "Epoch: 182/200 Iteration: 15545 Train loss: 1.007071 Train acc: 0.458984\n",
      "Epoch: 182/200 Iteration: 15550 Train loss: 1.189508 Train acc: 0.347656\n",
      "Epoch: 182/200 Iteration: 15555 Train loss: 1.220227 Train acc: 0.231934\n",
      "Epoch: 183/200 Iteration: 15560 Train loss: 1.060030 Train acc: 0.503418\n",
      "Epoch: 183/200 Iteration: 15565 Train loss: 1.094832 Train acc: 0.399902\n",
      "Epoch: 183/200 Iteration: 15570 Train loss: 1.137949 Train acc: 0.348633\n",
      "Epoch: 183/200 Iteration: 15575 Train loss: 1.093312 Train acc: 0.410156\n",
      "Epoch: 183/200 Iteration: 15580 Train loss: 1.150380 Train acc: 0.312988\n",
      "Epoch: 183/200 Iteration: 15585 Train loss: 1.058747 Train acc: 0.467773\n",
      "Epoch: 183/200 Iteration: 15590 Train loss: 0.994941 Train acc: 0.579102\n",
      "Epoch: 183/200 Iteration: 15595 Train loss: 0.993594 Train acc: 0.584961\n",
      "Epoch: 183/200 Iteration: 15600 Train loss: 1.132452 Train acc: 0.278320\n",
      "Epoch: 183/200 Iteration: 15605 Train loss: 0.974992 Train acc: 0.631836\n",
      "Epoch: 183/200 Iteration: 15610 Train loss: 1.054833 Train acc: 0.494629\n",
      "Epoch: 183/200 Iteration: 15615 Train loss: 1.117568 Train acc: 0.248047\n",
      "Epoch: 183/200 Iteration: 15620 Train loss: 1.061298 Train acc: 0.460449\n",
      "Epoch: 183/200 Iteration: 15625 Train loss: 1.092899 Train acc: 0.463379\n",
      "Epoch: 183/200 Iteration: 15630 Train loss: 1.012274 Train acc: 0.412598\n",
      "Epoch: 183/200 Iteration: 15635 Train loss: 1.175413 Train acc: 0.339844\n",
      "Epoch: 183/200 Iteration: 15640 Train loss: 1.203528 Train acc: 0.231445\n",
      "Epoch: 184/200 Iteration: 15645 Train loss: 1.051826 Train acc: 0.503418\n",
      "Epoch: 184/200 Iteration: 15650 Train loss: 1.093352 Train acc: 0.399902\n",
      "Epoch: 184/200 Iteration: 15655 Train loss: 1.131359 Train acc: 0.348633\n",
      "Epoch: 184/200 Iteration: 15660 Train loss: 1.091314 Train acc: 0.410156\n",
      "Epoch: 184/200 Iteration: 15665 Train loss: 1.152102 Train acc: 0.312988\n",
      "Epoch: 184/200 Iteration: 15670 Train loss: 1.058363 Train acc: 0.467773\n",
      "Epoch: 184/200 Iteration: 15675 Train loss: 0.996720 Train acc: 0.579102\n",
      "Epoch: 184/200 Iteration: 15680 Train loss: 0.995962 Train acc: 0.584961\n",
      "Epoch: 184/200 Iteration: 15685 Train loss: 1.140280 Train acc: 0.278320\n",
      "Epoch: 184/200 Iteration: 15690 Train loss: 0.975257 Train acc: 0.631836\n",
      "Epoch: 184/200 Iteration: 15695 Train loss: 1.052000 Train acc: 0.494629\n",
      "Epoch: 184/200 Iteration: 15700 Train loss: 1.095322 Train acc: 0.392578\n",
      "Epoch: 184/200 Iteration: 15705 Train loss: 1.098152 Train acc: 0.386230\n",
      "Epoch: 184/200 Iteration: 15710 Train loss: 1.094973 Train acc: 0.461426\n",
      "Epoch: 184/200 Iteration: 15715 Train loss: 1.006415 Train acc: 0.447266\n",
      "Epoch: 184/200 Iteration: 15720 Train loss: 1.193531 Train acc: 0.341797\n",
      "Epoch: 184/200 Iteration: 15725 Train loss: 1.220552 Train acc: 0.231445\n",
      "Epoch: 185/200 Iteration: 15730 Train loss: 1.059503 Train acc: 0.503418\n",
      "Epoch: 185/200 Iteration: 15735 Train loss: 1.094337 Train acc: 0.399902\n",
      "Epoch: 185/200 Iteration: 15740 Train loss: 1.136154 Train acc: 0.348633\n",
      "Epoch: 185/200 Iteration: 15745 Train loss: 1.092323 Train acc: 0.410156\n",
      "Epoch: 185/200 Iteration: 15750 Train loss: 1.148947 Train acc: 0.312988\n",
      "Epoch: 185/200 Iteration: 15755 Train loss: 1.059918 Train acc: 0.467773\n",
      "Epoch: 185/200 Iteration: 15760 Train loss: 0.995043 Train acc: 0.579102\n",
      "Epoch: 185/200 Iteration: 15765 Train loss: 0.995316 Train acc: 0.584961\n",
      "Epoch: 185/200 Iteration: 15770 Train loss: 1.135134 Train acc: 0.278809\n",
      "Epoch: 185/200 Iteration: 15775 Train loss: 0.977155 Train acc: 0.631836\n",
      "Epoch: 185/200 Iteration: 15780 Train loss: 1.054256 Train acc: 0.494629\n",
      "Epoch: 185/200 Iteration: 15785 Train loss: 1.109257 Train acc: 0.270508\n",
      "Epoch: 185/200 Iteration: 15790 Train loss: 1.068674 Train acc: 0.458008\n",
      "Epoch: 185/200 Iteration: 15795 Train loss: 1.093197 Train acc: 0.470215\n",
      "Epoch: 185/200 Iteration: 15800 Train loss: 1.012548 Train acc: 0.411133\n",
      "Epoch: 185/200 Iteration: 15805 Train loss: 1.173664 Train acc: 0.333008\n",
      "Epoch: 185/200 Iteration: 15810 Train loss: 1.209266 Train acc: 0.231445\n",
      "Epoch: 186/200 Iteration: 15815 Train loss: 1.054103 Train acc: 0.503418\n",
      "Epoch: 186/200 Iteration: 15820 Train loss: 1.093380 Train acc: 0.399902\n",
      "Epoch: 186/200 Iteration: 15825 Train loss: 1.135577 Train acc: 0.348633\n",
      "Epoch: 186/200 Iteration: 15830 Train loss: 1.090680 Train acc: 0.410156\n",
      "Epoch: 186/200 Iteration: 15835 Train loss: 1.151317 Train acc: 0.312988\n",
      "Epoch: 186/200 Iteration: 15840 Train loss: 1.057948 Train acc: 0.467773\n",
      "Epoch: 186/200 Iteration: 15845 Train loss: 0.996738 Train acc: 0.579102\n",
      "Epoch: 186/200 Iteration: 15850 Train loss: 0.993769 Train acc: 0.584961\n",
      "Epoch: 186/200 Iteration: 15855 Train loss: 1.138874 Train acc: 0.278320\n",
      "Epoch: 186/200 Iteration: 15860 Train loss: 0.975456 Train acc: 0.631836\n",
      "Epoch: 186/200 Iteration: 15865 Train loss: 1.052739 Train acc: 0.494629\n",
      "Epoch: 186/200 Iteration: 15870 Train loss: 1.095140 Train acc: 0.400391\n",
      "Epoch: 186/200 Iteration: 15875 Train loss: 1.093563 Train acc: 0.402344\n",
      "Epoch: 186/200 Iteration: 15880 Train loss: 1.095971 Train acc: 0.448730\n",
      "Epoch: 186/200 Iteration: 15885 Train loss: 1.007277 Train acc: 0.435547\n",
      "Epoch: 186/200 Iteration: 15890 Train loss: 1.191338 Train acc: 0.352051\n",
      "Epoch: 186/200 Iteration: 15895 Train loss: 1.218382 Train acc: 0.231445\n",
      "Epoch: 187/200 Iteration: 15900 Train loss: 1.057955 Train acc: 0.503418\n",
      "Epoch: 187/200 Iteration: 15905 Train loss: 1.094547 Train acc: 0.399902\n",
      "Epoch: 187/200 Iteration: 15910 Train loss: 1.133887 Train acc: 0.348633\n",
      "Epoch: 187/200 Iteration: 15915 Train loss: 1.093014 Train acc: 0.410156\n",
      "Epoch: 187/200 Iteration: 15920 Train loss: 1.148162 Train acc: 0.312988\n",
      "Epoch: 187/200 Iteration: 15925 Train loss: 1.059593 Train acc: 0.467773\n",
      "Epoch: 187/200 Iteration: 15930 Train loss: 0.995976 Train acc: 0.579102\n",
      "Epoch: 187/200 Iteration: 15935 Train loss: 0.995226 Train acc: 0.584961\n",
      "Epoch: 187/200 Iteration: 15940 Train loss: 1.136290 Train acc: 0.278320\n",
      "Epoch: 187/200 Iteration: 15945 Train loss: 0.976485 Train acc: 0.631836\n",
      "Epoch: 187/200 Iteration: 15950 Train loss: 1.053438 Train acc: 0.494629\n",
      "Epoch: 187/200 Iteration: 15955 Train loss: 1.102489 Train acc: 0.310059\n",
      "Epoch: 187/200 Iteration: 15960 Train loss: 1.074193 Train acc: 0.458008\n",
      "Epoch: 187/200 Iteration: 15965 Train loss: 1.091688 Train acc: 0.463867\n",
      "Epoch: 187/200 Iteration: 15970 Train loss: 1.008876 Train acc: 0.417969\n",
      "Epoch: 187/200 Iteration: 15975 Train loss: 1.175674 Train acc: 0.340820\n",
      "Epoch: 187/200 Iteration: 15980 Train loss: 1.213074 Train acc: 0.232910\n",
      "Epoch: 188/200 Iteration: 15985 Train loss: 1.055036 Train acc: 0.503906\n",
      "Epoch: 188/200 Iteration: 15990 Train loss: 1.093599 Train acc: 0.399902\n",
      "Epoch: 188/200 Iteration: 15995 Train loss: 1.135719 Train acc: 0.348633\n",
      "Epoch: 188/200 Iteration: 16000 Train loss: 1.091355 Train acc: 0.410156\n",
      "Epoch: 188/200 Iteration: 16005 Train loss: 1.150079 Train acc: 0.312988\n",
      "Epoch: 188/200 Iteration: 16010 Train loss: 1.059689 Train acc: 0.467773\n",
      "Epoch: 188/200 Iteration: 16015 Train loss: 0.995534 Train acc: 0.579102\n",
      "Epoch: 188/200 Iteration: 16020 Train loss: 0.995502 Train acc: 0.584961\n",
      "Epoch: 188/200 Iteration: 16025 Train loss: 1.137309 Train acc: 0.278320\n",
      "Epoch: 188/200 Iteration: 16030 Train loss: 0.976037 Train acc: 0.631836\n",
      "Epoch: 188/200 Iteration: 16035 Train loss: 1.052583 Train acc: 0.494629\n",
      "Epoch: 188/200 Iteration: 16040 Train loss: 1.095512 Train acc: 0.397949\n",
      "Epoch: 188/200 Iteration: 16045 Train loss: 1.093357 Train acc: 0.411621\n",
      "Epoch: 188/200 Iteration: 16050 Train loss: 1.096577 Train acc: 0.425781\n",
      "Epoch: 188/200 Iteration: 16055 Train loss: 1.004995 Train acc: 0.477051\n",
      "Epoch: 188/200 Iteration: 16060 Train loss: 1.190490 Train acc: 0.343262\n",
      "Epoch: 188/200 Iteration: 16065 Train loss: 1.219471 Train acc: 0.231445\n",
      "Epoch: 189/200 Iteration: 16070 Train loss: 1.058541 Train acc: 0.503418\n",
      "Epoch: 189/200 Iteration: 16075 Train loss: 1.094309 Train acc: 0.400391\n",
      "Epoch: 189/200 Iteration: 16080 Train loss: 1.134039 Train acc: 0.348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189/200 Iteration: 16085 Train loss: 1.092072 Train acc: 0.410156\n",
      "Epoch: 189/200 Iteration: 16090 Train loss: 1.149427 Train acc: 0.312988\n",
      "Epoch: 189/200 Iteration: 16095 Train loss: 1.059389 Train acc: 0.467773\n",
      "Epoch: 189/200 Iteration: 16100 Train loss: 0.996911 Train acc: 0.579102\n",
      "Epoch: 189/200 Iteration: 16105 Train loss: 0.994634 Train acc: 0.584961\n",
      "Epoch: 189/200 Iteration: 16110 Train loss: 1.135973 Train acc: 0.278809\n",
      "Epoch: 189/200 Iteration: 16115 Train loss: 0.975639 Train acc: 0.631836\n",
      "Epoch: 189/200 Iteration: 16120 Train loss: 1.053979 Train acc: 0.494629\n",
      "Epoch: 189/200 Iteration: 16125 Train loss: 1.105186 Train acc: 0.289551\n",
      "Epoch: 189/200 Iteration: 16130 Train loss: 1.068873 Train acc: 0.451172\n",
      "Epoch: 189/200 Iteration: 16135 Train loss: 1.089329 Train acc: 0.471680\n",
      "Epoch: 189/200 Iteration: 16140 Train loss: 1.010914 Train acc: 0.414062\n",
      "Epoch: 189/200 Iteration: 16145 Train loss: 1.173993 Train acc: 0.340820\n",
      "Epoch: 189/200 Iteration: 16150 Train loss: 1.209736 Train acc: 0.230957\n",
      "Epoch: 190/200 Iteration: 16155 Train loss: 1.054728 Train acc: 0.503418\n",
      "Epoch: 190/200 Iteration: 16160 Train loss: 1.093968 Train acc: 0.399902\n",
      "Epoch: 190/200 Iteration: 16165 Train loss: 1.136734 Train acc: 0.348633\n",
      "Epoch: 190/200 Iteration: 16170 Train loss: 1.091285 Train acc: 0.410156\n",
      "Epoch: 190/200 Iteration: 16175 Train loss: 1.151226 Train acc: 0.312988\n",
      "Epoch: 190/200 Iteration: 16180 Train loss: 1.056642 Train acc: 0.467773\n",
      "Epoch: 190/200 Iteration: 16185 Train loss: 0.997319 Train acc: 0.579102\n",
      "Epoch: 190/200 Iteration: 16190 Train loss: 0.993403 Train acc: 0.584961\n",
      "Epoch: 190/200 Iteration: 16195 Train loss: 1.139107 Train acc: 0.278320\n",
      "Epoch: 190/200 Iteration: 16200 Train loss: 0.974517 Train acc: 0.631836\n",
      "Epoch: 190/200 Iteration: 16205 Train loss: 1.053381 Train acc: 0.494629\n",
      "Epoch: 190/200 Iteration: 16210 Train loss: 1.093905 Train acc: 0.403809\n",
      "Epoch: 190/200 Iteration: 16215 Train loss: 1.099005 Train acc: 0.380371\n",
      "Epoch: 190/200 Iteration: 16220 Train loss: 1.098524 Train acc: 0.425781\n",
      "Epoch: 190/200 Iteration: 16225 Train loss: 1.002749 Train acc: 0.500488\n",
      "Epoch: 190/200 Iteration: 16230 Train loss: 1.201076 Train acc: 0.346191\n",
      "Epoch: 190/200 Iteration: 16235 Train loss: 1.215781 Train acc: 0.231445\n",
      "Epoch: 191/200 Iteration: 16240 Train loss: 1.061627 Train acc: 0.503418\n",
      "Epoch: 191/200 Iteration: 16245 Train loss: 1.095280 Train acc: 0.400391\n",
      "Epoch: 191/200 Iteration: 16250 Train loss: 1.133946 Train acc: 0.348633\n",
      "Epoch: 191/200 Iteration: 16255 Train loss: 1.093467 Train acc: 0.410156\n",
      "Epoch: 191/200 Iteration: 16260 Train loss: 1.147581 Train acc: 0.312988\n",
      "Epoch: 191/200 Iteration: 16265 Train loss: 1.060878 Train acc: 0.467773\n",
      "Epoch: 191/200 Iteration: 16270 Train loss: 0.995156 Train acc: 0.579102\n",
      "Epoch: 191/200 Iteration: 16275 Train loss: 0.995813 Train acc: 0.584961\n",
      "Epoch: 191/200 Iteration: 16280 Train loss: 1.133487 Train acc: 0.278320\n",
      "Epoch: 191/200 Iteration: 16285 Train loss: 0.976867 Train acc: 0.631836\n",
      "Epoch: 191/200 Iteration: 16290 Train loss: 1.055563 Train acc: 0.494629\n",
      "Epoch: 191/200 Iteration: 16295 Train loss: 1.116450 Train acc: 0.244629\n",
      "Epoch: 191/200 Iteration: 16300 Train loss: 1.063409 Train acc: 0.460449\n",
      "Epoch: 191/200 Iteration: 16305 Train loss: 1.091099 Train acc: 0.473633\n",
      "Epoch: 191/200 Iteration: 16310 Train loss: 1.016063 Train acc: 0.411621\n",
      "Epoch: 191/200 Iteration: 16315 Train loss: 1.166654 Train acc: 0.315918\n",
      "Epoch: 191/200 Iteration: 16320 Train loss: 1.192273 Train acc: 0.232910\n",
      "Epoch: 192/200 Iteration: 16325 Train loss: 1.052479 Train acc: 0.503418\n",
      "Epoch: 192/200 Iteration: 16330 Train loss: 1.093646 Train acc: 0.400391\n",
      "Epoch: 192/200 Iteration: 16335 Train loss: 1.134497 Train acc: 0.348633\n",
      "Epoch: 192/200 Iteration: 16340 Train loss: 1.090571 Train acc: 0.410156\n",
      "Epoch: 192/200 Iteration: 16345 Train loss: 1.150357 Train acc: 0.312988\n",
      "Epoch: 192/200 Iteration: 16350 Train loss: 1.059371 Train acc: 0.467773\n",
      "Epoch: 192/200 Iteration: 16355 Train loss: 0.997955 Train acc: 0.579102\n",
      "Epoch: 192/200 Iteration: 16360 Train loss: 0.996567 Train acc: 0.584961\n",
      "Epoch: 192/200 Iteration: 16365 Train loss: 1.138379 Train acc: 0.278320\n",
      "Epoch: 192/200 Iteration: 16370 Train loss: 0.977010 Train acc: 0.631836\n",
      "Epoch: 192/200 Iteration: 16375 Train loss: 1.051736 Train acc: 0.494629\n",
      "Epoch: 192/200 Iteration: 16380 Train loss: 1.090901 Train acc: 0.403320\n",
      "Epoch: 192/200 Iteration: 16385 Train loss: 1.107585 Train acc: 0.336914\n",
      "Epoch: 192/200 Iteration: 16390 Train loss: 1.098117 Train acc: 0.426270\n",
      "Epoch: 192/200 Iteration: 16395 Train loss: 1.003645 Train acc: 0.498047\n",
      "Epoch: 192/200 Iteration: 16400 Train loss: 1.191876 Train acc: 0.342285\n",
      "Epoch: 192/200 Iteration: 16405 Train loss: 1.218138 Train acc: 0.233887\n",
      "Epoch: 193/200 Iteration: 16410 Train loss: 1.065497 Train acc: 0.503906\n",
      "Epoch: 193/200 Iteration: 16415 Train loss: 1.095919 Train acc: 0.401367\n",
      "Epoch: 193/200 Iteration: 16420 Train loss: 1.134025 Train acc: 0.348633\n",
      "Epoch: 193/200 Iteration: 16425 Train loss: 1.092805 Train acc: 0.410156\n",
      "Epoch: 193/200 Iteration: 16430 Train loss: 1.149890 Train acc: 0.312988\n",
      "Epoch: 193/200 Iteration: 16435 Train loss: 1.059844 Train acc: 0.467773\n",
      "Epoch: 193/200 Iteration: 16440 Train loss: 0.993730 Train acc: 0.579102\n",
      "Epoch: 193/200 Iteration: 16445 Train loss: 0.994441 Train acc: 0.584961\n",
      "Epoch: 193/200 Iteration: 16450 Train loss: 1.131844 Train acc: 0.278809\n",
      "Epoch: 193/200 Iteration: 16455 Train loss: 0.976045 Train acc: 0.631836\n",
      "Epoch: 193/200 Iteration: 16460 Train loss: 1.055959 Train acc: 0.494629\n",
      "Epoch: 193/200 Iteration: 16465 Train loss: 1.126498 Train acc: 0.232910\n",
      "Epoch: 193/200 Iteration: 16470 Train loss: 1.060152 Train acc: 0.458496\n",
      "Epoch: 193/200 Iteration: 16475 Train loss: 1.093364 Train acc: 0.470215\n",
      "Epoch: 193/200 Iteration: 16480 Train loss: 1.012128 Train acc: 0.411621\n",
      "Epoch: 193/200 Iteration: 16485 Train loss: 1.163996 Train acc: 0.319824\n",
      "Epoch: 193/200 Iteration: 16490 Train loss: 1.179958 Train acc: 0.232422\n",
      "Epoch: 194/200 Iteration: 16495 Train loss: 1.049087 Train acc: 0.503418\n",
      "Epoch: 194/200 Iteration: 16500 Train loss: 1.092354 Train acc: 0.399902\n",
      "Epoch: 194/200 Iteration: 16505 Train loss: 1.133729 Train acc: 0.348633\n",
      "Epoch: 194/200 Iteration: 16510 Train loss: 1.089630 Train acc: 0.410156\n",
      "Epoch: 194/200 Iteration: 16515 Train loss: 1.150279 Train acc: 0.312988\n",
      "Epoch: 194/200 Iteration: 16520 Train loss: 1.059314 Train acc: 0.467773\n",
      "Epoch: 194/200 Iteration: 16525 Train loss: 1.000144 Train acc: 0.579102\n",
      "Epoch: 194/200 Iteration: 16530 Train loss: 0.995208 Train acc: 0.584961\n",
      "Epoch: 194/200 Iteration: 16535 Train loss: 1.139424 Train acc: 0.278320\n",
      "Epoch: 194/200 Iteration: 16540 Train loss: 0.976276 Train acc: 0.631836\n",
      "Epoch: 194/200 Iteration: 16545 Train loss: 1.051147 Train acc: 0.494629\n",
      "Epoch: 194/200 Iteration: 16550 Train loss: 1.087779 Train acc: 0.404785\n",
      "Epoch: 194/200 Iteration: 16555 Train loss: 1.122640 Train acc: 0.258301\n",
      "Epoch: 194/200 Iteration: 16560 Train loss: 1.099264 Train acc: 0.442383\n",
      "Epoch: 194/200 Iteration: 16565 Train loss: 1.006281 Train acc: 0.478027\n",
      "Epoch: 194/200 Iteration: 16570 Train loss: 1.197828 Train acc: 0.342285\n",
      "Epoch: 194/200 Iteration: 16575 Train loss: 1.215010 Train acc: 0.233398\n",
      "Epoch: 195/200 Iteration: 16580 Train loss: 1.064258 Train acc: 0.502930\n",
      "Epoch: 195/200 Iteration: 16585 Train loss: 1.093432 Train acc: 0.401855\n",
      "Epoch: 195/200 Iteration: 16590 Train loss: 1.135280 Train acc: 0.348633\n",
      "Epoch: 195/200 Iteration: 16595 Train loss: 1.092958 Train acc: 0.410156\n",
      "Epoch: 195/200 Iteration: 16600 Train loss: 1.148449 Train acc: 0.312988\n",
      "Epoch: 195/200 Iteration: 16605 Train loss: 1.058682 Train acc: 0.467773\n",
      "Epoch: 195/200 Iteration: 16610 Train loss: 0.996275 Train acc: 0.579102\n",
      "Epoch: 195/200 Iteration: 16615 Train loss: 0.993850 Train acc: 0.584961\n",
      "Epoch: 195/200 Iteration: 16620 Train loss: 1.129731 Train acc: 0.278320\n",
      "Epoch: 195/200 Iteration: 16625 Train loss: 0.974932 Train acc: 0.631836\n",
      "Epoch: 195/200 Iteration: 16630 Train loss: 1.057334 Train acc: 0.494629\n",
      "Epoch: 195/200 Iteration: 16635 Train loss: 1.136446 Train acc: 0.239258\n",
      "Epoch: 195/200 Iteration: 16640 Train loss: 1.061982 Train acc: 0.458984\n",
      "Epoch: 195/200 Iteration: 16645 Train loss: 1.088782 Train acc: 0.471191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195/200 Iteration: 16650 Train loss: 1.014587 Train acc: 0.411621\n",
      "Epoch: 195/200 Iteration: 16655 Train loss: 1.163934 Train acc: 0.313965\n",
      "Epoch: 195/200 Iteration: 16660 Train loss: 1.181510 Train acc: 0.230957\n",
      "Epoch: 196/200 Iteration: 16665 Train loss: 1.048955 Train acc: 0.503418\n",
      "Epoch: 196/200 Iteration: 16670 Train loss: 1.093425 Train acc: 0.399902\n",
      "Epoch: 196/200 Iteration: 16675 Train loss: 1.134734 Train acc: 0.348633\n",
      "Epoch: 196/200 Iteration: 16680 Train loss: 1.090197 Train acc: 0.410156\n",
      "Epoch: 196/200 Iteration: 16685 Train loss: 1.148510 Train acc: 0.312988\n",
      "Epoch: 196/200 Iteration: 16690 Train loss: 1.057268 Train acc: 0.467773\n",
      "Epoch: 196/200 Iteration: 16695 Train loss: 0.997826 Train acc: 0.579102\n",
      "Epoch: 196/200 Iteration: 16700 Train loss: 0.995526 Train acc: 0.584961\n",
      "Epoch: 196/200 Iteration: 16705 Train loss: 1.141239 Train acc: 0.278320\n",
      "Epoch: 196/200 Iteration: 16710 Train loss: 0.975798 Train acc: 0.631836\n",
      "Epoch: 196/200 Iteration: 16715 Train loss: 1.051654 Train acc: 0.494629\n",
      "Epoch: 196/200 Iteration: 16720 Train loss: 1.088652 Train acc: 0.395508\n",
      "Epoch: 196/200 Iteration: 16725 Train loss: 1.123210 Train acc: 0.251953\n",
      "Epoch: 196/200 Iteration: 16730 Train loss: 1.096415 Train acc: 0.451660\n",
      "Epoch: 196/200 Iteration: 16735 Train loss: 1.004749 Train acc: 0.486816\n",
      "Epoch: 196/200 Iteration: 16740 Train loss: 1.196806 Train acc: 0.343750\n",
      "Epoch: 196/200 Iteration: 16745 Train loss: 1.218400 Train acc: 0.231934\n",
      "Epoch: 197/200 Iteration: 16750 Train loss: 1.067783 Train acc: 0.502930\n",
      "Epoch: 197/200 Iteration: 16755 Train loss: 1.095560 Train acc: 0.404297\n",
      "Epoch: 197/200 Iteration: 16760 Train loss: 1.138503 Train acc: 0.348633\n",
      "Epoch: 197/200 Iteration: 16765 Train loss: 1.094371 Train acc: 0.410156\n",
      "Epoch: 197/200 Iteration: 16770 Train loss: 1.147276 Train acc: 0.312988\n",
      "Epoch: 197/200 Iteration: 16775 Train loss: 1.057636 Train acc: 0.467773\n",
      "Epoch: 197/200 Iteration: 16780 Train loss: 0.993520 Train acc: 0.579102\n",
      "Epoch: 197/200 Iteration: 16785 Train loss: 0.994899 Train acc: 0.584961\n",
      "Epoch: 197/200 Iteration: 16790 Train loss: 1.124673 Train acc: 0.280762\n",
      "Epoch: 197/200 Iteration: 16795 Train loss: 0.977321 Train acc: 0.631836\n",
      "Epoch: 197/200 Iteration: 16800 Train loss: 1.059139 Train acc: 0.494629\n",
      "Epoch: 197/200 Iteration: 16805 Train loss: 1.131959 Train acc: 0.242188\n",
      "Epoch: 197/200 Iteration: 16810 Train loss: 1.063181 Train acc: 0.461914\n",
      "Epoch: 197/200 Iteration: 16815 Train loss: 1.087892 Train acc: 0.473633\n",
      "Epoch: 197/200 Iteration: 16820 Train loss: 1.015633 Train acc: 0.411621\n",
      "Epoch: 197/200 Iteration: 16825 Train loss: 1.164008 Train acc: 0.304199\n",
      "Epoch: 197/200 Iteration: 16830 Train loss: 1.175381 Train acc: 0.233887\n",
      "Epoch: 198/200 Iteration: 16835 Train loss: 1.049029 Train acc: 0.503418\n",
      "Epoch: 198/200 Iteration: 16840 Train loss: 1.093552 Train acc: 0.399902\n",
      "Epoch: 198/200 Iteration: 16845 Train loss: 1.137717 Train acc: 0.348633\n",
      "Epoch: 198/200 Iteration: 16850 Train loss: 1.091267 Train acc: 0.410156\n",
      "Epoch: 198/200 Iteration: 16855 Train loss: 1.147921 Train acc: 0.312988\n",
      "Epoch: 198/200 Iteration: 16860 Train loss: 1.061035 Train acc: 0.467773\n",
      "Epoch: 198/200 Iteration: 16865 Train loss: 0.997620 Train acc: 0.579102\n",
      "Epoch: 198/200 Iteration: 16870 Train loss: 0.996639 Train acc: 0.584961\n",
      "Epoch: 198/200 Iteration: 16875 Train loss: 1.135766 Train acc: 0.278809\n",
      "Epoch: 198/200 Iteration: 16880 Train loss: 0.978562 Train acc: 0.631836\n",
      "Epoch: 198/200 Iteration: 16885 Train loss: 1.053145 Train acc: 0.494629\n",
      "Epoch: 198/200 Iteration: 16890 Train loss: 1.090528 Train acc: 0.397949\n",
      "Epoch: 198/200 Iteration: 16895 Train loss: 1.119111 Train acc: 0.278320\n",
      "Epoch: 198/200 Iteration: 16900 Train loss: 1.094786 Train acc: 0.459473\n",
      "Epoch: 198/200 Iteration: 16905 Train loss: 1.006724 Train acc: 0.472168\n",
      "Epoch: 198/200 Iteration: 16910 Train loss: 1.190411 Train acc: 0.345703\n",
      "Epoch: 198/200 Iteration: 16915 Train loss: 1.210066 Train acc: 0.239258\n",
      "Epoch: 199/200 Iteration: 16920 Train loss: 1.065927 Train acc: 0.503906\n",
      "Epoch: 199/200 Iteration: 16925 Train loss: 1.093876 Train acc: 0.401367\n",
      "Epoch: 199/200 Iteration: 16930 Train loss: 1.132713 Train acc: 0.348633\n",
      "Epoch: 199/200 Iteration: 16935 Train loss: 1.093226 Train acc: 0.410156\n",
      "Epoch: 199/200 Iteration: 16940 Train loss: 1.145994 Train acc: 0.312988\n",
      "Epoch: 199/200 Iteration: 16945 Train loss: 1.059581 Train acc: 0.467773\n",
      "Epoch: 199/200 Iteration: 16950 Train loss: 0.995674 Train acc: 0.579102\n",
      "Epoch: 199/200 Iteration: 16955 Train loss: 0.996506 Train acc: 0.584961\n",
      "Epoch: 199/200 Iteration: 16960 Train loss: 1.128977 Train acc: 0.278320\n",
      "Epoch: 199/200 Iteration: 16965 Train loss: 0.977599 Train acc: 0.631836\n",
      "Epoch: 199/200 Iteration: 16970 Train loss: 1.057976 Train acc: 0.494629\n",
      "Epoch: 199/200 Iteration: 16975 Train loss: 1.135701 Train acc: 0.235352\n",
      "Epoch: 199/200 Iteration: 16980 Train loss: 1.069235 Train acc: 0.445801\n",
      "Epoch: 199/200 Iteration: 16985 Train loss: 1.092732 Train acc: 0.469727\n",
      "Epoch: 199/200 Iteration: 16990 Train loss: 1.016401 Train acc: 0.412598\n",
      "Epoch: 199/200 Iteration: 16995 Train loss: 1.166560 Train acc: 0.330078\n",
      "Epoch: 199/200 Iteration: 17000 Train loss: 1.174902 Train acc: 0.232422\n"
     ]
    }
   ],
   "source": [
    "interation_compute_val = 1\n",
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%interation_compute_val == 50):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints/har-lstm.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAF3CAYAAACPC83LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % interation_compute_val == 50], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAF3CAYAAABUsGfpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVIpw0KSVMqwkCSV+v8/7QBo6muLGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot Accuracies# Plot A \n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % interation_compute_val == 50], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\har-lstm.ckpt\n",
      "(70626, 3)\n",
      "0.478516\n",
      "0.634766\n",
      "0.390137\n",
      "0.580078\n",
      "0.434082\n",
      "0.516113\n",
      "0.537109\n",
      "0.512207\n",
      "0.516602\n",
      "0.393555\n",
      "0.513672\n",
      "0.544922\n",
      "0.507813\n",
      "0.47998\n",
      "0.382324\n",
      "0.230469\n",
      "0.0126953\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Test accuracy: 0.225442\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    print(y_vld.shape)\n",
    "    for x_t, y_t in get_batches(X_vld, y_vld, batch_size):        \n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "        print(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  0  4  5  6  7  8 10]\n",
      " [ 0 11  0 13  0 15  0 17 18  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = np.array([[ 0,  1,  2,  0,  4,  5,  6,  7,  8,  10],\n",
    "              [ 0, 11,  0, 13,  0, 15,  0, 17, 18,  0]])\n",
    "print (a[a.max(axis=1) >= 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 0 4 5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = np.array([[ 0,  1,  2,  0,  4,  5,  6,  7,  8,  9],\n",
    "              [ 0, 11,  0, 13,  0, 15,  0, 17, 18,  0]])\n",
    "print (a[a[:,2]!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-a2bf8e0aca21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m Xtest = np.array([[ 0,  1,  2,  0,  4,  5,  6,  7,  8,  9],\n\u001b[0;32m      3\u001b[0m               \u001b[1;33m[\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m              [ 0, 11,  0, 13,  0, 15,  0, 11, 1,  1]])\n\u001b[0;32m      5\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ytest' is not defined"
     ]
    }
   ],
   "source": [
    "print(ytest[:]!=0)\n",
    "Xtest = np.array([[ 0,  1,  2,  0,  4,  5,  6,  7,  8,  9],\n",
    "              [ 0, 11,  0, 13,  0, 15,  0, 17, 18,  0],\n",
    "             [ 0, 11,  0, 13,  0, 15,  0, 11, 1,  1]])\n",
    "ytest = np.array([ 0,  1,  1])\n",
    "Xtest = Xtest[ytest[:]!=0,:]\n",
    "ytest = ytest[ytest[:]!=0]\n",
    "print(Xtest)\n",
    "print(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
